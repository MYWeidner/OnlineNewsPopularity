{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf2fc128-8a22-44de-9777-43aca4dd1a06",
   "metadata": {},
   "source": [
    "# Online News Popularity Project\n",
    "### A Prediction Study\n",
    "#### By: Mayra Weidner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c7b0a87-beb1-4379-8a23-a87c29d39e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.linear_model import RidgeCV, LassoCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bfd63fd-d6fd-4671-bf03-9f02e59763d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('Data/df_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b7f556-9b8b-4f4f-8bf0-477d7cc18bbd",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10aac295-913b-4abf-b82d-2d625f31c244",
   "metadata": {},
   "source": [
    "My project involves utilizing regression analysis to predict the number of times articles can be shared. Below I will describe the 3 different regression models that I could use and analyze any foreseen difficulties.\n",
    "\n",
    "**Linear Regression:**    \n",
    "Linear regression is a common and simple regression model used to establish a linear relationship between the predictor / independent variables (all attributes besides number of shares) and the dependent variable (number of shares). R-squared is important because it measures how well the model fits the data. R-squared closer to 1 indicates a better fit. R-squared near 0 means that the model is wrong, or the error variance is too big or both.\n",
    "\n",
    "I may choose linear regression because it is easy to interpret and implement. It can provide insights into the direction and magnitude of the relationships between the predictor variables and the number of shares.\n",
    "\n",
    "The boxplots of the attributes shows that there is some noise in the data so a linear regression model might be a good fit since it may be difficult to detect another shape for the model.\n",
    "\n",
    "As previously discussed in the Data Cleaning and EDA sections of this project, I dropped n_non_stop_words, n_unique_tokens, kw_avg_min, and kw_avg_avg because these attributes were strongly correlated with other attributes. Excluding strongly correlated attributes is unlikely to significantly impact the model's performance, and it can even be beneficial by preventing overfitting and simplifying the model. \n",
    "\n",
    "**Multilinear Regression:**  \n",
    "Multilinear regression extends linear regression by including more than one predictor variable. This allows for capturing the combined effect of multiple predictors on the dependent variable. \n",
    "\n",
    "I may use multilinear regression if there are multiple relevant attributes that can potentially impact the number of shares. For example, data channel and weekday. By incorporating multiple predictors, the model can provide a more comprehensive understanding of the factors influencing shareability.\n",
    "\n",
    "The ideal scenario for this model is when predictors are uncorrelated. Since I already removed some of the strongly correlated attributes, it may reduce the difficulty of interpreting regression coefficients so additional data cleaning and checks for collinearity (when two or more variables are exactly correlated) may not be necessary. R-squared is also important with this model because it measures how well the model fits the data\n",
    "\n",
    "**Logistic Regression:**  \n",
    "Logistic regression is a classification method commonly used when the dependent variable is categorical or binary. Using logistic regression, I could predict the likelihood of an article being highly shared or not (e.g., above a certain threshold) rather than predicting the exact number of shares. Logistic regression can be used to model the probability of an article falling into a specific category of high or low shares based on the given predictors. In this case I could use the median shares as the threshold. Articles under the median shares would be low shares (unpopular - 0) and articles with shares greater than or equal to the median shares would be high shares (popular - 1). This approach would require me to change the purpose of my project.\n",
    "\n",
    "**Conclusion: Model Selection**  \n",
    "Given my analysis of the 3 regression models that I could use, I decided to start with linear regression as this is the easiest to implement and interpret. Since I have already removed strongly correlated attributes, I will also try multilinear regression and determine which model is the better fit. \n",
    "\n",
    "I will not use logistic regression as this model would require me to change the purpose of my project and it's too late into the process to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c45f63-3b94-4a18-857e-63b00fe882c9",
   "metadata": {},
   "source": [
    "### Simple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05072cbd-8fb4-4e5e-8073-c4ae72c86fee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>average_token_length</th>\n",
       "      <th>num_keywords</th>\n",
       "      <th>kw_min_min</th>\n",
       "      <th>...</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "      <th>data_channel</th>\n",
       "      <th>weekday_is</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>0.815385</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.680365</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.350000</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>593</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.791946</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.913725</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.118750</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>711</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.393365</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.466667</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1500</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.0</td>\n",
       "      <td>531.0</td>\n",
       "      <td>0.665635</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.404896</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.369697</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1200</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.0</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>0.540890</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.682836</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.220192</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>505</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_tokens_title  n_tokens_content  n_non_stop_unique_tokens  num_hrefs   \n",
       "0            12.0             219.0                  0.815385        4.0  \\\n",
       "1             9.0             255.0                  0.791946        3.0   \n",
       "2             9.0             211.0                  0.663866        3.0   \n",
       "3             9.0             531.0                  0.665635        9.0   \n",
       "4            13.0            1072.0                  0.540890       19.0   \n",
       "\n",
       "   num_self_hrefs  num_imgs  num_videos  average_token_length  num_keywords   \n",
       "0             2.0       1.0         0.0              4.680365           5.0  \\\n",
       "1             1.0       1.0         0.0              4.913725           4.0   \n",
       "2             1.0       1.0         0.0              4.393365           6.0   \n",
       "3             0.0       1.0         0.0              4.404896           7.0   \n",
       "4            19.0      20.0         0.0              4.682836           7.0   \n",
       "\n",
       "   kw_min_min  ...  avg_negative_polarity  min_negative_polarity   \n",
       "0         0.0  ...              -0.350000                 -0.600  \\\n",
       "1         0.0  ...              -0.118750                 -0.125   \n",
       "2         0.0  ...              -0.466667                 -0.800   \n",
       "3         0.0  ...              -0.369697                 -0.600   \n",
       "4         0.0  ...              -0.220192                 -0.500   \n",
       "\n",
       "   max_negative_polarity  title_subjectivity  title_sentiment_polarity   \n",
       "0              -0.200000            0.500000                 -0.187500  \\\n",
       "1              -0.100000            0.000000                  0.000000   \n",
       "2              -0.133333            0.000000                  0.000000   \n",
       "3              -0.166667            0.000000                  0.000000   \n",
       "4              -0.050000            0.454545                  0.136364   \n",
       "\n",
       "   abs_title_subjectivity  abs_title_sentiment_polarity  shares  data_channel   \n",
       "0                0.000000                      0.187500     593             2  \\\n",
       "1                0.500000                      0.000000     711             3   \n",
       "2                0.500000                      0.000000    1500             3   \n",
       "3                0.500000                      0.000000    1200             2   \n",
       "4                0.045455                      0.136364     505             5   \n",
       "\n",
       "   weekday_is  \n",
       "0           1  \n",
       "1           1  \n",
       "2           1  \n",
       "3           1  \n",
       "4           1  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60798c81-aed5-4fec-8b6c-34daed171ce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['n_tokens_title', 'n_tokens_content', 'n_non_stop_unique_tokens',\n",
       "       'num_hrefs', 'num_self_hrefs', 'num_imgs', 'num_videos',\n",
       "       'average_token_length', 'num_keywords', 'kw_min_min', 'kw_max_min',\n",
       "       'kw_min_max', 'kw_max_max', 'kw_avg_max', 'kw_min_avg', 'kw_max_avg',\n",
       "       'self_reference_min_shares', 'self_reference_max_shares',\n",
       "       'self_reference_avg_sharess', 'LDA_00', 'LDA_01', 'LDA_02', 'LDA_03',\n",
       "       'LDA_04', 'global_subjectivity', 'global_sentiment_polarity',\n",
       "       'global_rate_positive_words', 'global_rate_negative_words',\n",
       "       'rate_positive_words', 'rate_negative_words', 'avg_positive_polarity',\n",
       "       'min_positive_polarity', 'max_positive_polarity',\n",
       "       'avg_negative_polarity', 'min_negative_polarity',\n",
       "       'max_negative_polarity', 'title_subjectivity',\n",
       "       'title_sentiment_polarity', 'abs_title_subjectivity',\n",
       "       'abs_title_sentiment_polarity', 'shares', 'data_channel', 'weekday_is'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ca23199-da89-45cd-b966-4c86548d7b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictors in the dataset\n",
    "predictors = ['n_tokens_title', 'n_tokens_content', 'n_non_stop_unique_tokens',\n",
    "       'num_hrefs', 'num_self_hrefs', 'num_imgs', 'num_videos',\n",
    "       'average_token_length', 'num_keywords', 'kw_min_min', 'kw_max_min',\n",
    "       'kw_min_max', 'kw_max_max', 'kw_avg_max', 'kw_min_avg', 'kw_max_avg',\n",
    "       'self_reference_min_shares', 'self_reference_max_shares',\n",
    "       'self_reference_avg_sharess', 'LDA_00', 'LDA_01', 'LDA_02', 'LDA_03',\n",
    "       'LDA_04', 'global_subjectivity', 'global_sentiment_polarity',\n",
    "       'global_rate_positive_words', 'global_rate_negative_words',\n",
    "       'rate_positive_words', 'rate_negative_words', 'avg_positive_polarity',\n",
    "       'min_positive_polarity', 'max_positive_polarity',\n",
    "       'avg_negative_polarity', 'min_negative_polarity',\n",
    "       'max_negative_polarity', 'title_subjectivity',\n",
    "       'title_sentiment_polarity', 'abs_title_subjectivity',\n",
    "       'abs_title_sentiment_polarity', 'data_channel', 'weekday_is']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b378adc2-4219-4fb2-b8b3-f79f2cb671b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features (X) and target (y)\n",
    "X = df[predictors]\n",
    "y = df['shares']\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3bdcdd6-a843-48fa-8004-6ff2721ec336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25756, 42)\n",
      "(12687, 42)\n"
     ]
    }
   ],
   "source": [
    "# Shape of train and test datasets\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d02eee37-62c6-4ab2-98be-88a1b99c3ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25756,)\n",
      "(12687,)\n"
     ]
    }
   ],
   "source": [
    "# Print shape of y objects\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "382e8ba9-d294-4658-bc31-ddf5c14b492e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>average_token_length</th>\n",
       "      <th>num_keywords</th>\n",
       "      <th>kw_min_min</th>\n",
       "      <th>...</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>data_channel</th>\n",
       "      <th>weekday_is</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35308</th>\n",
       "      <td>12.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>0.820359</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.625850</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.332143</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>-0.1250</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2595</th>\n",
       "      <td>11.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>0.762500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.690141</td>\n",
       "      <td>5.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.194444</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>-0.1000</td>\n",
       "      <td>0.60000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6100</th>\n",
       "      <td>10.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.748031</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>-0.1000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11827</th>\n",
       "      <td>9.0</td>\n",
       "      <td>292.0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>4.602740</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.475000</td>\n",
       "      <td>-0.700000</td>\n",
       "      <td>-0.2000</td>\n",
       "      <td>0.49513</td>\n",
       "      <td>0.211039</td>\n",
       "      <td>0.00487</td>\n",
       "      <td>0.211039</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12758</th>\n",
       "      <td>11.0</td>\n",
       "      <td>306.0</td>\n",
       "      <td>0.741573</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.205882</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.473438</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.1875</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       n_tokens_title  n_tokens_content  n_non_stop_unique_tokens  num_hrefs   \n",
       "35308            12.0             294.0                  0.820359        7.0  \\\n",
       "2595             11.0             284.0                  0.762500        1.0   \n",
       "6100             10.0             254.0                  0.745455        7.0   \n",
       "11827             9.0             292.0                  0.750000        6.0   \n",
       "12758            11.0             306.0                  0.741573       15.0   \n",
       "\n",
       "       num_self_hrefs  num_imgs  num_videos  average_token_length   \n",
       "35308             5.0       2.0         1.0              4.625850  \\\n",
       "2595              0.0       1.0         1.0              4.690141   \n",
       "6100              5.0       1.0         0.0              4.748031   \n",
       "11827             5.0       0.0        17.0              4.602740   \n",
       "12758             2.0      15.0         0.0              4.205882   \n",
       "\n",
       "       num_keywords  kw_min_min  ...  max_positive_polarity   \n",
       "35308           7.0        -1.0  ...                    1.0  \\\n",
       "2595            5.0       217.0  ...                    0.8   \n",
       "6100            6.0         4.0  ...                    0.7   \n",
       "11827           5.0         4.0  ...                    1.0   \n",
       "12758          10.0         4.0  ...                    1.0   \n",
       "\n",
       "       avg_negative_polarity  min_negative_polarity  max_negative_polarity   \n",
       "35308              -0.332143              -0.400000                -0.1250  \\\n",
       "2595               -0.194444              -0.300000                -0.1000   \n",
       "6100               -0.133333              -0.166667                -0.1000   \n",
       "11827              -0.475000              -0.700000                -0.2000   \n",
       "12758              -0.473438              -1.000000                -0.1875   \n",
       "\n",
       "       title_subjectivity  title_sentiment_polarity  abs_title_subjectivity   \n",
       "35308             0.00000                  0.000000                 0.50000  \\\n",
       "2595              0.60000                  0.200000                 0.10000   \n",
       "6100              0.00000                  0.000000                 0.50000   \n",
       "11827             0.49513                  0.211039                 0.00487   \n",
       "12758             0.00000                  0.000000                 0.50000   \n",
       "\n",
       "       abs_title_sentiment_polarity  data_channel  weekday_is  \n",
       "35308                      0.000000             2           3  \n",
       "2595                       0.200000             3           4  \n",
       "6100                       0.000000             3           2  \n",
       "11827                      0.211039             4           4  \n",
       "12758                      0.000000             7           3  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create DataFrame with training dataset\n",
    "x_df_train = pd.DataFrame(data=X_train, columns=predictors)\n",
    "x_df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "debb3a55-8756-44b1-b324-1379394a70db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35308</th>\n",
       "      <td>1400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2595</th>\n",
       "      <td>2300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6100</th>\n",
       "      <td>968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11827</th>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12758</th>\n",
       "      <td>74100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       shares\n",
       "35308    1400\n",
       "2595     2300\n",
       "6100      968\n",
       "11827    1500\n",
       "12758   74100"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training series with shares\n",
    "y_df_train = pd.DataFrame(data=y_train, columns=['shares']) \n",
    "y_df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "353be53e-fdb6-4eb3-8e89-774e601c2a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>average_token_length</th>\n",
       "      <th>num_keywords</th>\n",
       "      <th>kw_min_min</th>\n",
       "      <th>...</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>data_channel</th>\n",
       "      <th>weekday_is</th>\n",
       "      <th>shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35308</th>\n",
       "      <td>12.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>0.820359</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.625850</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.332143</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>-0.1250</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2595</th>\n",
       "      <td>11.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>0.762500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.690141</td>\n",
       "      <td>5.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.194444</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>-0.1000</td>\n",
       "      <td>0.60000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6100</th>\n",
       "      <td>10.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.748031</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>-0.1000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11827</th>\n",
       "      <td>9.0</td>\n",
       "      <td>292.0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>4.602740</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.475000</td>\n",
       "      <td>-0.700000</td>\n",
       "      <td>-0.2000</td>\n",
       "      <td>0.49513</td>\n",
       "      <td>0.211039</td>\n",
       "      <td>0.00487</td>\n",
       "      <td>0.211039</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12758</th>\n",
       "      <td>11.0</td>\n",
       "      <td>306.0</td>\n",
       "      <td>0.741573</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.205882</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.473438</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.1875</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>74100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       n_tokens_title  n_tokens_content  n_non_stop_unique_tokens  num_hrefs   \n",
       "35308            12.0             294.0                  0.820359        7.0  \\\n",
       "2595             11.0             284.0                  0.762500        1.0   \n",
       "6100             10.0             254.0                  0.745455        7.0   \n",
       "11827             9.0             292.0                  0.750000        6.0   \n",
       "12758            11.0             306.0                  0.741573       15.0   \n",
       "\n",
       "       num_self_hrefs  num_imgs  num_videos  average_token_length   \n",
       "35308             5.0       2.0         1.0              4.625850  \\\n",
       "2595              0.0       1.0         1.0              4.690141   \n",
       "6100              5.0       1.0         0.0              4.748031   \n",
       "11827             5.0       0.0        17.0              4.602740   \n",
       "12758             2.0      15.0         0.0              4.205882   \n",
       "\n",
       "       num_keywords  kw_min_min  ...  avg_negative_polarity   \n",
       "35308           7.0        -1.0  ...              -0.332143  \\\n",
       "2595            5.0       217.0  ...              -0.194444   \n",
       "6100            6.0         4.0  ...              -0.133333   \n",
       "11827           5.0         4.0  ...              -0.475000   \n",
       "12758          10.0         4.0  ...              -0.473438   \n",
       "\n",
       "       min_negative_polarity  max_negative_polarity  title_subjectivity   \n",
       "35308              -0.400000                -0.1250             0.00000  \\\n",
       "2595               -0.300000                -0.1000             0.60000   \n",
       "6100               -0.166667                -0.1000             0.00000   \n",
       "11827              -0.700000                -0.2000             0.49513   \n",
       "12758              -1.000000                -0.1875             0.00000   \n",
       "\n",
       "       title_sentiment_polarity  abs_title_subjectivity   \n",
       "35308                  0.000000                 0.50000  \\\n",
       "2595                   0.200000                 0.10000   \n",
       "6100                   0.000000                 0.50000   \n",
       "11827                  0.211039                 0.00487   \n",
       "12758                  0.000000                 0.50000   \n",
       "\n",
       "       abs_title_sentiment_polarity  data_channel  weekday_is  shares  \n",
       "35308                      0.000000             2           3    1400  \n",
       "2595                       0.200000             3           4    2300  \n",
       "6100                       0.000000             3           2     968  \n",
       "11827                      0.211039             4           4    1500  \n",
       "12758                      0.000000             7           3   74100  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate x and y together to create dataframe of training set\n",
    "train_df = pd.concat([x_df_train, y_df_train], axis=1, sort=False)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2251e8b9-5fbc-4f35-a051-b54c5e0dcf70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>average_token_length</th>\n",
       "      <th>num_keywords</th>\n",
       "      <th>kw_min_min</th>\n",
       "      <th>...</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>data_channel</th>\n",
       "      <th>weekday_is</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5532</th>\n",
       "      <td>9.0</td>\n",
       "      <td>218.0</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.353211</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-0.409524</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>-0.071429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15242</th>\n",
       "      <td>11.0</td>\n",
       "      <td>268.0</td>\n",
       "      <td>0.835616</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.492537</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-0.118750</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35329</th>\n",
       "      <td>14.0</td>\n",
       "      <td>851.0</td>\n",
       "      <td>0.633858</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.584019</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.307568</td>\n",
       "      <td>-0.800000</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.1125</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1125</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9288</th>\n",
       "      <td>9.0</td>\n",
       "      <td>725.0</td>\n",
       "      <td>0.536748</td>\n",
       "      <td>19.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.590345</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.246875</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>-0.6000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14634</th>\n",
       "      <td>8.0</td>\n",
       "      <td>616.0</td>\n",
       "      <td>0.753846</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.683442</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.337976</td>\n",
       "      <td>-0.625000</td>\n",
       "      <td>-0.071429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       n_tokens_title  n_tokens_content  n_non_stop_unique_tokens  num_hrefs   \n",
       "5532              9.0             218.0                  0.843750        2.0  \\\n",
       "15242            11.0             268.0                  0.835616        5.0   \n",
       "35329            14.0             851.0                  0.633858       15.0   \n",
       "9288              9.0             725.0                  0.536748       19.0   \n",
       "14634             8.0             616.0                  0.753846       10.0   \n",
       "\n",
       "       num_self_hrefs  num_imgs  num_videos  average_token_length   \n",
       "5532              1.0       0.0         1.0              4.353211  \\\n",
       "15242             3.0       1.0         0.0              4.492537   \n",
       "35329             6.0       1.0         0.0              4.584019   \n",
       "9288              6.0       1.0         0.0              4.590345   \n",
       "14634             0.0       1.0         0.0              4.683442   \n",
       "\n",
       "       num_keywords  kw_min_min  ...  max_positive_polarity   \n",
       "5532            7.0         4.0  ...                    0.6  \\\n",
       "15242           4.0         4.0  ...                    0.6   \n",
       "35329           7.0        -1.0  ...                    0.8   \n",
       "9288           10.0         4.0  ...                    1.0   \n",
       "14634           7.0         4.0  ...                    1.0   \n",
       "\n",
       "       avg_negative_polarity  min_negative_polarity  max_negative_polarity   \n",
       "5532               -0.409524              -0.666667              -0.071429  \\\n",
       "15242              -0.118750              -0.187500              -0.050000   \n",
       "35329              -0.307568              -0.800000              -0.050000   \n",
       "9288               -0.246875              -0.500000              -0.100000   \n",
       "14634              -0.337976              -0.625000              -0.071429   \n",
       "\n",
       "       title_subjectivity  title_sentiment_polarity  abs_title_subjectivity   \n",
       "5532                  0.0                    0.0000                     0.5  \\\n",
       "15242                 0.1                    0.3000                     0.4   \n",
       "35329                 0.7                    0.1125                     0.2   \n",
       "9288                  0.9                   -0.6000                     0.4   \n",
       "14634                 0.0                    0.0000                     0.5   \n",
       "\n",
       "       abs_title_sentiment_polarity  data_channel  weekday_is  \n",
       "5532                         0.0000             2           4  \n",
       "15242                        0.3000             6           3  \n",
       "35329                        0.1125             5           3  \n",
       "9288                         0.6000             6           3  \n",
       "14634                        0.0000             2           5  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create DataFrame with test dataset\n",
    "x_df_test = pd.DataFrame(data=X_test, columns=predictors)\n",
    "x_df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dea3f0d5-698e-488c-9b9a-e78b61eff624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5532</th>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15242</th>\n",
       "      <td>1100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35329</th>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9288</th>\n",
       "      <td>9500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14634</th>\n",
       "      <td>682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       shares\n",
       "5532     1500\n",
       "15242    1100\n",
       "35329    1600\n",
       "9288     9500\n",
       "14634     682"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test series with shares\n",
    "y_df_test = pd.DataFrame(data=y_test, columns=['shares']) \n",
    "y_df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f16ad929-72e9-429c-ac4c-6ca2d2ad8c43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>average_token_length</th>\n",
       "      <th>num_keywords</th>\n",
       "      <th>kw_min_min</th>\n",
       "      <th>...</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>data_channel</th>\n",
       "      <th>weekday_is</th>\n",
       "      <th>shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5532</th>\n",
       "      <td>9.0</td>\n",
       "      <td>218.0</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.353211</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.409524</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>-0.071429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15242</th>\n",
       "      <td>11.0</td>\n",
       "      <td>268.0</td>\n",
       "      <td>0.835616</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.492537</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.118750</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35329</th>\n",
       "      <td>14.0</td>\n",
       "      <td>851.0</td>\n",
       "      <td>0.633858</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.584019</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.307568</td>\n",
       "      <td>-0.800000</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.1125</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1125</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9288</th>\n",
       "      <td>9.0</td>\n",
       "      <td>725.0</td>\n",
       "      <td>0.536748</td>\n",
       "      <td>19.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.590345</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.246875</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>-0.6000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>9500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14634</th>\n",
       "      <td>8.0</td>\n",
       "      <td>616.0</td>\n",
       "      <td>0.753846</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.683442</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.337976</td>\n",
       "      <td>-0.625000</td>\n",
       "      <td>-0.071429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       n_tokens_title  n_tokens_content  n_non_stop_unique_tokens  num_hrefs   \n",
       "5532              9.0             218.0                  0.843750        2.0  \\\n",
       "15242            11.0             268.0                  0.835616        5.0   \n",
       "35329            14.0             851.0                  0.633858       15.0   \n",
       "9288              9.0             725.0                  0.536748       19.0   \n",
       "14634             8.0             616.0                  0.753846       10.0   \n",
       "\n",
       "       num_self_hrefs  num_imgs  num_videos  average_token_length   \n",
       "5532              1.0       0.0         1.0              4.353211  \\\n",
       "15242             3.0       1.0         0.0              4.492537   \n",
       "35329             6.0       1.0         0.0              4.584019   \n",
       "9288              6.0       1.0         0.0              4.590345   \n",
       "14634             0.0       1.0         0.0              4.683442   \n",
       "\n",
       "       num_keywords  kw_min_min  ...  avg_negative_polarity   \n",
       "5532            7.0         4.0  ...              -0.409524  \\\n",
       "15242           4.0         4.0  ...              -0.118750   \n",
       "35329           7.0        -1.0  ...              -0.307568   \n",
       "9288           10.0         4.0  ...              -0.246875   \n",
       "14634           7.0         4.0  ...              -0.337976   \n",
       "\n",
       "       min_negative_polarity  max_negative_polarity  title_subjectivity   \n",
       "5532               -0.666667              -0.071429                 0.0  \\\n",
       "15242              -0.187500              -0.050000                 0.1   \n",
       "35329              -0.800000              -0.050000                 0.7   \n",
       "9288               -0.500000              -0.100000                 0.9   \n",
       "14634              -0.625000              -0.071429                 0.0   \n",
       "\n",
       "       title_sentiment_polarity  abs_title_subjectivity   \n",
       "5532                     0.0000                     0.5  \\\n",
       "15242                    0.3000                     0.4   \n",
       "35329                    0.1125                     0.2   \n",
       "9288                    -0.6000                     0.4   \n",
       "14634                    0.0000                     0.5   \n",
       "\n",
       "       abs_title_sentiment_polarity  data_channel  weekday_is  shares  \n",
       "5532                         0.0000             2           4    1500  \n",
       "15242                        0.3000             6           3    1100  \n",
       "35329                        0.1125             5           3    1600  \n",
       "9288                         0.6000             6           3    9500  \n",
       "14634                        0.0000             2           5     682  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate x and y together to create dataframe of test set\n",
    "test_df = pd.concat([x_df_test, y_df_test], axis=1, sort=False)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb28d8fc-43ce-459c-b548-92bcce3c4e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependent variable: n_tokens_title\n",
      "R-squared: 0.00015152374394800106\n",
      "Coefficient 0 (Intercept): 2763.7066860374416\n",
      "Coefficient 1 (n_tokens_title): 41.38297036207493\n",
      "\n",
      "Dependent variable: n_tokens_content\n",
      "R-squared: 1.6160891647132303e-07\n",
      "Coefficient 0 (Intercept): 3189.821019878602\n",
      "Coefficient 1 (n_tokens_content): 0.006101657911187552\n",
      "\n",
      "Dependent variable: n_non_stop_unique_tokens\n",
      "R-squared: 4.628901279946085e-06\n",
      "Coefficient 0 (Intercept): 3190.5627214990263\n",
      "Coefficient 1 (n_non_stop_unique_tokens): 3.7546717872647184\n",
      "\n",
      "Dependent variable: num_hrefs\n",
      "R-squared: 0.006000091722718537\n",
      "Coefficient 0 (Intercept): 2651.9294168788438\n",
      "Coefficient 1 (num_hrefs): 48.15881586146364\n",
      "\n",
      "Dependent variable: num_self_hrefs\n",
      "R-squared: 8.405814718170923e-06\n",
      "Coefficient 0 (Intercept): 3175.4156501004113\n",
      "Coefficient 1 (num_self_hrefs): 5.250899905331085\n",
      "\n",
      "Dependent variable: num_imgs\n",
      "R-squared: 0.004309108074243118\n",
      "Coefficient 0 (Intercept): 2937.689088308962\n",
      "Coefficient 1 (num_imgs): 56.20863511898652\n",
      "\n",
      "Dependent variable: num_videos\n",
      "R-squared: 0.00115917659810727\n",
      "Coefficient 0 (Intercept): 3120.828443128116\n",
      "Coefficient 1 (num_videos): 58.40043885523404\n",
      "\n",
      "Dependent variable: average_token_length\n",
      "R-squared: 0.00032622120829928214\n",
      "Coefficient 0 (Intercept): 5293.431183682383\n",
      "Coefficient 1 (average_token_length): -447.88645511163577\n",
      "\n",
      "Dependent variable: num_keywords\n",
      "R-squared: 0.001597033029397621\n",
      "Coefficient 0 (Intercept): 2128.7919707504125\n",
      "Coefficient 1 (num_keywords): 147.56600158969763\n",
      "\n",
      "Dependent variable: kw_min_min\n",
      "R-squared: 7.821276758246398e-06\n",
      "Coefficient 0 (Intercept): 3200.7190246480077\n",
      "Coefficient 1 (kw_min_min): -0.28216907219679377\n",
      "\n",
      "Dependent variable: kw_max_min\n",
      "R-squared: 0.0007584718375316379\n",
      "Coefficient 0 (Intercept): 3132.0645465967427\n",
      "Coefficient 1 (kw_max_min): 0.052987069237177636\n",
      "\n",
      "Dependent variable: kw_min_max\n",
      "R-squared: 1.4247178284909623e-05\n",
      "Coefficient 0 (Intercept): 3187.107324460836\n",
      "Coefficient 1 (kw_min_max): 0.0004680876672045128\n",
      "\n",
      "Dependent variable: kw_max_max\n",
      "R-squared: 0.00015853004169497087\n",
      "Coefficient 0 (Intercept): 2883.638381215187\n",
      "Coefficient 1 (kw_max_max): 0.0004120945952549744\n",
      "\n",
      "Dependent variable: kw_avg_max\n",
      "R-squared: 0.0027614970596326893\n",
      "Coefficient 0 (Intercept): 2473.5590853303765\n",
      "Coefficient 1 (kw_avg_max): 0.002822519902888504\n",
      "\n",
      "Dependent variable: kw_min_avg\n",
      "R-squared: 0.00229685571462801\n",
      "Coefficient 0 (Intercept): 2861.6449487364516\n",
      "Coefficient 1 (kw_min_avg): 0.3007999101907769\n",
      "\n",
      "Dependent variable: kw_max_avg\n",
      "R-squared: 0.008831122613250253\n",
      "Coefficient 0 (Intercept): 2543.3598097754098\n",
      "Coefficient 1 (kw_max_avg): 0.11638521701139752\n",
      "\n",
      "Dependent variable: self_reference_min_shares\n",
      "R-squared: 0.0038962366980348495\n",
      "Coefficient 0 (Intercept): 3100.104721201278\n",
      "Coefficient 1 (self_reference_min_shares): 0.022840463860790777\n",
      "\n",
      "Dependent variable: self_reference_max_shares\n",
      "R-squared: 0.0033943264041240706\n",
      "Coefficient 0 (Intercept): 3089.086281614382\n",
      "Coefficient 1 (self_reference_max_shares): 0.009628639269545828\n",
      "\n",
      "Dependent variable: self_reference_avg_sharess\n",
      "R-squared: 0.004454717811236186\n",
      "Coefficient 0 (Intercept): 3065.731752374003\n",
      "Coefficient 1 (self_reference_avg_sharess): 0.01918277829238244\n",
      "\n",
      "Dependent variable: LDA_00\n",
      "R-squared: 9.616717046390111e-05\n",
      "Coefficient 0 (Intercept): 3242.556477339591\n",
      "Coefficient 1 (LDA_00): -259.17676809714766\n",
      "\n",
      "Dependent variable: LDA_01\n",
      "R-squared: 9.513129292826328e-05\n",
      "Coefficient 0 (Intercept): 3237.3756685933304\n",
      "Coefficient 1 (LDA_01): -314.2348872647856\n",
      "\n",
      "Dependent variable: LDA_02\n",
      "R-squared: 0.00708074831612604\n",
      "Coefficient 0 (Intercept): 3651.5914491730146\n",
      "Coefficient 1 (LDA_02): -2103.0231335838794\n",
      "\n",
      "Dependent variable: LDA_03\n",
      "R-squared: 0.01360311423512861\n",
      "Coefficient 0 (Intercept): 2583.0181155808004\n",
      "Coefficient 1 (LDA_03): 2861.20874970731\n",
      "\n",
      "Dependent variable: LDA_04\n",
      "R-squared: 0.00030134743968746047\n",
      "Coefficient 0 (Intercept): 3293.5306911090665\n",
      "Coefficient 1 (LDA_04): -421.04967363747096\n",
      "\n",
      "Dependent variable: global_subjectivity\n",
      "R-squared: 0.009733678847421823\n",
      "Coefficient 0 (Intercept): -430.4446250680123\n",
      "Coefficient 1 (global_subjectivity): 7936.35831784634\n",
      "\n",
      "Dependent variable: global_sentiment_polarity\n",
      "R-squared: 0.00022116305102160005\n",
      "Coefficient 0 (Intercept): 3057.9778371680504\n",
      "Coefficient 1 (global_sentiment_polarity): 1097.651101425991\n",
      "\n",
      "Dependent variable: global_rate_positive_words\n",
      "R-squared: 0.0002322659545516137\n",
      "Coefficient 0 (Intercept): 2920.3837415352073\n",
      "Coefficient 1 (global_rate_positive_words): 6681.068935942241\n",
      "\n",
      "Dependent variable: global_rate_negative_words\n",
      "R-squared: 0.0005961799695475278\n",
      "Coefficient 0 (Intercept): 2913.3725985746\n",
      "Coefficient 1 (global_rate_negative_words): 16352.290089652579\n",
      "\n",
      "Dependent variable: rate_positive_words\n",
      "R-squared: 8.418653993469416e-05\n",
      "Coefficient 0 (Intercept): 3498.0439720167333\n",
      "Coefficient 1 (rate_positive_words): -433.31803582090515\n",
      "\n",
      "Dependent variable: rate_negative_words\n",
      "R-squared: 8.583326580091644e-05\n",
      "Coefficient 0 (Intercept): 3063.3592341666545\n",
      "Coefficient 1 (rate_negative_words): 438.15491174411943\n",
      "\n",
      "Dependent variable: avg_positive_polarity\n",
      "R-squared: 0.0027207899956490644\n",
      "Coefficient 0 (Intercept): 1620.7102624087784\n",
      "Coefficient 1 (avg_positive_polarity): 4307.70344243607\n",
      "\n",
      "Dependent variable: min_positive_polarity\n",
      "R-squared: 0.00030858494869789865\n",
      "Coefficient 0 (Intercept): 3020.7416994383616\n",
      "Coefficient 1 (min_positive_polarity): 1750.5863405190375\n",
      "\n",
      "Dependent variable: max_positive_polarity\n",
      "R-squared: 0.0013075524026625596\n",
      "Coefficient 0 (Intercept): 2252.8692785925455\n",
      "Coefficient 1 (max_positive_polarity): 1205.1918031755995\n",
      "\n",
      "Dependent variable: avg_negative_polarity\n",
      "R-squared: 0.0036030529510824305\n",
      "Coefficient 0 (Intercept): 2258.8852348514874\n",
      "Coefficient 1 (avg_negative_polarity): -3496.8540597111296\n",
      "\n",
      "Dependent variable: min_negative_polarity\n",
      "R-squared: 0.0017330748687286457\n",
      "Coefficient 0 (Intercept): 2628.7195003494608\n",
      "Coefficient 1 (min_negative_polarity): -1050.0873043784572\n",
      "\n",
      "Dependent variable: max_negative_polarity\n",
      "R-squared: 0.0007739818092667994\n",
      "Coefficient 0 (Intercept): 2964.0642997333803\n",
      "Coefficient 1 (max_negative_polarity): -2073.7589360701504\n",
      "\n",
      "Dependent variable: title_subjectivity\n",
      "R-squared: 0.0019803986610165136\n",
      "Coefficient 0 (Intercept): 2920.647900607446\n",
      "Coefficient 1 (title_subjectivity): 971.3835981890477\n",
      "\n",
      "Dependent variable: title_sentiment_polarity\n",
      "R-squared: 0.00127057214954307\n",
      "Coefficient 0 (Intercept): 3124.4450398638155\n",
      "Coefficient 1 (title_sentiment_polarity): 951.7083139953286\n",
      "\n",
      "Dependent variable: abs_title_subjectivity\n",
      "R-squared: 7.424576420334006e-08\n",
      "Coefficient 0 (Intercept): 3196.7474067459325\n",
      "Coefficient 1 (abs_title_subjectivity): -10.1901406162544\n",
      "\n",
      "Dependent variable: abs_title_sentiment_polarity\n",
      "R-squared: 0.0026217984032076425\n",
      "Coefficient 0 (Intercept): 2944.898700678149\n",
      "Coefficient 1 (abs_title_sentiment_polarity): 1599.7811307287889\n",
      "\n",
      "Dependent variable: data_channel\n",
      "R-squared: 0.002264934887315939\n",
      "Coefficient 0 (Intercept): 2414.731452360408\n",
      "Coefficient 1 (data_channel): 178.8437081405026\n",
      "\n",
      "Dependent variable: weekday_is\n",
      "R-squared: 9.864917034185794e-05\n",
      "Coefficient 0 (Intercept): 3058.0390089459015\n",
      "Coefficient 1 (weekday_is): 39.786922780479564\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use Statsmodel to create simple linear regression model of shares based on a single predictor \n",
    "# train model on the training set \n",
    "\n",
    "# Loop through predictors and fit linear models\n",
    "for i in predictors:\n",
    "    lin_model = smf.ols(formula=f'shares ~ {i}', data=train_df).fit()\n",
    "    print(f\"Dependent variable: {i}\")\n",
    "    print(f\"R-squared: {lin_model.rsquared}\")\n",
    "    print(f\"Coefficient 0 (Intercept): {lin_model.params[0]}\")\n",
    "    print(f\"Coefficient 1 ({i}): {lin_model.params[1]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1fb19c3-62d1-4628-8f58-9c3dcca39d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LDA_03_model = smf.ols(formula=f'shares~ LDA_03', data=train_df).fit()\n",
    "sm.qqplot(LDA_03_model.resid, line='s');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5412047f-49b6-4b4b-8806-5390969970e9",
   "metadata": {},
   "source": [
    "#### Summary of Results & Analysis: Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd7640b-231d-41c3-a9e9-1fb487d8296f",
   "metadata": {},
   "source": [
    "R-squared is important because it measures how well the regression model fits the data. A higher R-squared value, closer to 1, signifies that a larger proportion of the variance in the dependent variable is explained by the independent variables, indicating a better fit. Conversely, an R-squared value closer to 0 indicates that the model's explanatory power is limited, or that the variance in the dependent variable is largely unaccounted for. \n",
    "\n",
    "In this case, the highest R-squared is 0.014 for the model with the LDA_03 predictor. Most of the models produced R-squared values of 0.000 suggesting that these models do not adequately capture variability in the target variable 'shares' based solely on the individual predictor variables. This limited explanatory power may arise due to the presence of multicollinearity or the assumption of a linear relationship not holding true.\n",
    "\n",
    "**Evaluating Regression: Diagnostic Plot:**   \n",
    "The model predicting shares based on LDA_03 had the highest R-squared value of 0.014. This value is nowhere near 1. Another check that I can do is for residuals which should be normally distributed. If the residuals are not normally distributed, it’s likely that errors can be introduced into the model that would not be made clear by R-squared. Below, I checked the residuals against the theoretical quantiles of the normal distribution for the model with LDA_03 as the predictor. The line is not linear and diagonal as expected if the residuals were normally distributed so this model may not be a good fit.\n",
    "\n",
    "**Next steps:**    \n",
    "Since I've explored simple linear regression, the next step is to try multilinear regression. This allows me to consider the relationships between multiple predictors and the target variable simultaneously. It might be that there are combined effects of predictors that a simple linear regression can't capture. Linear regression creates a line, whereas multilinear regression creates a hyperplane. This broader scope allows us to comprehend complex interactions that may not be adequately captured by a linear model. \n",
    "\n",
    "A critical consideration in multilinear regression is the presence of correlated predictors. Correlation among predictors can lead to multicollinearity, which may hinder the accurate interpretation of individual predictor impacts. To mitigate this, I have already taken steps during EDA to remove strongly correlated predictors. This preprocessing aims to enhance the stability and interpretability of the forthcoming multilinear regression model.\n",
    "\n",
    "In the subsequent analysis, I will construct and evaluate a multilinear regression model, considering both the individual and combined contributions of predictors to the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5528925-dc42-4592-bb66-df49c2158ec8",
   "metadata": {},
   "source": [
    "### Multilinear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8817587-a63c-4c24-8182-7d237893c0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn list of predictors into string with plus signs that can be entered as model formula\n",
    "def list_to_string(predictor_array):\n",
    "    listToStr = '+'.join(map(str, predictor_array))\n",
    "    print(listToStr)\n",
    "    return listToStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aaac1daf-a97b-47ce-98ad-23b1bb67482c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_tokens_title+n_tokens_content+n_non_stop_unique_tokens+num_hrefs+num_self_hrefs+num_imgs+num_videos+average_token_length+num_keywords+kw_min_min+kw_max_min+kw_min_max+kw_max_max+kw_avg_max+kw_min_avg+kw_max_avg+self_reference_min_shares+self_reference_max_shares+self_reference_avg_sharess+LDA_00+LDA_01+LDA_02+LDA_03+LDA_04+global_subjectivity+global_sentiment_polarity+global_rate_positive_words+global_rate_negative_words+rate_positive_words+rate_negative_words+avg_positive_polarity+min_positive_polarity+max_positive_polarity+avg_negative_polarity+min_negative_polarity+max_negative_polarity+title_subjectivity+title_sentiment_polarity+abs_title_subjectivity+abs_title_sentiment_polarity+data_channel+weekday_is\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 shares   R-squared:                       0.038\n",
      "Model:                            OLS   Adj. R-squared:                  0.037\n",
      "Method:                 Least Squares   F-statistic:                     24.28\n",
      "Date:                Fri, 11 Aug 2023   Prob (F-statistic):          1.82e-182\n",
      "Time:                        11:33:15   Log-Likelihood:            -2.6431e+05\n",
      "No. Observations:               25756   AIC:                         5.287e+05\n",
      "Df Residuals:                   25713   BIC:                         5.291e+05\n",
      "Df Model:                          42                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "================================================================================================\n",
      "                                   coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------------------\n",
      "Intercept                    -8.402e+05   3.88e+05     -2.166      0.030    -1.6e+06   -7.99e+04\n",
      "n_tokens_title                  62.4085     21.433      2.912      0.004      20.399     104.418\n",
      "n_tokens_content                -0.1201      0.147     -0.817      0.414      -0.408       0.168\n",
      "n_non_stop_unique_tokens      1299.5546    596.276      2.179      0.029     130.821    2468.289\n",
      "num_hrefs                       39.0351      4.847      8.053      0.000      29.534      48.536\n",
      "num_self_hrefs                 -63.0143     12.852     -4.903      0.000     -88.206     -37.823\n",
      "num_imgs                        28.1012      6.667      4.215      0.000      15.033      41.169\n",
      "num_videos                       2.5467     11.676      0.218      0.827     -20.338      25.432\n",
      "average_token_length          -534.5721    170.166     -3.141      0.002    -868.107    -201.038\n",
      "num_keywords                   121.6709     27.051      4.498      0.000      68.649     174.693\n",
      "kw_min_min                       2.2418      1.196      1.875      0.061      -0.102       4.586\n",
      "kw_max_min                      -0.0303      0.015     -2.025      0.043      -0.060      -0.001\n",
      "kw_min_max                      -0.0018      0.001     -2.069      0.039      -0.004   -9.63e-05\n",
      "kw_max_max                       0.0001      0.000      0.293      0.770      -0.001       0.001\n",
      "kw_avg_max                       0.0014      0.001      2.432      0.015       0.000       0.002\n",
      "kw_min_avg                       0.1378      0.044      3.116      0.002       0.051       0.224\n",
      "kw_max_avg                       0.0750      0.010      7.395      0.000       0.055       0.095\n",
      "self_reference_min_shares        0.0218      0.005      4.196      0.000       0.012       0.032\n",
      "self_reference_max_shares        0.0075      0.003      2.612      0.009       0.002       0.013\n",
      "self_reference_avg_sharess      -0.0120      0.007     -1.681      0.093      -0.026       0.002\n",
      "LDA_00                        8.386e+05   3.87e+05      2.165      0.030    7.93e+04     1.6e+06\n",
      "LDA_01                        8.381e+05   3.87e+05      2.164      0.031    7.88e+04     1.6e+06\n",
      "LDA_02                        8.366e+05   3.87e+05      2.160      0.031    7.74e+04     1.6e+06\n",
      "LDA_03                        8.389e+05   3.87e+05      2.166      0.030    7.96e+04     1.6e+06\n",
      "LDA_04                        8.382e+05   3.87e+05      2.164      0.030    7.89e+04     1.6e+06\n",
      "global_subjectivity           3487.7131    627.943      5.554      0.000    2256.910    4718.516\n",
      "global_sentiment_polarity      603.7445   1246.180      0.484      0.628   -1838.839    3046.327\n",
      "global_rate_positive_words   -3996.3570   5335.318     -0.749      0.454   -1.45e+04    6461.165\n",
      "global_rate_negative_words   -3178.5984   1.02e+04     -0.313      0.755   -2.31e+04    1.68e+04\n",
      "rate_positive_words            857.4127   4021.585      0.213      0.831   -7025.119    8739.945\n",
      "rate_negative_words           1864.1969   4058.495      0.459      0.646   -6090.682    9819.076\n",
      "avg_positive_polarity        -1030.9889   1012.679     -1.018      0.309   -3015.896     953.918\n",
      "min_positive_polarity         -528.1581    826.258     -0.639      0.523   -2147.671    1091.355\n",
      "max_positive_polarity          121.5099    314.750      0.386      0.699    -495.418     738.437\n",
      "avg_negative_polarity        -1167.6483    925.681     -1.261      0.207   -2982.034     646.738\n",
      "min_negative_polarity          -87.6383    335.671     -0.261      0.794    -745.572     570.295\n",
      "max_negative_polarity          -25.7545    766.229     -0.034      0.973   -1527.606    1476.097\n",
      "title_subjectivity             192.7572    205.897      0.936      0.349    -210.812     596.326\n",
      "title_sentiment_polarity       695.0430    188.118      3.695      0.000     326.322    1063.764\n",
      "abs_title_subjectivity         874.1610    273.027      3.202      0.001     339.013    1409.310\n",
      "abs_title_sentiment_polarity   500.3675    296.817      1.686      0.092     -81.410    1082.145\n",
      "data_channel                   203.7964     28.777      7.082      0.000     147.392     260.201\n",
      "weekday_is                       1.7324     24.682      0.070      0.944     -46.646      50.111\n",
      "==============================================================================\n",
      "Omnibus:                    38057.511   Durbin-Watson:                   1.994\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):         15780317.120\n",
      "Skew:                           9.100   Prob(JB):                         0.00\n",
      "Kurtosis:                     122.888   Cond. No.                     1.82e+10\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.82e+10. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "# Create a multilinear model that includes all predictors \n",
    "# Find coefficients that reduce the sum of least squares \n",
    "ml_model = smf.ols(formula='shares ~ {}'.format(list_to_string(predictors)), data=train_df).fit()\n",
    "print(ml_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3793594c-1bfa-41fb-bfb3-50c7e670ee2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlgAAAGwCAYAAAB1mRuuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABhd0lEQVR4nO3deVyU9fYH8M8MMuyLKKsgImqK4ooCZi5J4lp4K9OscMnS3KlcytxKLW0xy63b72qLppnbdYlC3FJxAUREFDcQZREUYQRZZ57fH9wZGYdlBmcYBj7v12te15nnzDNnsOLc7/k+5xEJgiCAiIiIiHRGbOgEiIiIiBoaFlhEREREOsYCi4iIiEjHWGARERER6RgLLCIiIiIdY4FFREREpGMssIiIiIh0rImhE2is5HI50tPTYWNjA5FIZOh0iIiISAOCIODhw4dwc3ODWFz1OhULLANJT0+Hh4eHodMgIiKiWrh9+zbc3d2rPM4Cy0BsbGwAlP8F2draGjgbIiIi0oRUKoWHh4fy93hVWGAZiKItaGtrywKLiIjIyNS0vYeb3ImIiIh0jAUWERERkY6xwCIiIiLSMRZYRERERDrGAouIiIhIx1hgEREREekYCywiIiIiHWOBRURERKRjLLCIiIiIdIyT3ImIiKhRk8kFnE3OQdbDIjjZmKOXlwNMxNVPaq8JCywiIiJqtMITMrBkXyIy8oqUr7namWPRCB8M7uRa6/OyRUhEREQNkkwuIOrGfeyNS0PUjfuQyQWV4+EJGZjya6xKcQUAmXlFmPJrLMITMmr92VzBIiIiIqNWWYsvIjGz2pUpmVzAkn2JECo5nwBABGDJvkS84ONSq3YhCywiIiIyGk8WUw8KSvDpAdVCyt7SFLmPStXeq1iZWv9Gd9hZSNRWrioSAGTkFeFscg4CvZtpnScLLCIiIjIKle2XqkxlxRWgujI1Z3B7jT4z62H1n1UVFlhERERU7yn2S1XW0tOGYmUqJ79Yo3gnG/NafQ4LLCIiIqp3KrYCHSwk+Gh3wlMXVxU5WEngameOzLyiSs8rAuBiV76fqzZYYBEREVG9omkr8Gm42Flg0QgfTPk1FiJApchSbGlfNMKn1vOwOKaBiIiI6gWZXMC3h65hciWjE3RFhPKrCXt5OWBwJ1esf6M7XOxU24AuduZY/0b3p5qDxRUsIiIiMrjwhAws/u8lZEo12xtVG5WtTA3u5IoXfFw4yZ2IiIgaFl1tYFdQtPyeHNfgUsWEdhOxqFajGKrDAouIiIgMprqBn7WlKKT0sTKlKRZYREREZDBnk3Oear9VMysJPn2pI5pamVVaSOl6ZUpTLLCIiIjIYGo7yBMAHKxMETV/ICRN6t81eyywiIiIyGBqM8hT0eRbPtK3XhZXAMc0EBERkQH18nKAq505tNkZpYsxCvrGFSwiIiIyGBOxqMqBnwozB7ZBL69muJdfXOeb1WuLBRYREREZlGLg55PT212rGKtgDFhgERERkcHpa+CnobDAIiIionpBHwM/DYWb3ImIiIh0jAUWERERkY6xRUhEREQGJZMLDWbvlQILLCIiIjKY8ISMBnX1oAJbhERERGQQ4QkZmPJrrNq9CDPzijDl11iEJ2QYKLOnxwKLiIiI6pxMLmDJvsRKB4sqXluyLxEyeWUR9R8LLCIiIqpzZ5Nz1FauKhIAZOQV4WxyTt0lpUMssIiIiKjOZT2suriqTVx9wwKLiIiI6pyTjblO4+obFlhERERU53p5OcDVzhxVDWMQofxqwl5eDnWZls7UmwLr+PHjGDFiBNzc3CASibBnzx7lsdLSUsydOxe+vr6wsrKCm5sb3nrrLaSnp6uco1WrVhCJRCqPzz//XCUmPj4ezz33HMzNzeHh4YGVK1eq5bJjxw60b98e5ubm8PX1xcGDB1WOC4KAhQsXwtXVFRYWFggKCsK1a9d098MgIiJq4EzEIiwa4QMAakWW4vmiET5GOw+r3hRYBQUF6NKlC9auXat27NGjR4iNjcUnn3yC2NhY7Nq1C0lJSXjxxRfVYpcuXYqMjAzlY/r06cpjUqkUgwYNgqenJ2JiYrBq1SosXrwYP/zwgzLm1KlTGDNmDCZOnIjz588jJCQEISEhSEhIUMasXLkSa9aswYYNG3DmzBlYWVkhODgYRUXG2ScmIiIyhMGdXLH+je5wsVNtA7rYmWP9G92Neg6WSBCEenf9o0gkwu7duxESElJlzLlz59CrVy/cunULLVu2BFC+gjVr1izMmjWr0vesX78eH3/8MTIzMyGRSAAA8+bNw549e3DlyhUAwGuvvYaCggLs379f+b6AgAB07doVGzZsgCAIcHNzw/vvv48PPvgAAJCXlwdnZ2ds3rwZo0ePrvSzi4uLUVxcrHwulUrh4eGBvLw82NraavyzISIiaigUE9wz8wqRU1ACB2szuNjW70nuUqkUdnZ2Nf7+rjcrWNrKy8uDSCSCvb29yuuff/45mjVrhm7dumHVqlUoKytTHouKikLfvn2VxRUABAcHIykpCQ8ePFDGBAUFqZwzODgYUVFRAIDk5GRkZmaqxNjZ2cHf318ZU5kVK1bAzs5O+fDw8Kj1dyciIjJ24QkZ6PPFYYz592nM/v0CPj1wGSvDryCvsKTeFlfaMMoCq6ioCHPnzsWYMWNUqscZM2Zg27ZtOHLkCN59910sX74cc+bMUR7PzMyEs7OzyrkUzzMzM6uNqXi84vsqi6nM/PnzkZeXp3zcvn1b269NRETUIDTkCe4KRncvwtLSUowaNQqCIGD9+vUqx8LCwpR/7ty5MyQSCd59912sWLECZmZmdZ2qCjMzM4PnQEREZGg1TXAXoXyC+ws+Lka9kmVUK1iK4urWrVuIiIioce+Sv78/ysrKkJKSAgBwcXHB3bt3VWIUz11cXKqNqXi84vsqiyEiIqLKNfQJ7gpGU2Apiqtr167h0KFDaNasWY3viYuLg1gshpOTEwAgMDAQx48fR2lpqTImIiICzzzzDJo2baqMiYyMVDlPREQEAgMDAQBeXl5wcXFRiZFKpThz5owyhoiIiCrX0Ce4K9SbFmF+fj6uX7+ufJ6cnIy4uDg4ODjA1dUVr7zyCmJjY7F//37IZDLlficHBwdIJBJERUXhzJkzGDBgAGxsbBAVFYXZs2fjjTfeUBZPr7/+OpYsWYKJEydi7ty5SEhIwLfffotvvvlG+bkzZ85Ev3798NVXX2HYsGHYtm0boqOjlaMcRCIRZs2ahc8++wxt27aFl5cXPvnkE7i5uVV71SMREREBydkFGsUZ6wR3hXozpuHo0aMYMGCA2uuhoaFYvHgxvLy8Kn3fkSNH0L9/f8TGxuK9997DlStXUFxcDC8vL7z55psICwtT2fsUHx+PqVOn4ty5c2jevDmmT5+OuXPnqpxzx44dWLBgAVJSUtC2bVusXLkSQ4cOVR4XBAGLFi3CDz/8gNzcXPTp0wfr1q1Du3btNP6+ml7mSURE1FCsOJiIjceTq40RoXwO1om5z9fLPVia/v6uNwVWY8MCi4iIGpOD8el4b+v5GuNEQL0eMtrg52ARERGRcZDJBSzYm1BzIIBZQe3qbXGlDRZYREREpFdnk3OQU1BacyCAVs0t9ZxN3WCBRURERHqlzRWBxr65XYEFFhEREemVpkVTMysJenk56DmbusECi4iIiPSql5cDXO1qLrI+falTvbxysDZYYBEREZFemYhFWDTCB9WVTu/29cLQzsa/uV2BBRYRERHp3eBOrlj/Rne1lSwHK1Ose70b5g/1MVBm+lFvJrkTERFRwza4kyte8HHB2eQcZD0sgpONOXp5OTSYtmBFLLCIiIiozpiIRQj0rvl+wsaOLUIiIiIiHWOBRURERKRjLLCIiIiIdIwFFhEREZGOscAiIiIi0jEWWEREREQ6xgKLiIiISMdYYBERERHpGAssIiIiIh1jgUVERESkYyywiIiIiHSMBRYRERGRjrHAIiIiItKxJoZOgIiIiBo2mVzA6Rv3EXXzHgARAr2bIaB1M5iIRYZOTW9YYBEREZHehCdkYN6ui8h9VKp87fsj12FvaYrP/+WLwZ1cDZid/rBFSERERHoRnpCByb/GqhRXCrmPSjH511iEJ2QYIDP9Y4FFREREOieTC5i7M77GuCX7EiGTC3WQUd1igUVEREQ6N+O3WOQVltUYl5FXhLPJOXWQUd1igUVEREQ6tezAJRy4mKlxfNbDIj1mYxjc5E5EREQ6IZML+PbQVfz7nxSt3udkY66fhAyIBRYRERE9tfCEDMz9Ix55RTW3BStytTNHLy8HPWVlOCywiIiI6KkorhasjUUjfBrkPCzuwSIiIqJa0/RqwSeJAKx7vTvnYBERERE96fTN+xpdLfik70Z3w9DODbO4AlhgERER0VOIunFf6/dMes4Lw7u66SGb+oMFFhERET0F7YaETnquFT4e5qOnXOoPFlhERERUa4Gtm2sc+/3obvh4WEc9ZlN/sMAiIiKiWgvwbgZ7S9Ma474f3a3BtwUrYoFFREREtWYiFuHzf/lWG/Nu34a/5+pJnINFRERET0UuB6zNmiC/WPVqQiuJGKte6YKhnRtXcQXUoxWs48ePY8SIEXBzc4NIJMKePXtUjguCgIULF8LV1RUWFhYICgrCtWvXVGJycnIwduxY2Nrawt7eHhMnTkR+fr5KTHx8PJ577jmYm5vDw8MDK1euVMtlx44daN++PczNzeHr64uDBw9qnQsREVFjsOxAIt7bGqtWXAFAQYkc4gY4RFQT9abAKigoQJcuXbB27dpKj69cuRJr1qzBhg0bcObMGVhZWSE4OBhFRY9vEDl27FhcunQJERER2L9/P44fP4533nlHeVwqlWLQoEHw9PRETEwMVq1ahcWLF+OHH35Qxpw6dQpjxozBxIkTcf78eYSEhCAkJAQJCQla5UJERNTQLTtwCf/+J7namCX7EiGTa3elYUMgEgSh3n1rkUiE3bt3IyQkBED5ipGbmxvef/99fPDBBwCAvLw8ODs7Y/PmzRg9ejQuX74MHx8fnDt3Dn5+fgCA8PBwDB06FHfu3IGbmxvWr1+Pjz/+GJmZmZBIJACAefPmYc+ePbhy5QoA4LXXXkNBQQH279+vzCcgIABdu3bFhg0bNMpFE1KpFHZ2dsjLy4Otra1Ofm5ERER15WB8Ot7bel6j2N8mBSDQu5meM6obmv7+rjcrWNVJTk5GZmYmgoKClK/Z2dnB398fUVFRAICoqCjY29sriysACAoKglgsxpkzZ5Qxffv2VRZXABAcHIykpCQ8ePBAGVPxcxQxis/RJJfKFBcXQyqVqjyIiIiMkUwuYMHehJoD/yfrYePr8BhFgZWZmQkAcHZ2Vnnd2dlZeSwzMxNOTk4qx5s0aQIHBweVmMrOUfEzqoqpeLymXCqzYsUK2NnZKR8eHh41fGsiIqL66WxyDnIKSjWOd7Ix12M29ZNRFFgNwfz585GXl6d83L5929ApERER1Yo2K1LNrCTo5eWgx2zqJ6MosFxcXAAAd+/eVXn97t27ymMuLi7IyspSOV5WVoacnByVmMrOUfEzqoqpeLymXCpjZmYGW1tblQcREZEx0mZF6tOXOsGkEV5JaBQFlpeXF1xcXBAZGal8TSqV4syZMwgMDAQABAYGIjc3FzExMcqYw4cPQy6Xw9/fXxlz/PhxlJY+XtaMiIjAM888g6ZNmypjKn6OIkbxOZrkQkRE1JA9KCiGSIOaadJzXhja2VX/CdVD9abAys/PR1xcHOLi4gCUbyaPi4tDamoqRCIRZs2ahc8++wz//e9/cfHiRbz11ltwc3NTXmnYoUMHDB48GJMmTcLZs2dx8uRJTJs2DaNHj4abW/mAs9dffx0SiQQTJ07EpUuXsH37dnz77bcICwtT5jFz5kyEh4fjq6++wpUrV7B48WJER0dj2rRpAKBRLkRERA1VeEIG3tt6HjXNIGgsN3WuSr0Z03D06FEMGDBA7fXQ0FBs3rwZgiBg0aJF+OGHH5Cbm4s+ffpg3bp1aNeunTI2JycH06ZNw759+yAWi/Hyyy9jzZo1sLa2VsbEx8dj6tSpOHfuHJo3b47p06dj7ty5Kp+5Y8cOLFiwACkpKWjbti1WrlyJoUOHKo9rkktNOKaBiIiMjUwu4NnPI5EpLa42zs6iCWI/GdQgW4Oa/v6uNwVWY8MCi4iIjE3UjfsY8+/TGsU2pNlXFTWoOVhERERkeD8cv65xbGOcfVURCywiIiKq0cH4dBxJuqdxfGOcfVURCywiIiKqlkwu4MOd8RrH21uYNsrZVxWxwCIiIqJqnb55HwXFMo3jxz/r1SA3uGuDBRYRERFVK+rGfY1jrc2aYNrzbfSYjXFggUVEREQ10HzgwMqXOzf61SuABRYRERHVIOVevkZxw31dG+3k9iexwCIiIqIqHYxPx/6Ld2uMszAV49sx3eogI+PAAouIiIgqpc3Vg5P7tWFrsAIWWERERFSp7w9f0/jqwVbNLfWcjXFhgUVERERqZHIB/zmZrHF8Yx8s+iQWWERERKTmbHIO8grLNIq1NW/S6AeLPokFFhEREanJlGp+L8GXu7fg/qsnsMAiIiIiNSevZWscO6gjRzM8iQUWERERqZDJBRy4mKFRLO87WDkWWERERKTi9M37KCyVaxTL+w5WjgUWERERqdD03oPmTcS872AVWGARERHREzS79+CA9o5cvaoCCywiIiJSEdi6uUZxb/i30m8iRowFFhEREanIKyypMcbe0hQB3s3qIBvjxAKLiIiIlGRyAZ8euFxj3PIQX7YHq8ECi4iIiJTOJucgI6/mIaNNrSR1kI3x0rrAio2NxcWLF5XP9+7di5CQEHz00UcoKal5SZGIiIjqL00nuGsz6b0x0rrAevfdd3H16lUAwM2bNzF69GhYWlpix44dmDNnjs4TJCIiorqj6QT3nPxiPWdi3LQusK5evYquXbsCAHbs2IG+ffti69at2Lx5M3bu3Knr/IiIiKiOyOQCIhLvahTrwBZhtbQusARBgFxePt310KFDGDp0KADAw8MD9+7d0212REREVGfOJucgr6hMo1gXOws9Z2PctC6w/Pz88Nlnn+GXX37BsWPHMGzYMABAcnIynJ2ddZ4gERER1Y2sh5rtq7K35P0Ha6J1gbV69WrExsZi2rRp+Pjjj9GmTfmI/D/++AO9e/fWeYJERERUN5pbm2kUNy6wFUc01KCJtm/o3LmzylWECqtWrYKJiYlOkiIiIqK6dzZZs3sQ9mzF1aua1GoOVm5uLn788UfMnz8fOTk5AIDExERkZWXpNDkiIiKqGzK5gJ9O3dIo9l4BryCsidYrWPHx8Rg4cCDs7e2RkpKCSZMmwcHBAbt27UJqaip+/vlnfeRJREREenQ2OQe5haUaxTrZmOs5G+On9QpWWFgYxo8fj2vXrsHc/PEPeOjQoTh+/LhOkyMiIqK6oengUHsLbnDXhNYF1rlz5/Duu++qvd6iRQtkZmbqJCkiIiKqW5oOGA3q4MQN7hrQusAyMzODVCpVe/3q1atwdHTUSVJERERUd7QZMPpsm+Z6zqZh0LrAevHFF7F06VKUlpb3aUUiEVJTUzF37ly8/PLLOk+QiIiI9IsDRnVP6wLrq6++Qn5+PpycnFBYWIh+/fqhTZs2sLGxwbJly/SRIxEREekR91/pntZXEdrZ2SEiIgInTpxAfHw88vPz0b17dwQFBekjPyIiItIzTW/czP1XmtO6wFLo06cP+vTpo8tciIiIyAA0vXEz919pTqMCa82aNRqfcMaMGbVOhoiIiOqepvuquP9KcxoVWN98841GJxOJRHorsFq1aoVbt9QnzL733ntYu3Yt+vfvj2PHjqkce/fdd7Fhwwbl89TUVEyZMgVHjhyBtbU1QkNDsWLFCjRp8vjHcPToUYSFheHSpUvw8PDAggULMG7cOJXzrl27FqtWrUJmZia6dOmC7777Dr169dLtFyYiIqojDzSYzO5qZ879V1rQqMBKTk7Wdx41OnfuHGQymfJ5QkICXnjhBbz66qvK1yZNmoSlS5cqn1taWir/LJPJMGzYMLi4uODUqVPIyMjAW2+9BVNTUyxfvhxA+fccNmwYJk+ejC1btiAyMhJvv/02XF1dERwcDADYvn07wsLCsGHDBvj7+2P16tUIDg5GUlISnJyc9P1jICIi0imZXMCnBy7XGPfJMB/uv9KCSBAEwdBJ1MasWbOwf/9+XLt2DSKRCP3790fXrl2xevXqSuP//PNPDB8+HOnp6XB2dgYAbNiwAXPnzkV2djYkEgnmzp2LAwcOICEhQfm+0aNHIzc3F+Hh4QAAf39/9OzZE99//z0AQC6Xw8PDA9OnT8e8efM0zl8qlcLOzg55eXmwtbWt5U+BiIjo6UTduI8x/z5dY9xvkwIQ6N2sDjKq3zT9/a3RClZYWBg+/fRTWFlZISwsrNrYr7/+WrtMa6GkpAS//vorwsLCIBI9rqa3bNmCX3/9FS4uLhgxYgQ++eQT5SpWVFQUfH19lcUVAAQHB2PKlCm4dOkSunXrhqioKLWrIYODgzFr1izl58bExGD+/PnK42KxGEFBQYiKiqo25+LiYhQXP16CrWxYKxERUV3LeqjZiAZN46icRgXW+fPnlYNFz58/r9eENLFnzx7k5uaq7I16/fXX4enpCTc3N8THx2Pu3LlISkrCrl27AACZmZkqxRUA5XPFLX6qipFKpSgsLMSDBw8gk8kqjbly5Uq1Oa9YsQJLliyp1fclIiLSl+bWZjqNo3IaFVhHjhyp9M+G8n//938YMmQI3NzclK+98847yj/7+vrC1dUVAwcOxI0bN+Dt7W2INFXMnz9fZfVPKpXCw8PDgBkREREB0HSjkFFuKDIcrSe5T5gwAQ8fPlR7vaCgABMmTNBJUtW5desWDh06hLfffrvaOH9/fwDA9evXAQAuLi64e1f1PkuK5y4uLtXG2NrawsLCAs2bN4eJiUmlMYpzVMXMzAy2trYqDyIiIkM7fEWzexDe0+BKQ3pM6wLrp59+QmFhodrrhYWF+Pnnn3WSVHU2bdoEJycnDBs2rNq4uLg4AICrqysAIDAwEBcvXkRWVpYyJiIiAra2tvDx8VHGREZGqpwnIiICgYGBAACJRIIePXqoxMjlckRGRipjiIiIjIVMLuDnKPURSJVxsjHXczYNi8aT3KVSKQRBgCAIePjwIczNH/+gZTIZDh48qPcxBXK5HJs2bUJoaKjK7KobN25g69atGDp0KJo1a4b4+HjMnj0bffv2RefOnQEAgwYNgo+PD958802sXLkSmZmZWLBgAaZOnQozs/K+8uTJk/H9999jzpw5mDBhAg4fPozff/8dBw4cUH5WWFgYQkND4efnh169emH16tUoKCjA+PHj9frdiYiIdO3VDSdRKq+59+dgJeEMLC1pXGDZ29tDJBJBJBKhXbt2asdFIpHeN3EfOnQIqampaq1IiUSCQ4cOKYsdDw8PvPzyy1iwYIEyxsTEBPv378eUKVMQGBgIKysrhIaGqszN8vLywoEDBzB79mx8++23cHd3x48//qicgQUAr732GrKzs7Fw4UJkZmaia9euCA8PV9v4TkREVJ/tj0tDbGqeRrG9vJpyBpaWNJ6DdezYMQiCgOeffx47d+6Eg8PjSlYikSiv4CPNcA4WEREZikwuoMuSv5BfLKs5GEBIVzesHt1Nz1kZB53OwQKAfv36ASifdu7h4QGxWOvtW0RERFQPnE3O0bi4AgA3e+6/0pbGBZaCp6cncnNzcfbsWWRlZUEul6scf+utt3SWHBEREemetkNDn/V21FMmDZfWBda+ffswduxY5Ofnw9bWVmWSukgkYoFFRERUz2kzNNTarAkCeIscrWnd53v//fcxYcIE5OfnIzc3Fw8ePFA+cnJy9JEjERER6dDZ5Psax658uTM3uNeC1gVWWloaZsyYobzHHxERERkPmVzAhqM3NIp9vr0jhnZ21XNGDZPWBVZwcDCio6P1kQsRERHp2XeRV1Es0+y+N5OeM/yt5oyV1nuwhg0bhg8//BCJiYnw9fWFqampyvEXX3xRZ8kRERGR7sjkAr6NvK5RrLWZCYeLPgWtC6xJkyYBgMqATgWRSASZTPPLPomIiKjuTN8ao/E9m31cbbn36iloXWA9OZaBiIiI6r+SMjkOJmh2Y2cA8GvVVI/ZNHycFkpERNQIDFtzXKt4zr56OlqvYAFAQUEBjh07htTUVJSUlKgcmzFjhk4SIyIiIt0oLJHhWlaBxvEWpmLOvnpKWhdY58+fx9ChQ/Ho0SMUFBTAwcEB9+7dg6WlJZycnFhgERER1TPLDyZqFb/ylS7cf/WUtG4Rzp49GyNGjMCDBw9gYWGB06dP49atW+jRowe+/PJLfeRIRERETyE65YHGsd097DGii5ses2kctC6w4uLi8P7770MsFsPExATFxcXw8PDAypUr8dFHH+kjRyIiIqolmVzAjex8jeN3TOmtx2waD60LLFNTU4jF5W9zcnJCamoqAMDOzg63b9/WbXZERET0VM4m56BEw8GibwW0ZGtQR7Teg9WtWzecO3cObdu2Rb9+/bBw4ULcu3cPv/zyCzp16qSPHImIiKiWsh4WaRw7xJetQV3RegVr+fLlcHUtvy/RsmXL0LRpU0yZMgXZ2dn44YcfdJ4gERER1V5zazON4hysJJzcrkNar2D5+fkp/+zk5ITw8HCdJkREREQ6pOHo9rcCPNke1CEOGiUiImrA7hUUaxTn5Wil50waF61XsLy8vCASVV3h3rx586kSIiIiIt1xsjHXaRxpRusCa9asWSrPS0tLcf78eYSHh+PDDz/UVV5ERESkAz08m0KE6juFYlF5HOmO1gXWzJkzK3197dq1iI6OfuqEiIiISHfe//18jduw5AIQc+sBAnl7HJ3R2R6sIUOGYOfOnbo6HRERET2lkjI59sVnahSrzTgHqpnOCqw//vgDDg68vJOIiKi+2HwyWeNY7sHSrVoNGq24yV0QBGRmZiI7Oxvr1q3TaXJERERUe//RsMAyNRFxBpaOaV1ghYSEqDwXi8VwdHRE//790b59e13lRURERE+hpEyOTKlmIxpaOlhyBpaOaV1gLVq0SB95EBERkQ79EpWicewoP3f9JdJIaV1gpaWlYefOnbh69SokEgmeeeYZjBo1Ck2b8vJOIiKi+uJWziONY8c/21qPmTROWhVY69atQ1hYGEpKSmBrawsAkEqlCAsLw48//ogxY8ZAEATExcWhW7duekmYiIiIanY1U6pRXICXAyRNeGMXXdP4J3rgwAHMmDED06ZNQ1paGnJzc5Gbm4u0tDS8++67CA0NxYkTJzB27Fjs27dPnzkTERFRNUrK5Did/ECj2Cn9vPWcTeOk8QrWqlWrMG/ePHz22Wcqr7u6uuLrr7+GpaUlXnjhBbi4uGDFihU6T5SIiIg0M++PCxrHnrv1AP3aO+kxm8ZJ4xWs2NhYvPnmm1Uef/PNN1FcXIxjx47B09NTJ8kRERGRdmRyAXsvpGvxjprmvFNtaFxgyWQymJqaVnnc1NQUFhYWaNmypU4SIyIiIu19f/gaZFrUTIGtm+svmUZM4wKrY8eO2Lt3b5XH9+zZg44dO+okKSIiItKeTC7g20PXNI43NxUjgPcf1AuN92BNnToVU6ZMgZmZGd555x00aVL+1rKyMmzcuBELFizgJHciIiIDOnEtG3It4le90oUDRvVE4wIrNDQUFy9exLRp0zB//nx4e3tDEATcvHkT+fn5mDFjBsaNG6fHVImIiKg6H+2O1zjWwdIUI7q46TGbxk2rOVhffvklXnnlFfz222+4dq18CbJv374YM2YMAgIC9JIgERER1aykTI60XM1ujQMAwzq76jEb0nqSe0BAAIspIiKieuY/J25qFf/RUB89ZUKAFpvciYiIqP7afT5N49jn2zvCQmKix2yIBRYREVEDkKrhvQfFIuA/43rpORsymgJr8eLFEIlEKo/27dsrjxcVFWHq1Klo1qwZrK2t8fLLL+Pu3bsq50hNTcWwYcNgaWkJJycnfPjhhygrK1OJOXr0KLp37w4zMzO0adMGmzdvVstl7dq1aNWqFczNzeHv74+zZ8/q5TsTERFpYn9cGgpLNbt+8LWe7nrOhgAjKrCA8llcGRkZyseJEyeUx2bPno19+/Zhx44dOHbsGNLT0/Gvf/1LeVwmk2HYsGEoKSnBqVOn8NNPP2Hz5s1YuHChMiY5ORnDhg3DgAEDEBcXh1mzZuHtt9/GX3/9pYzZvn07wsLCsGjRIsTGxqJLly4IDg5GVlZW3fwQiIiIKpDJBUzbFqdx/MLhnfSXDCmJBEHQekZ+WVkZjh49ihs3buD111+HjY0N0tPTYWtrC2tra33kicWLF2PPnj2Ii4tTO5aXlwdHR0ds3boVr7zyCgDgypUr6NChA6KiohAQEIA///wTw4cPR3p6OpydnQEAGzZswNy5c5GdnQ2JRIK5c+fiwIEDSEhIUJ579OjRyM3NRXh4OADA398fPXv2xPfffw8AkMvl8PDwwPTp0zFv3rwq8y8uLkZx8eOrO6RSKTw8PJCXlwdbW9un/vkQEVHj5PfpX7hXUFZzIAAriQkuLR2s54waNqlUCjs7uxp/f2u9gnXr1i34+vripZdewtSpU5GdnQ0A+OKLL/DBBx/UPmMNXLt2DW5ubmjdujXGjh2L1NRUAEBMTAxKS0sRFBSkjG3fvj1atmyJqKgoAEBUVBR8fX2VxRUABAcHQyqV4tKlS8qYiudQxCjOUVJSgpiYGJUYsViMoKAgZUxVVqxYATs7O+XDw8PjKX4SREREwJJ9FzUurgDAydZMj9lQRVoXWDNnzoSfnx8ePHgACwsL5esjR45EZGSkTpOryN/fH5s3b0Z4eDjWr1+P5ORkPPfcc3j48CEyMzMhkUhgb2+v8h5nZ2dkZmYCADIzM1WKK8VxxbHqYqRSKQoLC3Hv3j3IZLJKYxTnqMr8+fORl5enfNy+fVvrnwEREZFCSZkcm06mavWegNYOesqGnqT1HKx//vkHp06dgkQiUXm9VatWSEvT/BJRbQ0ZMkT5586dO8Pf3x+enp74/fffVQq9+srMzAxmZvx/DkREpBs/nUrR+j3cf1V3tF7BksvlkMlkaq/fuXMHNjY2OklKE/b29mjXrh2uX78OFxcXlJSUIDc3VyXm7t27cHFxAQC4uLioXVWoeF5TjK2tLSwsLNC8eXOYmJhUGqM4BxERUV1YE3lVq/huHnacfVWHtC6wBg0ahNWrVyufi0Qi5OfnY9GiRRg6dKguc6tWfn4+bty4AVdXV/To0QOmpqYqLcqkpCSkpqYiMDAQABAYGIiLFy+qXO0XEREBW1tb+Pj4KGOebHNGREQozyGRSNCjRw+VGLlcjsjISGUMERGRvuUXleFhsfpiR3X+mPKsnrKhymh9FeGdO3cQHBwMQRBw7do1+Pn54dq1a2jevDmOHz8OJycnvST6wQcfYMSIEfD09ER6ejoWLVqEuLg4JCYmwtHREVOmTMHBgwexefNm2NraYvr06QCAU6dOASgf09C1a1e4ublh5cqVyMzMxJtvvom3334by5cvB1A+pqFTp06YOnUqJkyYgMOHD2PGjBk4cOAAgoODAZSPaQgNDcXGjRvRq1cvrF69Gr///juuXLmitjerOppehUBERPSk9h8fRJFM81/f343pxhs764imv7+13oPl7u6OCxcuYNu2bYiPj0d+fj4mTpyIsWPH6nUv1J07dzBmzBjcv38fjo6O6NOnD06fPg1HR0cAwDfffAOxWIyXX34ZxcXFCA4Oxrp165TvNzExwf79+zFlyhQEBgbCysoKoaGhWLp0qTLGy8sLBw4cwOzZs/Htt9/C3d0dP/74o7K4AoDXXnsN2dnZWLhwITIzM9G1a1eEh4drVVwRERHV1p7o21oVV+2crFlcGUCt5mDR0+MKFhERaUsmF9Du44PQor7C1c+GQNLEqOaK12s6XcH673//q/EHv/jiixrHEhERkeZO37yvVXHVs5U9iysD0ajACgkJ0ehkIpGo0isMiYiI6Ol9EX5Zq/gtb/MCLEPRqMCSyzW7gSQRERHpR0mZHPF3pBrHc/XKsPiTJyIiMgK/RKVoFc/VK8OqVYEVGRmJ4cOHw9vbG97e3hg+fDgOHTqk69yIiIjof77++4rGsW525ly9MjCtf/rr1q3D4MGDYWNjg5kzZ2LmzJmwtbXF0KFDsXbtWn3kSERE1KjlPSpFQanmu9tXhPjqMRvShNZjGtzd3TFv3jxMmzZN5fW1a9di+fLler0fYUPCMQ1ERKSpf607idjUXI3jbywfChOxSH8JNWKa/v7WegUrNzcXgwcPVnt90KBByMvL0/Z0REREVIOLd3I1ju3cwpbFVT2gdYH14osvYvfu3Wqv7927F8OHD9dJUkRERFSusESGUi0u5t86iZvb6wOtb5Xj4+ODZcuW4ejRo8obHJ8+fRonT57E+++/jzVr1ihjZ8yYobtMiYiIGqFJP53TOLaZtSmszbX+1U56oPUeLC8vL81OLBLh5s2btUqqMeAeLCIiqolMLsD7o4Max2+Z6I9n2zbXY0akt5s9JycnP1ViREREpJnpW2O0ig/wbqanTEhbHJJBRERUD5WUyXEw4a7G8e0cLbm5vR7RegVLEAT88ccfOHLkCLKystRuo7Nr1y6dJUdERNRY/XQqRav4j4d21E8iVCtaF1izZs3Cxo0bMWDAADg7O0MkYrVMRESka2sir2oV3+cZRz1lQrWhdYH1yy+/YNeuXRg6dKg+8iEiImr08ovK8LBYpnG8VzMLtgfrGa33YNnZ2aF169b6yIWIiIgA9Pj0L63i90x9Tk+ZUG1pXWAtXrwYS5YsQWFhoT7yISIiatTyHpVCi8UrmDURw87SVH8JUa1o3SIcNWoUfvvtNzg5OaFVq1YwNVX9S42NjdVZckRERI2N32d/axX/w9geesqEnobWBVZoaChiYmLwxhtvcJM7ERGRDuU9KtXqtjgicHN7faV1gXXgwAH89ddf6NOnjz7yISIiarTGbz6rVfzXo7pyc3s9pfUeLA8PD97ahYiISA9iU3M1jrWUiDGyewv9JUNPResC66uvvsKcOXOQkpKih3SIiIgap5z8Eq3iYxYM0lMmpAtatwjfeOMNPHr0CN7e3rC0tFTb5J6Tk6Oz5IiIiBqLYWuOaxxrLRHDQmKix2zoaWldYK1evVoPaRARETVeMrmADGmxxvEvdWNrsL6r1VWEREREpDtfh1/RKn7BMN53sL7TusCqqKioCCUlqj1jboAnIiLSnEwuYO3xmxrHN7NswvagEdB6k3tBQQGmTZsGJycnWFlZoWnTpioPIiIi0py2q1eT+7fRUyakS1oXWHPmzMHhw4exfv16mJmZ4ccff8SSJUvg5uaGn3/+WR85EhERNUgyuYB1WqxeAUBoby89ZUO6pHWLcN++ffj555/Rv39/jB8/Hs899xzatGkDT09PbNmyBWPHjtVHnkRERA3O6Zv3IWgR72FvDkkTrddGyAC0/lvKyclB69atAZTvt1KMZejTpw+OH9f8ElMiIqLGbt4f57WK/3NWPz1lQrqmdYHVunVrJCcnAwDat2+P33//HUD5ypa9vb1OkyMiImqoCktkuJ2r+XBRiyZiWJs/1bVpVIe0LrDGjx+PCxcuAADmzZuHtWvXwtzcHLNnz8aHH36o8wSJiIgaoqX7LmkVf/qjID1lQvqgdSk8e/Zs5Z+DgoJw+fJlxMbGok2bNujcubNOkyMiImqo/kzI0DhWDMDO0rTGOKo/nnqtsVWrVmjVqpUOUiEiImocZHIBuYVlGsefmjdQj9mQPmjcIoyKisL+/ftVXvv555/h5eUFJycnvPPOOygu1nzMPxERUWP18rp/tIp3sTfXUyakLxoXWEuXLsWlS4/7xRcvXsTEiRMRFBSEefPmYd++fVixYoVekiQiImooCktkiLvzUON47+YWesyGAAByOZCQAGzeDAjaDM6omsYtwri4OHz66afK59u2bYO/vz/+/e9/AwA8PDywaNEiLF68WCeJERERNUQdFoZrFf+Kn4eeMmnEHj0Czp0DTp4sf5w6BeTmlh8bMADw9Hzqj9C4wHrw4AGcnZ2Vz48dO4YhQ4Yon/fs2RO3b99+6oSIiIgaqvm7tZt7BQAT+3jrIZNG5u7d8kLqxIny/42NBcqe2ANnZQX4+wN5eTr5SI1bhM7Ozsr5VyUlJYiNjUVAQIDy+MOHD2Fqqr8rHFasWIGePXvCxsYGTk5OCAkJQVJSkkpM//79IRKJVB6TJ09WiUlNTcWwYcNgaWkJJycnfPjhhyh74od89OhRdO/eHWZmZmjTpg02b96sls/atWvRqlUrmJubw9/fH2fPntX5dyYiooajpEyO386ka/UeO3MxJ7drSy4HLl0CfvgBCA0F2rQBXFyAl18GvvkGOHu2vLhycwNGjQK+/RaIji5fwYqMBHQ0EUHjFayhQ4di3rx5+OKLL7Bnzx5YWlriueeeUx6Pj4+Ht7f+quxjx45h6tSp6NmzJ8rKyvDRRx9h0KBBSExMhJWVlTJu0qRJWLp0qfK5paWl8s8ymQzDhg2Di4sLTp06hYyMDLz11lswNTXF8uXLAQDJyckYNmwYJk+ejC1btiAyMhJvv/02XF1dERwcDADYvn07wsLCsGHDBvj7+2P16tUIDg5GUlISnJyc9PYzICIi49V+wZ9av+fIB7x6sEaFheXtPsXqVFQU8OCBaoxIBPj6As8++/jh6Vn+up6IBEGz3Vz37t3Dv/71L5w4cQLW1tb46aefMHLkSOXxgQMHIiAgAMuWLdNbshVlZ2fDyckJx44dQ9++fQGUr2B17doVq1evrvQ9f/75J4YPH4709HRlu3PDhg2YO3cusrOzIZFIMHfuXBw4cAAJCQnK940ePRq5ubkIDy/vm/v7+6Nnz574/vvvAQByuRweHh6YPn065s2bp1H+UqkUdnZ2yMvLg62tbW1/DEREZAT+OHsLH+xKqDmwAlvzJohfHKynjIyYot2neMTGAqWlqjGWluXtvj59youpgADAzk4nH6/p72+NV7CaN2+O48ePIy8vD9bW1jAxMVE5vmPHDlhbW9c+Yy3l/a9H6uDgoPL6li1b8Ouvv8LFxQUjRozAJ598olzFioqKgq+vr8pesuDgYEyZMgWXLl1Ct27dEBUVhaAg1Wm5wcHBmDVrFoDy9mhMTAzmz5+vPC4WixEUFISoqKgq8y0uLlYZYyGVSmv3xYmIyKjI5ILWxRUAnF84SA/ZGBm5HLhy5XExdeIEcOOGepybm+rqVJcugB63LWlC60GjdlVUgE8WOvokl8sxa9YsPPvss+jUqZPy9ddffx2enp5wc3NDfHw85s6di6SkJOzatQsAkJmZqVJcAVA+z8zMrDZGKpWisLAQDx48gEwmqzTmypUrVea8YsUKLFmypPZfmoiIjJL3Rwe1fs9Xr3aBiVh/7at6S9Huq3h1X2Xtvk6dVAuqVq302u6rDaO8a+TUqVORkJCAEydOqLz+zjvvKP/s6+sLV1dXDBw4EDdu3NDr/jBNzJ8/H2FhYcrnUqkUHh689JaIqCHr+NEBrd9jYSrGyz3c9ZBNPZSVpdrui4mput2nKKYCAgB7e4Okqw2jK7CmTZuG/fv34/jx43B3r/4fQH9/fwDA9evX4e3tDRcXF7Wr/e7evQsAcHFxUf6v4rWKMba2trCwsICJiQlMTEwqjVGcozJmZmYwMzPT7EsSEZHRy8kvQYFc+/ddWNRA91092e47eRK4fl09ztVVdXWqa1eDt/tqw2gKLEEQMH36dOzevRtHjx6Fl5dXje+Ji4sDALi6ugIAAgMDsWzZMmRlZSmv9ouIiICtrS18fHyUMQcPqi7nRkREIDAwEAAgkUjQo0cPREZGIiQkBEB5yzIyMhLTpk3TxVclIqIGwH95hNbvedPfs+GMZSgqUm/35eSoxhhJu682jKbAmjp1KrZu3Yq9e/fCxsZGuWfKzs4OFhYWuHHjBrZu3YqhQ4eiWbNmiI+Px+zZs9G3b190/t9Mi0GDBsHHxwdvvvkmVq5ciczMTCxYsABTp05Vri5NnjwZ33//PebMmYMJEybg8OHD+P3333HgwONl3rCwMISGhsLPzw+9evXC6tWrUVBQgPHjx9f9D4aIiOqd/KIylGq5emUiAj4d2anmwPoqK6u8iFKMS6is3WdhodruCww0inZfbWg8psHQRFVUs5s2bcK4ceNw+/ZtvPHGG0hISEBBQQE8PDwwcuRILFiwQOUyylu3bmHKlCk4evQorKysEBoais8//xxNmjyuNY8ePYrZs2cjMTER7u7u+OSTTzBu3DiVz/3++++xatUqZGZmomvXrlizZo2yJakJjmkgImq4Ws3Tfu/VjeVDjWdjuyCot/uuXVOPc3EpL6QU4xKMtN1Xkaa/v42mwGpoWGARETVM2dJi9Fx+SKv3fDOqK0Z2b6GnjHSgqKh82nnFgqqydl/HjqrtPi+vBtHuq0jnc7CIiIioZtoWV+ZNRPWvuMrOVr+6r6RENcbCAujVS7Xd17SpYfKth1hgERER6UhtWoPRCww8UFQQgKQk1YLq6lX1OEW7T/Ho1s3o2336xAKLiIhIB2pTXDnZSGBtXse/ip9s9506Bdy/rx73ZLuvdesG1+7TJxZYRERET6k2xRUAnJhbBzdzzs4uL6IUBVV0tHq7z9xc/eo+tvueCgssIiKip9C6lsXVC+0cdT/zShDK23uK+/ZV1e5zdlZv90kkus2lkWOBRUREVEvDvj2KWgxrBwBsGNfz6RMoLlZv9927px7n46M6LoHtPr1jgUVERFQL+UVluJRRUKv3rnu9e+1mXt2797jdd+JE1e2+J6/uc3CoVZ5UeyywiIiIaqHT4r9q9b7Xe3lgaGfXmgMrtvsUj6Qk9TgnJ9V2X/fubPfVAyywiIiItFBYIkOHheG1eq8YwPJ/da78YHFx+bypiu2+7Gz1OEW7T/Hw9ma7rx5igUVERKSh0P87i2PXKil6NHTz82GPn1Rs9ymu7isuVn2DuTnQs+fjYqp3b7b7jAQLLCIiIg14zz8AWW1vLicISJnYDti06XFBdeWKepyjo+pmdLb7jBYLLCIiohpoO+dKUlaKTpnX4ZeWCL+0yxiUewNYWcnKV4cOqu2+Nm3Y7msgWGARERFVQ5Piyr5Qih5pl+F35zJ6pCWiS8Y1mMlKVYPMzB63+/r0Kb+6r1kzPWVNhsYCi4iIqBLZ0uLKb9wsCGj1IB1+aZfR404iet5JRJucO2ph9yztkO3bAx1eGfK43WdmVgeZU33AAouIiOgJPgvD8ahEBuB/7b6719HjzmX4pSWie9oVOD7KVXvPdQd3RLv7IKZFB0S7+yDbuSUSPh1cx5lTfcECi4iIqIIuM7chIK28mPK7U3m7r9jEFBdc2yKmhQ+i3TsgpkUH5FrYqsSksLhq1FhgERFR4yUIwPXrwMmTyPnrCO5HHMGF+7fVwu5b2CLG3QfRLToguoUPElzaoKSJaZWnTak4joEaJRZYRETUeJSUALGxqtPRs7IAAA7/ewDADQf38mLK3QfR7j5Ibuqm0dV9fb3t8POkPvrLn4wGCywiImq4cnJUh3meOwcUFamEFJs0QbxLO8S4l69OxbRojweWdlp/1OWlg2EhMdFV5mTkWGAREVHDIAjAjRuqq1OJiepxzZqhxD8QXz1qjnMtOtbY7tMEW4L0JBZYRERknEpKgPPnywupEyfKV6ru3lWPe+YZlWGenbcmQ1os01kaLK6oMiywiIjIODx4oNruO3tWrd0HiQTw81O9d5+jIwAgKf0hgtcc12lKLK6oKiywiIio/hEE4ObNx8XUiRNVtvtUbjXTo0f5DZIryMwtQsDnkTpNr4WdKU7OH6TTc1LDwgKLiIgMr2K7T/GorN3Xrp1qQfXMM9Ve3df2o4Moldf2Ds2Vu7BwEOwsn27PFjV8LLCIiKjuPXgAREWptvsKC1VjJJLyFSnFvfsqtPtqcvrqfYz+z2mdp82WIGmKBRYREemXIADJyeVtPkVBdemSelyzZuVFlGJ1ys9Prd1Xk6PxdzFua7SOEn9sZLfm+OY1f52flxouFlhERKRbpaXq7b7MTPW4tm0fF1N9+tTY7quOPjawK1z9bAgkTcR6OTc1XCywiIjo6eTmlrf7FCtUlbX7TE3Vr+5zcnrqj/756FUsDL/21OepzJ7Jz6JrK3u9nJsaPhZYRESkOUW7r+Lq1KVL5a9X5OCg3u6zsNBJCnmPStFl6d86OVdVuNeKnhYLLCIiqlppKRAXpzouobJ2X5s25W2+ilf3iXXXVruYmocR607o7HxV+f3tQPRq41BzIFENWGAREdFjinZfxav7Hj1SjTE1fXx1n6Ld5+ys81TOXs/BqB+jdH7eqnDVinSJBRYRUWMlCEBKimq7LyFBvd3XtOnjdl+fPjpt9z1p16kUhP23kisM9Wjfe33g21L7mzsTVYcFFhFRY1FaCly4oDouISNDPa5NG9Vhnu3b67Td96T3Nv2Fg0llejt/VX54pRsG+bnV+edS48ACi4ioocrLU233nTlTebuve3fVYZ56aPdVNGH9ARy+pdePqBHbgaRvLLCIiBoCQQBu3Xq8Eb2qdp+9verqVM+eemv3AcDi3Wew+cw9vZ1fW0fC+sPLycrQaVAjwAKLiMgYlZWpXt138iSQnq4e5+2tWlB16KCXdl9cSi5CNpzU+Xl1he1AqmsssIiIjEFeHnD69OMVqsrafU2alLf7FOMSevcGXFx0msbqPy9g9bE7Oj2nPv04qjuCursaOg1qhFhgERHVNxXbfYrHxYuVt/sqDvPs2ROwtKz1xxpqs7k+fDakHd7o19bQaVAjxgKLiMjQysrKr+6rOMyzsnZf69aq9+6rod3XZd4B5Okx7fqIIxeovmCB9RTWrl2LVatWITMzE126dMF3332HXr16GTotIqrvpFL1q/sKClRCSsUmuOTsjegWHRDt7oOYFh2Qbf2/CePJAJJTAKTUceL10zu9nfHRi36GToNIBQusWtq+fTvCwsKwYcMG+Pv7Y/Xq1QgODkZSUhKcdHADUyKq32ZtOYI9Fx/VHCgIaCHNRo+0RPjduQy/tES0z0qBGKrtPqmZFWJatEd0Cx9Eu/vggmtbFJma6yl74xfW3wMzBnc2dBpEVRIJwpNNfdKEv78/evbsie+//x4AIJfL4eHhgenTp2PevHk1vl8qlcLOzg55eXmwtbXVd7pEjdK4tQdw9HbdfqaJXIb2WcnoeScRfmmX0eNOIlzz76vFpdo545y7D2LcfRDdogOuNW8JQaS/YZ4Nwax+7pg1pIuh06BGTtPf31zBqoWSkhLExMRg/vz5ytfEYjGCgoIQFVX5fbOKi4tRXFysfC6VSvWeJ5ExGbLwAC6XGDoL7VkXP0K39Cvwu3MZPdIS0S09CValRSox5e2+1ohp4aNs+SnbfVStpYPb4q3+7QydBpHWWGDVwr179yCTyeD8xLRjZ2dnXLlypdL3rFixAkuWLKmL9Ijq1J6oW5i1N8HQadQZN2mWspjqeScRz2TfgokgV4mRmlkh1q39/1aoOuCCSzsUStju00Rzc+DovGBYm/PXExk3/hNcR+bPn4+wsDDlc6lUCg8PDwNmRKTqywPn8f0/lVy51ogp2n1+aZfhdycRPdIuw+2h+lTyVDtn5Ub0aHcfXGW7T2OulsDheYNhITExdCpEOsUCqxaaN28OExMT3L17V+X1u3fvwqWKoX5mZmYwMzOri/SokUvOKsCAr48aOg2jZFX8CN3Sk+CXlogedy6jW0YSrEsKVWLKRGJccvZGTIsO5StULTogy6aZgTI2PiuHd8CoPq0NnQaR3rHAqgWJRIIePXogMjISISEhAMo3uUdGRmLatGmGTY4ajLxHpei/9G88MHQiDVjFdp/fncton51SZbsv2r0DYlr4IM6V7T5NPOcG/DKDN1SmxosFVi2FhYUhNDQUfn5+6NWrF1avXo2CggKMHz/e0KlRPXP66n2M/s9pQ6fR6InlMnTITkEP5dV9l9HiYbZa3G07Z0S36IAYdx+cc/fBtWYekIvZvqrKv7pY4+sx/QydBlG9wwKrll577TVkZ2dj4cKFyMzMRNeuXREeHq628Z0alqT0hwhec9zQaZAGrIofoWvGVfS8c0mjdl/0/8YlsN332IQARywM4fBkotrgHCwD4Rys+iMnvwQvfRuJ2w/lNQdTveUqzVZuRPdLu4wOWcnq7T6JJc63aK8clXDBtR0eSSwMlHHd+2tGXzzjZmPoNIiMGudgEQGQyQUcvpiJObvP40ER/79EQyGWy9A++5ZyVEJV7b47tk6Idu+AaPeOiG7RAVebtzTKdt/3IztjuD+vOiYyJiywyOjJ5AKOX87C5+GXkJRdWPMbyOhYlhSWX933vxWqbulXYFNJuy/RuXV5u6+FD6LdO+CuTfM6y7G9KRD+KTd1E1E5FlhkNGRyASeSsrHu6FXEp+WhqAzgmlTD5CK9Vz4q4X/zp3wqafc9lFjgvFt7RP9vM7om7b4h7UywfsJgfaZORASABRbVYyVlcvzfiRv4I/o2bucUooRbpBqkiu0+xcgEd6l6uw8tWwJ9+gDPPgs8+yxsOnVCXxMT9K37lImIasQCi+oVmVzAqWv3sHhfAm7ce2TodKgWRnW3w8pRfaoOyM8HzpwBTp4sf0RFAQ8fqsaIxUDXrspiCs8+C7i76zVvIiJdYoFF9UJJmRzzdl7Anrh0yNn3qxe2TQhAQDsdjCy4c+dxMXXyJHDhAiCTqcbY2ACBgY+LKX9/wNr66T+biMhAWGBRnVOsUu2ISUVihhQZeUUoYP9Pb5YObou3+rermw+TyYCEBNWC6tYt9biWLVVXp3x9ARPju7qPiKgqLLBI7xQF1R+xtxF/Jw8p9x9xc3otbHi5Kwb3bGHoNFQVFDxu9504AZw+DUilqjFiMdClS3khpdhDxXYfETVwLLBIrw7GZyDs9zgUlXGFCgDe6NkSC1/qCEkTsaFTqZ20NNXVqbg49XaftbV6u8+Gwy2JqHFhgUU6J5MLOH3jPlb+dRkX7khrfoMRMhEBf83shzYuDXifkEwGXLr0eHWqqnafh8fjYqpPH7b7iIjAAot0RFFU/Xw6BYevZKFUZnxNQDMTESb3a4Opz7cx3hWmp1Gx3ae4uq+6dp/i4cEJ40RET2KBRU9FJhfwXeQ1bDh2o163AU1EQCc3W/w8MQB2lqaGTqd+SE9XbfedP195uy8g4HExFRDAdh8RkQZYYJHGZHIBZ5NzkJlXiHv5xYhJfYDIxCyU1oO5CmIAVmYmGOLriiUvdoKFhC0qFRXbfYpHSop6nLu7yjBP+PoCTfifCSIibfG/nKSR8IQMLNmXiIy8IoPmYW4qhr2FKQY844SFIzqykKpKQQFw9qxquy8vTzVGLAY6d1Zt97VsaZh8iYgaGBZYpFyZynpYhOZWZoAIyJIWIaegBPaWEkTduIc/YtMMll8HZ2vsmtqHxVR1MjIeF1MnTpRf3VdWphpjZfW43denT/nVfba2BkmXiKihY4HViFQspJxszNHDsynWH72BTSeTkVtYauj0VDjZmOHtPl4Y96xX49xwXh25XL3dl5ysHufurro61bkz231ERHWE/7U1ck8WTb28HGAiFqkdS7n3CL+dTUWm9HGLTwTUi4Gflk1EcLA2g7OtOYI7urCoetKjR4/bfSdOVN7uE4lU2319+rDdR0RkQCyw6qnqCieFyvZFudqZY9EIHwCocc+UoYorazMTBLZuhl5ezRDauxWLqSdVbPcpru6rrt2nuLqP7T4ionqDBVY9VF3hNLiTqzJmyq+xakVSZl4RJv8aW4fZamfSc63w8bCOhk6j/pDLgcRE1YLq5k31uBYtVNt9Xbqw3UdEVI/xv9D1THWF05RfY7H+je54wccFS/YlVroCVR9afhWZiICOLWwxonMLrlYBqu0+xdV9ubmqMSJR+XiEiuMSWrYsf52IiIwCC6x6RCYXqi2cRChv+9mYmxp8XEJ1TMXAwA7OeDOwFQJaN1NrbTYqmZmqq1OxsertPktL9XafnZ1h8iUiIp1ggVWPnE3OqXHPVEZeEaJu3K+7pDRk3kSMAe2d8EaAZ+MtquRy4PLlx/ftq6rd5+amuhmd7T4iogaH/1WvR7IearoqVX8agUM6uTTeourRI+DcucfF1KlTVbf7Ku6f8vRku4+IqIFjgVWPONmYaxQX2Lo5dsamITOvyGCl1pOb7huFu3dVh3lW1e7z939cTAUGst1HRNQIscCqR3p5OcDVzrzKwkkEwMXOHAHezbBohA+m/BqrNsuq4vOa5lyJRUDF2wjaW5gitLcnenk1U5nknvvo8f86WJvBxbbysRENiqLdV3H/1I0b6nGurqqb0bt0AUx5M2kiosaOBVY9YiIWVVs4AcCiET4wEYswuJMr1r/RXW2cg0s1c7BcbM0wpldLtGpupZzkHnPrQbWzthqNwkL1dt+DB6oxIhHQqZNqu69VK7b7iIhIjUgQhPqzoacRkUqlsLOzQ15eHmyfGBCpyRwsBU0nuTf6AupJFdt9iqv7Sp+4XZCFRXm7T7FCFRAA2NsbJF0iIqofqvv9XRELLAOp6S+IxZEOyeXAlSuqBdX16+pxrq6qq1Ndu7LdR0REKjQtsNgirKdMxCIEejczdBrGqbAQiI5+PC6hqnZfx46q4xLY7iMiIh1hgUXGLytLdXUqJqbqdl/Fq/vY7iMiIj1hgUXGRS4HkpIej0qoqt3n4qLa7uvWje0+IiKqMyywqH4rKlK/ui8nRz2uY0fVcQleXmz3ERGRwbDAovolO1u13RcdXXm7r1cv1XZf06aGyZeIiKgSLLDIcARB/eq+a9fU45ydH29EV1zdJ5HUebpERESaYoFFdaeoqHxFqmJBVVW7r+L+qdat2e4jIiKjwgKL9Cc7u3zPlGIzekwMUFKiGmNurt7uc3AwTL5EREQ6wgKLdEMQHl/dp3hcvaoep2j3Vby6j+0+IiJqYFhgUe0UFZWvSCnGJZw6Bdy/rx7n46NaUHl7s91HREQNHgss0oyi3Vfx6r7K2n09ez7ejM52HxERNVIssEidIJS39yq2+5KS1OOcnFRXp7p3Z7uPiIgIgNjQCWgiJSUFEydOhJeXFywsLODt7Y1FixahpMIKSkpKCkQikdrj9OnTKufasWMH2rdvD3Nzc/j6+uLgwYMqxwVBwMKFC+Hq6goLCwsEBQXh2hOjA3JycjB27FjY2trC3t4eEydORH5+vv5+APpWXFxeRK1cCbz0Unnh1L49MHEi8J//PC6uOnQAJk0CNm8uH6eQmQns2gW8/z4QEMDiioiI6H+MYgXrypUrkMvl2LhxI9q0aYOEhARMmjQJBQUF+PLLL1ViDx06hI4dOyqfN2v2+IbJp06dwpgxY7BixQoMHz4cW7duRUhICGJjY9GpUycAwMqVK7FmzRr89NNP8PLywieffILg4GAkJibC3NwcADB27FhkZGQgIiICpaWlGD9+PN555x1s3bq1Dn4aOnDvnmq779w59XafmZn61X3NePNpIiIiTYgEQRAMnURtrFq1CuvXr8fNmzcBlK9geXl54fz58+jatWul73nttddQUFCA/fv3K18LCAhA165dsWHDBgiCADc3N7z//vv44IMPAAB5eXlwdnbG5s2bMXr0aFy+fBk+Pj44d+4c/Pz8AADh4eEYOnQo7ty5Azc3t0o/u7i4GMXFxcrnUqkUHh4eyMvLg62trS5+JJUThPLVJsWohKrafY6O6u0+MzP95UVERGSEpFIp7Ozsavz9bRQrWJXJy8uDQyUbqF988UUUFRWhXbt2mDNnDl588UXlsaioKISFhanEBwcHY8+ePQCA5ORkZGZmIigoSHnczs4O/v7+iIqKwujRoxEVFQV7e3tlcQUAQUFBEIvFOHPmDEaOHFlpvitWrMCSJUue5itrprj48dV9inv3ZWerx3XooFpQtWnDq/uIiIh0xCgLrOvXr+O7775TaQ9aW1vjq6++wrPPPguxWIydO3ciJCQEe/bsURZZmZmZcHZ2VjmXs7MzMjMzlccVr1UX4+TkpHK8SZMmcHBwUMZUZv78+SrFnWIF66ndv686zDM6urzIqsjMrPzqPkUx1bs3231ERER6ZNACa968efjiiy+qjbl8+TLat2+vfJ6WlobBgwfj1VdfxaRJk5SvN2/eXKWA6dmzJ9LT07Fq1SqVVSxDMTMzg9nTttwU7b6KV/dduaIe17y56r372O4jIiKqUwYtsN5//32MGzeu2pjWrVsr/5yeno4BAwagd+/e+OGHH2o8v7+/PyIiIpTPXVxccPfuXZWYu3fvwsXFRXlc8Zqrq6tKjGJfl4uLC7KyslTOUVZWhpycHOX7daa4GIiNVS2oKmv3tW+v2u5r25btPiIiIgMyaIHl6OgIR0dHjWLT0tIwYMAA9OjRA5s2bYJYXPOEibi4OJVCKTAwEJGRkZg1a5bytYiICAQGBgIAvLy84OLigsjISGVBJZVKcebMGUyZMkV5jtzcXMTExKBHjx4AgMOHD0Mul8Pf31+j76IRmQxwc1O/GbKZGeDnp9rua95cd59LRERET80o9mClpaWhf//+8PT0xJdffonsCqs4ilWjn376CRKJBN26dQMA7Nq1C//5z3/w448/KmNnzpyJfv364auvvsKwYcOwbds2REdHK1fDRCIRZs2ahc8++wxt27ZVjmlwc3NDSEgIAKBDhw4YPHgwJk2ahA0bNqC0tBTTpk3D6NGjq7yCsFZMTABfX+DSJdXVqR492O4jIiKq7wQjsGnTJgFApQ+FzZs3Cx06dBAsLS0FW1tboVevXsKOHTvUzvX7778L7dq1EyQSidCxY0fhwIEDKsflcrnwySefCM7OzoKZmZkwcOBAISkpSSXm/v37wpgxYwRra2vB1tZWGD9+vPDw4UOtvlNeXp4AQMjLy6s6KCdHEORyrc5LRERE+qPR729BEIx2Dpax03SOBhEREdUfmv7+Nopb5RAREREZExZYRERERDrGAouIiIhIx1hgEREREekYCywiIiIiHWOBRURERKRjLLCIiIiIdIwFFhEREZGOscAiIiIi0jEWWEREREQ6xgKLiIiISMdYYBERERHpGAssIiIiIh1rYugEGitBEACU35WbiIiIjIPi97bi93hVWGAZyMOHDwEAHh4eBs6EiIiItPXw4UPY2dlVeVwk1FSCkV7I5XKkp6fDxsYGIpFI+bpUKoWHhwdu374NW1tbA2ZYdxrbd+b3bfga23fm9234Gtt3ru77CoKAhw8fws3NDWJx1TutuIJlIGKxGO7u7lUet7W1bRT/EFfU2L4zv2/D19i+M79vw9fYvnNV37e6lSsFbnInIiIi0jEWWEREREQ6xgKrnjEzM8OiRYtgZmZm6FTqTGP7zvy+DV9j+878vg1fY/vOuvi+3OROREREpGNcwSIiIiLSMRZYRERERDrGAouIiIhIx1hgEREREekYCywjUVxcjK5du0IkEiEuLs7Q6ejNiy++iJYtW8Lc3Byurq548803kZ6ebui09CIlJQUTJ06El5cXLCws4O3tjUWLFqGkpMTQqenVsmXL0Lt3b1haWsLe3t7Q6ejc2rVr0apVK5ibm8Pf3x9nz541dEp6c/z4cYwYMQJubm4QiUTYs2ePoVPSqxUrVqBnz56wsbGBk5MTQkJCkJSUZOi09Gb9+vXo3LmzcthmYGAg/vzzT0OnVWc+//xziEQizJo1q1bvZ4FlJObMmQM3NzdDp6F3AwYMwO+//46kpCTs3LkTN27cwCuvvGLotPTiypUrkMvl2LhxIy5duoRvvvkGGzZswEcffWTo1PSqpKQEr776KqZMmWLoVHRu+/btCAsLw6JFixAbG4suXbogODgYWVlZhk5NLwoKCtClSxesXbvW0KnUiWPHjmHq1Kk4ffo0IiIiUFpaikGDBqGgoMDQqemFu7s7Pv/8c8TExCA6OhrPP/88XnrpJVy6dMnQqenduXPnsHHjRnTu3Ln2JxGo3jt48KDQvn174dKlSwIA4fz584ZOqc7s3btXEIlEQklJiaFTqRMrV64UvLy8DJ1Gndi0aZNgZ2dn6DR0qlevXsLUqVOVz2UymeDm5iasWLHCgFnVDQDC7t27DZ1GncrKyhIACMeOHTN0KnWmadOmwo8//mjoNPTq4cOHQtu2bYWIiAihX79+wsyZM2t1Hq5g1XN3797FpEmT8Msvv8DS0tLQ6dSpnJwcbNmyBb1794apqamh06kTeXl5cHBwMHQaVAslJSWIiYlBUFCQ8jWxWIygoCBERUUZMDPSl7y8PABoFP/OymQybNu2DQUFBQgMDDR0Ono1depUDBs2TOXf5dpggVWPCYKAcePGYfLkyfDz8zN0OnVm7ty5sLKyQrNmzZCamoq9e/caOqU6cf36dXz33Xd49913DZ0K1cK9e/cgk8ng7Oys8rqzszMyMzMNlBXpi1wux6xZs/Dss8+iU6dOhk5Hby5evAhra2uYmZlh8uTJ2L17N3x8fAydlt5s27YNsbGxWLFixVOfiwWWAcybNw8ikajax5UrV/Ddd9/h4cOHmD9/vqFTfiqafl+FDz/8EOfPn8fff/8NExMTvPXWWxCM6IYD2n5fAEhLS8PgwYPx6quvYtKkSQbKvPZq852JjNnUqVORkJCAbdu2GToVvXrmmWcQFxeHM2fOYMqUKQgNDUViYqKh09KL27dvY+bMmdiyZQvMzc2f+ny8VY4BZGdn4/79+9XGtG7dGqNGjcK+ffsgEomUr8tkMpiYmGDs2LH46aef9J2qTmj6fSUSidrrd+7cgYeHB06dOmU0y9Laft/09HT0798fAQEB2Lx5M8Ri4/v/PbX5O968eTNmzZqF3NxcPWdXN0pKSmBpaYk//vgDISEhytdDQ0ORm5vb4FdiRSIRdu/erfLdG6pp06Zh7969OH78OLy8vAydTp0KCgqCt7c3Nm7caOhUdG7Pnj0YOXIkTExMlK/JZDKIRCKIxWIUFxerHKtJE30kSdVzdHSEo6NjjXFr1qzBZ599pnyenp6O4OBgbN++Hf7+/vpMUac0/b6VkcvlAMrHVBgLbb5vWloaBgwYgB49emDTpk1GWVwBT/d33FBIJBL06NEDkZGRyiJDLpcjMjIS06ZNM2xypBOCIGD69OnYvXs3jh492uiKK6D8n2lj+u+xNgYOHIiLFy+qvDZ+/Hi0b98ec+fO1aq4Alhg1WstW7ZUeW5tbQ0A8Pb2hru7uyFS0qszZ87g3Llz6NOnD5o2bYobN27gk08+gbe3t9GsXmkjLS0N/fv3h6enJ7788ktkZ2crj7m4uBgwM/1KTU1FTk4OUlNTIZPJlHPd2rRpo/xn3FiFhYUhNDQUfn5+6NWrF1avXo2CggKMHz/e0KnpRX5+Pq5fv658npycjLi4ODg4OKj996shmDp1KrZu3Yq9e/fCxsZGubfOzs4OFhYWBs5O9+bPn48hQ4agZcuWePjwIbZu3YqjR4/ir7/+MnRqemFjY6O2n06xH7hW++x0dl0j6V1ycnKDHtMQHx8vDBgwQHBwcBDMzMyEVq1aCZMnTxbu3Llj6NT0YtOmTQKASh8NWWhoaKXf+ciRI4ZOTSe+++47oWXLloJEIhF69eolnD592tAp6c2RI0cq/bsMDQ01dGp6UdW/r5s2bTJ0anoxYcIEwdPTU5BIJIKjo6MwcOBA4e+//zZ0WnXqacY0cA8WERERkY4Z54YPIiIionqMBRYRERGRjrHAIiIiItIxFlhEREREOsYCi4iIiEjHWGARERER6RgLLCIiIiIdY4FFREREpGMssIhI544ePQqRSGR0N3IWiUTYs2ePzs7XqlUrrF69WmfnM5SUlBSIRCLlbY2M9e+XqC6xwCIirYhEomofixcvNnSKNVq8eDG6du2q9npGRgaGDBlSp7nk5ORg1qxZ8PT0hEQigZubGyZMmIDU1NQ6zUNh3LhxyptVK3h4eCAjI6N292MjaqR4s2ci0kpGRobyz9u3b8fChQuRlJSkfM3a2hrR0dGGSA0lJSWQSCS1fn9d32Q7JycHAQEBkEgk2LBhAzp27IiUlBQsWLAAPXv2RFRUFFq3bl2nOVXGxMSkQd+AnEgfuIJFRFpxcXFRPuzs7CASiVRes7a2VsbGxMTAz88PlpaW6N27t0ohBgB79+5F9+7dYW5ujtatW2PJkiUoKytTHk9NTcVLL70Ea2tr2NraYtSoUbh7967yuGIl6scff4SXlxfMzc0BALm5uXj77bfh6OgIW1tbPP/887hw4QIAYPPmzViyZAkuXLigXHXbvHkzAPUW4Z07dzBmzBg4ODjAysoKfn5+OHPmDADgxo0beOmll+Ds7Axra2v07NkThw4d0upn+fHHHyM9PR2HDh3CkCFD0LJlS/Tt2xd//fUXTE1NMXXqVGVsZe3Grl27qqwYfv311/D19YWVlRU8PDzw3nvvIT8/X3l88+bNsLe3x19//YUOHTrA2toagwcPVhbNixcvxk8//YS9e/cqfzZHjx5VaxFW5sSJE3juuedgYWEBDw8PzJgxAwUFBcrj69atQ9u2bWFubg5nZ2e88sorWv2siIwNCywi0puPP/4YX331FaKjo9GkSRNMmDBBeeyff/7BW2+9hZkzZyIxMREbN27E5s2bsWzZMgCAXC7HSy+9hJycHBw7dgwRERG4efMmXnvtNZXPuH79Onbu3Ildu3YpC4BXX30VWVlZ+PPPPxETE4Pu3btj4MCByMnJwWuvvYb3338fHTt2REZGBjIyMtTOCQD5+fno168f0tLS8N///hcXLlzAnDlzIJfLlceHDh2KyMhInD9/HoMHD8aIESM0bu3J5XJs27YNY8eOVVsdsrCwwHvvvYe//voLOTk5Gv+8xWIx1qxZg0uXLuGnn37C4cOHMWfOHJWYR48e4csvv8Qvv/yC48ePIzU1FR988AEA4IMPPsCoUaOURVdGRgZ69+5d4+feuHEDgwcPxssvv4z4+Hhs374dJ06cwLRp0wAA0dHRmDFjBpYuXYqkpCSEh4ejb9++Gn8vIqMkEBHV0qZNmwQ7Ozu1148cOSIAEA4dOqR87cCBAwIAobCwUBAEQRg4cKCwfPlylff98ssvgqurqyAIgvD3338LJiYmQmpqqvL4pUuXBADC2bNnBUEQhEWLFgmmpqZCVlaWMuaff/4RbG1thaKiIpVze3t7Cxs3blS+r0uXLmp5AxB2794tCIIgbNy4UbCxsRHu37+v4U9DEDp27Ch89913yueenp7CN998U2lsZmamAKDK47t27RIACGfOnKnyXF26dBEWLVpUZT47duwQmjVrpny+adMmAYBw/fp15Wtr164VnJ2dlc9DQ0OFl156SeU8ycnJAgDh/PnzgiA8/vt98OCBIAiCMHHiROGdd95Rec8///wjiMViobCwUNi5c6dga2srSKXSKnMlami4B4uI9KZz587KP7u6ugIAsrKy0LJlS1y4cAEnT55UrlgBgEwmQ1FRER49eoTLly/Dw8MDHh4eyuM+Pj6wt7fH5cuX0bNnTwCAp6cnHB0dlTEXLlxAfn4+mjVrppJLYWEhbty4oXHucXFx6NatGxwcHCo9np+fj8WLF+PAgQPIyMhAWVkZCgsLtd6cLghCtce12VN26NAhrFixAleuXIFUKkVZWZny52lpaQkAsLS0hLe3t/I9rq6uyMrK0irnJ124cAHx8fHYsmWL8jVBECCXy5GcnIwXXngBnp6eaN26NQYPHozBgwdj5MiRypyIGiIWWESkN6ampso/i0QiAFBpsS1ZsgT/+te/1N6n2EulCSsrK5Xn+fn5cHV1xdGjR9Vi7e3tNT6vhYVFtcc/+OADRERE4Msvv0SbNm1gYWGBV155BSUlJRqd39HRUVksVuby5cto0qQJvLy8AJS3/54sxkpLS5V/TklJwfDhwzFlyhQsW7YMDg4OOHHiBCZOnIiSkhJlMVPx7wQo/3upqcirSX5+Pt59913MmDFD7VjLli0hkUgQGxuLo0eP4u+//8bChQuxePFinDt3Tqu/EyJjwgKLiAyie/fuSEpKQps2bSo93qFDB9y+fRu3b99WrmIlJiYiNzcXPj4+1Z43MzMTTZo0QatWrSqNkUgkkMlk1ebXuXNn/Pjjj8jJyal0FevkyZMYN24cRo4cCaC8yEhJSan2nBWJxWKMGjUKW7ZswdKlS1X2YRUWFmLdunUYOXIk7OzsAJQXZBWv4JRKpUhOTlY+j4mJgVwux1dffQWxuHx77e+//65xPgqa/Gye1L17dyQmJlb5dwkATZo0QVBQEIKCgrBo0SLY29vj8OHDlRbYRA0BN7kTkUEsXLgQP//8M5YsWYJLly7h8uXL2LZtGxYsWAAACAoKgq+vL8aOHYvY2FicPXsWb731Fvr16wc/P78qzxsUFITAwECEhITg77//RkpKCk6dOoWPP/5YOT6iVatWSE5ORlxcHO7du4fi4mK184wZMwYuLi4ICQnByZMncfPmTezcuRNRUVEAgLZt2yo31l+4cAGvv/66cnVOU8uWLYOLiwteeOEF/Pnnn7h9+zaOHz+O4OBgiMVifPvtt8rY559/Hr/88gv++ecfXLx4EaGhoTAxMVEeb9OmDUpLS/Hdd9/h5s2b+OWXX7Bhwwat8lH8bOLj45GUlIR79+6prJJVZe7cuTh16hSmTZuGuLg4XLt2DXv37lVuct+/fz/WrFmDuLg43Lp1Cz///DPkcjmeeeYZrfMjMhYssIjIIIKDg7F//378/fff6NmzJwICAvDNN9/A09MTQHnrau/evWjatCn69u2LoKAgtG7dGtu3b6/2vCKRCAcPHkTfvn0xfvx4tGvXDqNHj8atW7fg7OwMAHj55ZcxePBgDBgwAI6Ojvjtt9/UziORSPD333/DyckJQ4cOha+vLz7//HNlUfP111+jadOm6N27N0aMGIHg4GB0795dq59B8+bNcfr0aQwYMADvvvsuvLy80K9fP8hkMsTFxSn3rQHA/Pnz0a9fPwwfPhzDhg1DSEiIyl6qLl264Ouvv8YXX3yBTp06YcuWLVixYoVW+QDApEmT8Mwzz8DPzw+Ojo44efJkje/p3Lkzjh07hqtXr+K5555Dt27dsHDhQri5uQEob83u2rULzz//PDp06IANGzbgt99+Q8eOHbXOj8hYiISnbb4TEZHO/N///R/ee+89bN++XW2iOhEZD65gERHVIxMnTsS2bdtw+fJlFBYWGjodIqolrmARERER6RhXsIiIiIh0jAUWERERkY6xwCIiIiLSMRZYRERERDrGAouIiIhIx1hgEREREekYCywiIiIiHWOBRURERKRjLLCIiIiIdOz/Ab4bAHIdSmSkAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a QQ plot for the residuals\n",
    "sm.qqplot(ml_model.resid, line='s');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae19e59c-b60a-43e9-b4af-e09a45d76d8a",
   "metadata": {},
   "source": [
    "#### Summary of Results & Analysis: Multilinear Regression with all Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10307627-1229-44f9-b1b5-bfb29ccc1ea2",
   "metadata": {},
   "source": [
    "The R-squared value of 0.038 produced by the multilinear model, which incorporates all predictors, represents a notable improvement compared to the R-squared values achieved through simple linear regression with single predictors. This value indicates that approximately 3.8% of the variability in the shares variable can be attributed to the collective influence of the predictor variables.\n",
    "\n",
    "**Evaluating Regression: Diagnostic Plot:**  \n",
    "I also checked the residuals against the theoretical quantiles of the normal distribution for the model with all predictors. The line is not linear and diagonal as expected if the residuals were normally distributed, so this model may not be a good fit. This suggests that further investigation and potentially model refinement might be necessary to address any violations of the normality assumption.\n",
    "\n",
    "**Next steps:**  \n",
    "To hopefully improve the model's predictive performance, I will explore Ridge and Lasso regression. These techniques are particularly suited for situations where multicollinearity is suspected, as they introduce regularization terms to the regression equation. Regularization methods like Ridge and Lasso help prevent overfitting and improve model stability by adding penalty terms to the model coefficients with different approaches. Ridge regression shrinks the coefficients towards zero, while Lasso regression encourages some of them to be exactly zero.\n",
    "\n",
    "Upon careful analysis of the coefficient summary, a range of p-values associated with the predictors is observed. Lower p-values indicate a higher level of statistical significance, suggesting a stronger relationship between the predictor and the target variable, shares. Conversely, higher p-values imply that a predictor might have a limited impact on predicting shares.\n",
    "\n",
    "Based on this assessment, I have identified the following predictors as statistically significant (p-value < 0.05): \n",
    "* n_tokens_title\n",
    "* n_non_stop_unique_tokens\n",
    "* num_hrefs\n",
    "* num_self_hrefs\n",
    "* num_imgs\n",
    "* average_token_length\n",
    "* num_keywords\n",
    "* kw_max_min\n",
    "* kw_min_max\n",
    "* kw_avg_max\n",
    "* kw_min_avg\n",
    "* kw_max_avg\n",
    "* self_reference_min_shares\n",
    "* self_reference_max_shares\n",
    "* LDA_00\n",
    "* LDA_01\n",
    "* LDA_02\n",
    "* LDA_03\n",
    "* LDA_04\n",
    "* global_subjectivity\n",
    "* title_sentiment_polarity\n",
    "* abs_title_subjectivity\n",
    "* data_channel\n",
    "\n",
    "With this understanding of predictor significance, I will also refine the multilinear regression model by focusing on these statistically significant predictors. I'll aim to create a more accurate model that captures the underlying dynamics of article shares."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799d8327-16c1-4f42-aade-b1181ce48b43",
   "metadata": {},
   "source": [
    "#### Enhancing Predictive Accuracy Through Regularization: Exploring Ridge and Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65e33d26-aa55-4a9d-a952-c50e3bb7a00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared for Ridge regression: 0.0281\n",
      "R-squared for Lasso regression: 0.0284\n"
     ]
    }
   ],
   "source": [
    "# Instantiate Ridge and Lasso regression models\n",
    "# Alpha is a tuning parameter that determines strength of the regularization applied to the model\n",
    "ridge_model = Ridge(alpha=1.0) \n",
    "lasso_model = Lasso(alpha=1.0)\n",
    "\n",
    "# Fit the models to the training data\n",
    "ridge_model.fit(X_train, y_train)\n",
    "lasso_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the models on the test data\n",
    "ridge_r2 = ridge_model.score(X_test, y_test)\n",
    "lasso_r2 = lasso_model.score(X_test, y_test)\n",
    "\n",
    "print(f\"R-squared for Ridge regression: {ridge_r2:.4f}\")\n",
    "print(f\"R-squared for Lasso regression: {lasso_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591f7677-c717-4311-ab35-ea4837101482",
   "metadata": {},
   "source": [
    "Implementing Ridge and Lasso regression and evaluating their performance on the test data yields lower R-squared values that multilinear regression with all predictors.\n",
    "\n",
    "**Next steps:**  \n",
    "I'll attempt to enhance the performance of the Ridge and Lasso regression models by tuning the alpha parameter. The alpha parameter plays a crucial role in controlling the degree of regularization in these models. Too high an alpha value can lead to excessive regularization, underfitting the data, while too low an alpha value may result in inadequate regularization, leading to overfitting.\n",
    "\n",
    "To identify the optimal alpha values for Ridge and Lasso, I will create models using a range of alpha values. The RidgeCV and LassoCV functions perform cross-validation to find the best alpha value. I will fit these models to the training data and extract the best alpha values determined by the cross-validation process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2754b4fd-7d2c-4b0d-b651-771094be8318",
   "metadata": {},
   "source": [
    "#### Ridge and Lasso regression models using cross-validation for alpha selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca9c34e9-182e-47b4-8fed-7d9d6b13643f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha for Ridge: 50.0000\n",
      "Best alpha for Lasso: 10.0000\n"
     ]
    }
   ],
   "source": [
    "# Create Ridge and Lasso models with a range of alpha values\n",
    "ridge_model = RidgeCV(alphas=[0.01, 0.1, 1.0, 5.0, 10.0, 20.0, 50.0])\n",
    "lasso_model = LassoCV(alphas=[0.01, 0.1, 1.0, 5.0, 10.0, 20.0, 50.0], max_iter=10000)\n",
    "\n",
    "# Fit the models to the training data\n",
    "ridge_model.fit(X_train, y_train)\n",
    "lasso_model.fit(X_train, y_train)\n",
    "\n",
    "# Get the best alpha values from the models determined from cross-validation process\n",
    "best_ridge_alpha = ridge_model.alpha_\n",
    "best_lasso_alpha = lasso_model.alpha_\n",
    "\n",
    "print(f\"Best alpha for Ridge: {best_ridge_alpha:.4f}\")\n",
    "print(f\"Best alpha for Lasso: {best_lasso_alpha:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b94e1d3-8fe5-46df-982d-f064243892cd",
   "metadata": {},
   "source": [
    "Now that I have the best alpha values, I will update the Ridge and Lasso models with this value and assess its performance on the test data. This iterative approach will allow me to strike the right balance between regularization and model fit, ultimately leading to a more accurate predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "748192b0-523b-4b5f-bfbf-903a49e5dd4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared for Ridge regression: 0.0284\n",
      "R-squared for Lasso regression: 0.0291\n"
     ]
    }
   ],
   "source": [
    "# Instantiate Ridge and Lasso regression models\n",
    "ridge_model = Ridge(alpha=best_ridge_alpha) \n",
    "lasso_model = Lasso(alpha=best_lasso_alpha)\n",
    "\n",
    "# Fit the models to the training data\n",
    "ridge_model.fit(X_train, y_train)\n",
    "lasso_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the models on the test data\n",
    "ridge_r2 = ridge_model.score(X_test, y_test)\n",
    "lasso_r2 = lasso_model.score(X_test, y_test)\n",
    "\n",
    "print(f\"R-squared for Ridge regression: {ridge_r2:.4f}\")\n",
    "print(f\"R-squared for Lasso regression: {lasso_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7736bf-6f78-44b5-8591-f820c52731bb",
   "metadata": {},
   "source": [
    "#### Summary of Results & Analysis: Exploring Ridge and Lasso Regression\n",
    "Implementing Ridge and Lasso regression with the fine-tuned hyperparameters (best alpha) and evaluating their performance on the test data yields a lower R-squared values that the multilinear model with all predictors. The R-squared values remain closer to 0 than to 1. \n",
    "\n",
    "**Next steps:**  \n",
    "I will refine the multilinear regression model by focusing on statistically significant predictors. I'll aim to create a more accurate model that captures the underlying dynamics of article shares."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ca6a4b-c11b-42dd-b9ef-a1d5e49201eb",
   "metadata": {},
   "source": [
    "#### Multilinear Regression with Statistically Significant Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b11f86d-954a-4d5c-a005-5f9949bda77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_predictors = ['n_tokens_title', 'n_non_stop_unique_tokens', 'num_hrefs',\n",
    "                          'num_self_hrefs', 'num_imgs', 'average_token_length', 'num_keywords',\n",
    "                          'kw_max_min', 'kw_min_max', 'kw_avg_max', 'kw_min_avg', 'kw_max_avg',\n",
    "                          'self_reference_min_shares', 'self_reference_max_shares', 'LDA_00',\n",
    "                          'LDA_01', 'LDA_02', 'LDA_03', 'LDA_04', 'global_subjectivity', \n",
    "                          'title_sentiment_polarity', 'abs_title_subjectivity', 'data_channel'\n",
    "                         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ca1907d-8897-45ed-80f6-f332857d7996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Added predictor 'LDA_03' (Adjusted R-squared: 0.0136)\n",
      "Step 2: Added predictor 'kw_max_avg' (Adjusted R-squared: 0.0186)\n",
      "Step 3: Added predictor 'global_subjectivity' (Adjusted R-squared: 0.0228)\n",
      "Step 4: Added predictor 'self_reference_min_shares' (Adjusted R-squared: 0.0254)\n",
      "Step 5: Added predictor 'num_hrefs' (Adjusted R-squared: 0.0281)\n",
      "Step 6: Added predictor 'LDA_02' (Adjusted R-squared: 0.0294)\n",
      "Step 7: Added predictor 'data_channel' (Adjusted R-squared: 0.0318)\n",
      "Step 8: Added predictor 'title_sentiment_polarity' (Adjusted R-squared: 0.0323)\n",
      "Step 9: Added predictor 'num_self_hrefs' (Adjusted R-squared: 0.0329)\n",
      "Step 10: Added predictor 'num_imgs' (Adjusted R-squared: 0.0332)\n",
      "Step 11: Added predictor 'n_tokens_title' (Adjusted R-squared: 0.0336)\n",
      "Step 12: Added predictor 'self_reference_max_shares' (Adjusted R-squared: 0.0339)\n",
      "Step 13: Added predictor 'average_token_length' (Adjusted R-squared: 0.0341)\n",
      "Step 14: Added predictor 'num_keywords' (Adjusted R-squared: 0.0343)\n",
      "Step 15: Added predictor 'kw_min_avg' (Adjusted R-squared: 0.0347)\n",
      "Step 16: Added predictor 'LDA_00' (Adjusted R-squared: 0.0350)\n",
      "Step 17: Added predictor 'abs_title_subjectivity' (Adjusted R-squared: 0.0352)\n",
      "Step 18: Added predictor 'kw_max_min' (Adjusted R-squared: 0.0353)\n",
      "Step 19: Added predictor 'kw_avg_max' (Adjusted R-squared: 0.0354)\n",
      "Step 20: Added predictor 'kw_min_max' (Adjusted R-squared: 0.0354)\n",
      "Step 21: Added predictor 'n_non_stop_unique_tokens' (Adjusted R-squared: 0.0354)\n",
      "Step 22: Added predictor 'LDA_01' (Adjusted R-squared: 0.0354)\n",
      "Step 23: Added predictor 'LDA_04' (Adjusted R-squared: 0.0356)\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 shares   R-squared:                       0.036\n",
      "Model:                            OLS   Adj. R-squared:                  0.036\n",
      "Method:                 Least Squares   F-statistic:                     42.33\n",
      "Date:                Fri, 11 Aug 2023   Prob (F-statistic):          2.86e-187\n",
      "Time:                        11:33:23   Log-Likelihood:            -2.6433e+05\n",
      "No. Observations:               25756   AIC:                         5.287e+05\n",
      "Df Residuals:                   25732   BIC:                         5.289e+05\n",
      "Df Model:                          23                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=============================================================================================\n",
      "                                coef    std err          t      P>|t|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------------\n",
      "Intercept                  -8.55e+05    3.4e+05     -2.518      0.012   -1.52e+06    -1.9e+05\n",
      "LDA_03                     8.552e+05   3.39e+05      2.522      0.012    1.91e+05    1.52e+06\n",
      "kw_max_avg                    0.0754      0.010      7.453      0.000       0.056       0.095\n",
      "global_subjectivity        3649.8095    533.881      6.836      0.000    2603.374    4696.245\n",
      "self_reference_min_shares     0.0142      0.003      5.624      0.000       0.009       0.019\n",
      "num_hrefs                    40.3462      4.638      8.698      0.000      31.255      49.438\n",
      "LDA_02                     8.529e+05   3.39e+05      2.515      0.012    1.88e+05    1.52e+06\n",
      "data_channel                213.9218     28.185      7.590      0.000     158.677     269.167\n",
      "title_sentiment_polarity    726.4795    169.656      4.282      0.000     393.943    1059.016\n",
      "num_self_hrefs              -64.0754     12.574     -5.096      0.000     -88.722     -39.429\n",
      "num_imgs                     27.1041      6.443      4.207      0.000      14.476      39.732\n",
      "n_tokens_title               64.4948     21.356      3.020      0.003      22.636     106.354\n",
      "self_reference_max_shares     0.0033      0.001      2.825      0.005       0.001       0.006\n",
      "average_token_length       -525.0197    165.126     -3.180      0.001    -848.676    -201.364\n",
      "num_keywords                103.5292     26.049      3.974      0.000      52.471     154.587\n",
      "kw_min_avg                    0.1377      0.044      3.120      0.002       0.051       0.224\n",
      "LDA_00                     8.548e+05   3.39e+05      2.520      0.012     1.9e+05    1.52e+06\n",
      "abs_title_subjectivity      566.7871    239.706      2.365      0.018      96.950    1036.624\n",
      "kw_max_min                   -0.0305      0.015     -2.037      0.042      -0.060      -0.001\n",
      "kw_avg_max                    0.0008      0.000      1.828      0.068   -5.55e-05       0.002\n",
      "kw_min_max                   -0.0015      0.001     -1.778      0.075      -0.003       0.000\n",
      "n_non_stop_unique_tokens   1322.4704    521.964      2.534      0.011     299.392    2345.549\n",
      "LDA_01                     8.543e+05   3.39e+05      2.519      0.012     1.9e+05    1.52e+06\n",
      "LDA_04                     8.542e+05   3.39e+05      2.519      0.012     1.9e+05    1.52e+06\n",
      "==============================================================================\n",
      "Omnibus:                    38108.204   Durbin-Watson:                   1.994\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):         15898803.484\n",
      "Skew:                           9.122   Prob(JB):                         0.00\n",
      "Kurtosis:                     123.341   Cond. No.                     5.54e+09\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 5.54e+09. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame with only significant predictors\n",
    "X_train_significant = X_train[significant_predictors]\n",
    "X_train_significant.insert(0, 'shares', y_train)  # Insert the target variable as the first column\n",
    "\n",
    "# Initialize the list to keep track of selected predictors\n",
    "selected_predictors = []\n",
    "\n",
    "# Initialize the formula\n",
    "formula = \"shares ~ 1\"  # Starting with intercept only\n",
    "\n",
    "# Initialize variables to keep track of the best R-squared and predictor\n",
    "best_r_squared = 0.0\n",
    "best_predictor = None\n",
    "\n",
    "# Perform forward stepwise regression\n",
    "for _ in range(len(significant_predictors)):\n",
    "    remaining_predictors = list(set(significant_predictors) - set(selected_predictors))\n",
    "    r_squared_values = []\n",
    "    \n",
    "    for predictor in remaining_predictors:\n",
    "        predictors = selected_predictors + [predictor]\n",
    "        temp_formula = formula + \" + \" + \" + \".join(predictors)\n",
    "        \n",
    "        temp_model = smf.ols(formula=temp_formula, data=X_train_significant).fit()\n",
    "        r_squared = temp_model.rsquared_adj\n",
    "        r_squared_values.append((predictor, r_squared))\n",
    "\n",
    "    # Find the predictor with the highest increase in R-squared\n",
    "    best_predictor, max_r_squared_increase = max(r_squared_values, key=lambda x: x[1])\n",
    "    \n",
    "    # Add the best predictor to the model\n",
    "    selected_predictors.append(best_predictor)\n",
    "    \n",
    "    # Update the formula\n",
    "    formula = formula + \" + \" + best_predictor\n",
    "\n",
    "    # Update the best R-squared value\n",
    "    best_r_squared = max_r_squared_increase\n",
    "    \n",
    "    # Print the current step's information\n",
    "    print(f\"Step {_+1}: Added predictor '{best_predictor}' (Adjusted R-squared: {best_r_squared:.4f})\")\n",
    "\n",
    "# Fit the final model\n",
    "final_model = smf.ols(formula=formula, data=X_train_significant).fit()\n",
    "\n",
    "print(final_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40117bea-efad-457f-ac01-4f9eaea9e14a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a QQ plot for the residuals\n",
    "sm.qqplot(final_model.resid, line='s');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a22a7a-82c7-49c2-af90-8e9fb5cc0b0c",
   "metadata": {},
   "source": [
    "#### Summary of Results & Analysis: Multilinear Regression with Statistically Significant Predictors (Part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8dddec-9c7a-4a2b-a1d1-751401733bda",
   "metadata": {},
   "source": [
    "To build a more refined model that captures the significant predictors influencing the number of shares an article receives, a forward stepwise regression was performed. This technique involves iteratively adding predictors to the model, starting from an intercept-only model, and selecting the predictor that contributes the most to the increase in the adjusted R-squared value. The following predictors were identified as statistically significant and added to the model:\n",
    "\n",
    "* LDA_03\n",
    "* kw_max_avg\n",
    "* global_subjectivity\n",
    "* self_reference_min_shares\n",
    "* num_hrefs\n",
    "* LDA_02\n",
    "* data_channel\n",
    "* title_sentiment_polarity\n",
    "* num_self_hrefs\n",
    "* num_imgs\n",
    "* n_tokens_title\n",
    "* self_reference_max_shares\n",
    "* average_token_length\n",
    "* num_keywords\n",
    "* kw_min_avg\n",
    "* LDA_00\n",
    "* abs_title_subjectivity\n",
    "* kw_max_min\n",
    "* kw_avg_max\n",
    "* kw_min_max\n",
    "* n_non_stop_unique_tokens\n",
    "* LDA_01\n",
    "* LDA_04\n",
    "\n",
    "The resulting multilinear regression model with these predictors yielded an adjusted R-squared value of 0.036. This indicates that approximately 3.6% of the variability in the shares variable can be explained by the collective influence of these significant predictors.\n",
    "\n",
    "However, it's noteworthy that the adjusted R-squared value has decreased compared to the multilinear model with all predictors (adjusted R-squared of 0.038). This suggests that the inclusion of some statistically insignificant predictors might have contributed slightly to the predictive power of the model.\n",
    "\n",
    "**Evaluating Regression: Diagnostic Plot:**   \n",
    "A QQ plot was generated to assess the normality of the residuals of the final model. Ideally, in a well-fitted linear regression model, the residuals should closely follow a straight diagonal line on the QQ plot, indicating normal distribution. In this case, the QQ plot displays results that are non-linear. This suggests that the assumption of normality for the residuals might not hold perfectly.\n",
    "\n",
    "**Next steps:**  \n",
    "Given the findings of this analysis, there are a few avenues that I could explore.\n",
    "\n",
    "Model Refinement: Revisit the list of statistically insignificant predictors and perform feature selection by gradually removing those with higher p-values (this would be the 2nd time I do this). This could help streamline the model by eliminating predictors that do not significantly contribute to the outcome.\n",
    "\n",
    "The following predictors are statistically significant (p-value < 0.05):\n",
    "* kw_max_avg\n",
    "* global_subjectivity\n",
    "* self_reference_min_shares\n",
    "* num_hrefs\n",
    "* data_channel\n",
    "* title_sentiment_polarity\n",
    "* num_self_hrefs\n",
    "* num_imgs\n",
    "* n_tokens_title\n",
    "* average_token_length\n",
    "* num_keywords\n",
    "* kw_min_avg\n",
    "\n",
    "Interaction Terms: Investigate if interactions between predictors exist, meaning their combined effect on the outcome is different from what would be expected from the individual effects.\n",
    "\n",
    "Nonlinear Models: If the relationship between the predictors and the outcome is inherently nonlinear, I could explore nonlinear regression techniques, such as polynomial regression. These models can capture more complex relationships by introducing polynomial terms of the predictors.\n",
    "\n",
    "I'll investigate these techniques in this order and determine what to do next based on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e6de5f7-82c3-41be-9c4f-eb2750810050",
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_predictors2 = ['kw_max_avg', 'global_subjectivity', 'self_reference_min_shares',\n",
    "                          'num_hrefs', 'data_channel', 'title_sentiment_polarity', 'num_self_hrefs',\n",
    "                          'num_imgs', 'n_tokens_title', 'average_token_length', 'num_keywords',\n",
    "                          'kw_min_avg'\n",
    "                         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab656036-2b52-444e-8485-0af148945c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Added predictor 'global_subjectivity' (Adjusted R-squared: 0.0097)\n",
      "Step 2: Added predictor 'kw_max_avg' (Adjusted R-squared: 0.0166)\n",
      "Step 3: Added predictor 'num_hrefs' (Adjusted R-squared: 0.0202)\n",
      "Step 4: Added predictor 'self_reference_min_shares' (Adjusted R-squared: 0.0231)\n",
      "Step 5: Added predictor 'data_channel' (Adjusted R-squared: 0.0244)\n",
      "Step 6: Added predictor 'num_imgs' (Adjusted R-squared: 0.0254)\n",
      "Step 7: Added predictor 'average_token_length' (Adjusted R-squared: 0.0261)\n",
      "Step 8: Added predictor 'num_self_hrefs' (Adjusted R-squared: 0.0269)\n",
      "Step 9: Added predictor 'title_sentiment_polarity' (Adjusted R-squared: 0.0276)\n",
      "Step 10: Added predictor 'kw_min_avg' (Adjusted R-squared: 0.0282)\n",
      "Step 11: Added predictor 'num_keywords' (Adjusted R-squared: 0.0286)\n",
      "Step 12: Added predictor 'n_tokens_title' (Adjusted R-squared: 0.0289)\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 shares   R-squared:                       0.029\n",
      "Model:                            OLS   Adj. R-squared:                  0.029\n",
      "Method:                 Least Squares   F-statistic:                     64.78\n",
      "Date:                Fri, 11 Aug 2023   Prob (F-statistic):          3.19e-156\n",
      "Time:                        11:33:24   Log-Likelihood:            -2.6443e+05\n",
      "No. Observations:               25756   AIC:                         5.289e+05\n",
      "Df Residuals:                   25743   BIC:                         5.290e+05\n",
      "Df Model:                          12                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=============================================================================================\n",
      "                                coef    std err          t      P>|t|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------------\n",
      "Intercept                  1189.7174    874.695      1.360      0.174    -524.733    2904.168\n",
      "global_subjectivity        5494.1464    511.260     10.746      0.000    4492.048    6496.245\n",
      "kw_max_avg                    0.0819      0.008     10.492      0.000       0.067       0.097\n",
      "num_hrefs                    39.5722      4.473      8.847      0.000      30.805      48.340\n",
      "self_reference_min_shares     0.0187      0.002      8.301      0.000       0.014       0.023\n",
      "data_channel                145.3847     23.579      6.166      0.000      99.168     191.601\n",
      "num_imgs                     29.4859      5.667      5.203      0.000      18.377      40.594\n",
      "average_token_length       -752.0465    161.445     -4.658      0.000   -1068.487    -435.606\n",
      "num_self_hrefs              -60.6276     12.333     -4.916      0.000     -84.801     -36.455\n",
      "title_sentiment_polarity    697.2811    164.570      4.237      0.000     374.715    1019.847\n",
      "kw_min_avg                    0.2064      0.041      5.034      0.000       0.126       0.287\n",
      "num_keywords                 90.1779     24.394      3.697      0.000      42.365     137.991\n",
      "n_tokens_title               53.2970     20.763      2.567      0.010      12.599      93.995\n",
      "==============================================================================\n",
      "Omnibus:                    38128.757   Durbin-Watson:                   1.996\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):         15843941.824\n",
      "Skew:                           9.135   Prob(JB):                         0.00\n",
      "Kurtosis:                     123.125   Cond. No.                     4.18e+05\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 4.18e+05. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "shares ~ 1 + global_subjectivity + kw_max_avg + num_hrefs + self_reference_min_shares + data_channel + num_imgs + average_token_length + num_self_hrefs + title_sentiment_polarity + kw_min_avg + num_keywords + n_tokens_title\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame with only significant predictors\n",
    "X_train_significant = X_train[significant_predictors2]\n",
    "X_train_significant.insert(0, 'shares', y_train)  # Insert the target variable as the first column\n",
    "\n",
    "# Initialize the list to keep track of selected predictors\n",
    "selected_predictors = []\n",
    "\n",
    "# Initialize the formula\n",
    "formula = \"shares ~ 1\"  # Starting with intercept only\n",
    "\n",
    "# Initialize variables to keep track of the best R-squared and predictor\n",
    "best_r_squared = 0.0\n",
    "best_predictor = None\n",
    "\n",
    "# Perform forward stepwise regression\n",
    "for _ in range(len(significant_predictors2)):\n",
    "    remaining_predictors = list(set(significant_predictors2) - set(selected_predictors))\n",
    "    r_squared_values = []\n",
    "    \n",
    "    for predictor in remaining_predictors:\n",
    "        predictors = selected_predictors + [predictor]\n",
    "        temp_formula = formula + \" + \" + \" + \".join(predictors)\n",
    "        \n",
    "        temp_model = smf.ols(formula=temp_formula, data=X_train_significant).fit()\n",
    "        r_squared = temp_model.rsquared_adj\n",
    "        r_squared_values.append((predictor, r_squared))\n",
    "\n",
    "    # Find the predictor with the highest increase in R-squared\n",
    "    best_predictor, max_r_squared_increase = max(r_squared_values, key=lambda x: x[1])\n",
    "    \n",
    "    # Add the best predictor to the model\n",
    "    selected_predictors.append(best_predictor)\n",
    "    \n",
    "    # Update the formula\n",
    "    formula = formula + \" + \" + best_predictor\n",
    "\n",
    "    # Update the best R-squared value\n",
    "    best_r_squared = max_r_squared_increase\n",
    "    \n",
    "    # Print the current step's information\n",
    "    print(f\"Step {_+1}: Added predictor '{best_predictor}' (Adjusted R-squared: {best_r_squared:.4f})\")\n",
    "\n",
    "# Fit the final model\n",
    "final_model2 = smf.ols(formula=formula, data=X_train_significant).fit()\n",
    "\n",
    "print(final_model2.summary())\n",
    "print(formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5406a843-c3b4-41a9-a429-580d2c77eec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a QQ plot for the residuals\n",
    "sm.qqplot(final_model2.resid, line='s');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e404d8d-8a40-4b32-aef9-5720c8bfd05c",
   "metadata": {},
   "source": [
    "#### Summary of Results & Analysis: Multilinear Regression with Statistically Significant Predictors (Part 2)\n",
    "In this second phase, I aimed to build a multilinear regression model that includes a reduced set of statistically significant predictors. I anticipated a further improvement in model performance. Surprisingly, the adjusted R-squared value decreased compared to the initial multilinear model, suggesting that the inclusion of certain predictors, even if they were statistically significant, did not collectively enhance the model's explanatory power.\n",
    "\n",
    "**Evaluating Regression: Diagnostic Plot:**     \n",
    "Despite the reduction in predictor variables, the diagnostic plot, specifically the QQ plot, still exhibits a non-linear pattern resembling a backward L shape. This continuing pattern raises the possibility of deviations from the assumption of normality in the residuals.\n",
    "\n",
    "**Next steps:**  \n",
    "The next phase of my analysis involves investigating if interactions between predictors exist, meaning their combined effect on the outcome is different from what would be expected from the individual effects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd98e80-6546-467e-8fb5-04eaa92d29f0",
   "metadata": {},
   "source": [
    "#### Exploring Predictor Interactions with Random Forest\n",
    "In this section, I employ a Random Forest algorithm to delve into potential interactions among predictors in the model. Random Forest, a powerful ensemble learning technique, inherently captures complex relationships between variables. By utilizing this approach, I aim to uncover intricate predictor interactions that may have a significant impact on predicting the number of shares an article receives. This method offers an efficient and comprehensive way to identify key features and interactions that contribute to the model's predictive performance.\n",
    "\n",
    "Results from the Random Forest analysis shed light on the relative importance of each predictor and provide valuable insights into how these predictors jointly influence the target variable. This information empowers me to make informed decisions about feature selection and model refinement in our pursuit of creating an accurate predictive model for article sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c42153d-2d0a-4e50-bdbd-6c5c5c0ab9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances:\n",
      "num_self_hrefs: 0.0331\n",
      "num_hrefs: 0.0325\n",
      "num_keywords: 0.0310\n",
      "self_reference_min_shares: 0.0305\n",
      "kw_max_avg: 0.0291\n",
      "global_subjectivity: 0.0253\n",
      "num_imgs: 0.0184\n",
      "data_channel: 0.0164\n",
      "average_token_length: 0.0137\n",
      "n_tokens_title: 0.0121\n",
      "title_sentiment_polarity: 0.0098\n",
      "kw_min_avg: 0.0033\n",
      "Random Forest R-squared: -0.0643\n"
     ]
    }
   ],
   "source": [
    "# Initialize and configure the model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate the R-squared value of the model\n",
    "rf_r2 = rf_model.score(X_test, y_test)\n",
    "\n",
    "# Get feature importances from the model\n",
    "feature_importances = rf_model.feature_importances_\n",
    "\n",
    "# Pair feature names with feature importances\n",
    "feature_importance_pairs = list(zip(predictors, feature_importances))\n",
    "\n",
    "# Sort feature importances in descending order\n",
    "feature_importance_pairs.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Feature Importances:\")\n",
    "for feature, importance in feature_importance_pairs:\n",
    "    print(f\"{feature}: {importance:.4f}\")\n",
    "\n",
    "print(f\"Random Forest R-squared: {rf_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c97ab1f-ce24-4215-8d80-b1e6bc4a3799",
   "metadata": {},
   "source": [
    "To further explore the potential interactions among predictors identified by the Random Forest analysis, I conducted a multilinear regression analysis. The goal was to assess how the inclusion of interaction terms could enhance the model's explanatory power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "579cabf1-5271-446a-b64d-5484bf09c386",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = [feature for feature, importance in feature_importance_pairs[:12]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "020495b5-fc70-4fd8-8867-2eced133f1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with only the selected features for training\n",
    "train_df_selected = train_df[selected_features + ['shares']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b575ea29-2d53-4a1b-a581-cb5e0054a546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction: self_reference_min_shares_global_subjectivity, Adjusted R-squared: 0.0308\n",
      "Interaction: global_subjectivity_self_reference_min_shares, Adjusted R-squared: 0.0308\n",
      "Interaction: self_reference_min_shares_num_imgs, Adjusted R-squared: 0.0306\n",
      "Interaction: num_imgs_self_reference_min_shares, Adjusted R-squared: 0.0306\n",
      "Interaction: kw_max_avg_num_imgs, Adjusted R-squared: 0.0301\n",
      "Interaction: num_imgs_kw_max_avg, Adjusted R-squared: 0.0301\n",
      "Interaction: global_subjectivity_data_channel, Adjusted R-squared: 0.0298\n",
      "Interaction: data_channel_global_subjectivity, Adjusted R-squared: 0.0298\n",
      "Interaction: num_keywords_kw_max_avg, Adjusted R-squared: 0.0297\n",
      "Interaction: kw_max_avg_num_keywords, Adjusted R-squared: 0.0297\n",
      "Interaction: num_imgs_average_token_length, Adjusted R-squared: 0.0297\n",
      "Interaction: average_token_length_num_imgs, Adjusted R-squared: 0.0297\n",
      "Interaction: num_self_hrefs_num_imgs, Adjusted R-squared: 0.0297\n",
      "Interaction: num_imgs_num_self_hrefs, Adjusted R-squared: 0.0297\n",
      "Interaction: num_self_hrefs_n_tokens_title, Adjusted R-squared: 0.0295\n",
      "Interaction: n_tokens_title_num_self_hrefs, Adjusted R-squared: 0.0295\n",
      "Interaction: num_self_hrefs_average_token_length, Adjusted R-squared: 0.0295\n",
      "Interaction: average_token_length_num_self_hrefs, Adjusted R-squared: 0.0295\n",
      "Interaction: num_hrefs_n_tokens_title, Adjusted R-squared: 0.0295\n",
      "Interaction: n_tokens_title_num_hrefs, Adjusted R-squared: 0.0295\n",
      "Interaction: kw_max_avg_data_channel, Adjusted R-squared: 0.0292\n",
      "Interaction: data_channel_kw_max_avg, Adjusted R-squared: 0.0292\n",
      "Interaction: global_subjectivity_title_sentiment_polarity, Adjusted R-squared: 0.0292\n",
      "Interaction: title_sentiment_polarity_global_subjectivity, Adjusted R-squared: 0.0292\n",
      "Interaction: num_hrefs_num_imgs, Adjusted R-squared: 0.0292\n",
      "Interaction: num_imgs_num_hrefs, Adjusted R-squared: 0.0292\n",
      "Interaction: data_channel_title_sentiment_polarity, Adjusted R-squared: 0.0291\n",
      "Interaction: title_sentiment_polarity_data_channel, Adjusted R-squared: 0.0291\n",
      "Interaction: num_self_hrefs_global_subjectivity, Adjusted R-squared: 0.0291\n",
      "Interaction: global_subjectivity_num_self_hrefs, Adjusted R-squared: 0.0291\n",
      "Interaction: kw_max_avg_n_tokens_title, Adjusted R-squared: 0.0291\n",
      "Interaction: n_tokens_title_kw_max_avg, Adjusted R-squared: 0.0291\n",
      "Interaction: self_reference_min_shares_title_sentiment_polarity, Adjusted R-squared: 0.0290\n",
      "Interaction: title_sentiment_polarity_self_reference_min_shares, Adjusted R-squared: 0.0290\n",
      "Interaction: average_token_length_n_tokens_title, Adjusted R-squared: 0.0290\n",
      "Interaction: n_tokens_title_average_token_length, Adjusted R-squared: 0.0290\n",
      "Interaction: num_keywords_num_imgs, Adjusted R-squared: 0.0290\n",
      "Interaction: num_imgs_num_keywords, Adjusted R-squared: 0.0290\n",
      "Interaction: average_token_length_title_sentiment_polarity, Adjusted R-squared: 0.0290\n",
      "Interaction: title_sentiment_polarity_average_token_length, Adjusted R-squared: 0.0290\n",
      "Interaction: num_keywords_data_channel, Adjusted R-squared: 0.0290\n",
      "Interaction: data_channel_num_keywords, Adjusted R-squared: 0.0290\n",
      "Interaction: num_imgs_n_tokens_title, Adjusted R-squared: 0.0290\n",
      "Interaction: n_tokens_title_num_imgs, Adjusted R-squared: 0.0290\n",
      "Interaction: num_keywords_average_token_length, Adjusted R-squared: 0.0290\n",
      "Interaction: average_token_length_num_keywords, Adjusted R-squared: 0.0290\n",
      "Interaction: num_self_hrefs_num_hrefs, Adjusted R-squared: 0.0290\n",
      "Interaction: num_hrefs_num_self_hrefs, Adjusted R-squared: 0.0290\n",
      "Interaction: kw_max_avg_title_sentiment_polarity, Adjusted R-squared: 0.0290\n",
      "Interaction: title_sentiment_polarity_kw_max_avg, Adjusted R-squared: 0.0290\n",
      "Interaction: num_self_hrefs_title_sentiment_polarity, Adjusted R-squared: 0.0290\n",
      "Interaction: title_sentiment_polarity_num_self_hrefs, Adjusted R-squared: 0.0290\n",
      "Interaction: num_keywords_self_reference_min_shares, Adjusted R-squared: 0.0289\n",
      "Interaction: self_reference_min_shares_num_keywords, Adjusted R-squared: 0.0289\n",
      "Interaction: num_hrefs_self_reference_min_shares, Adjusted R-squared: 0.0289\n",
      "Interaction: self_reference_min_shares_num_hrefs, Adjusted R-squared: 0.0289\n",
      "Interaction: num_hrefs_num_keywords, Adjusted R-squared: 0.0289\n",
      "Interaction: num_keywords_num_hrefs, Adjusted R-squared: 0.0289\n",
      "Interaction: num_keywords_kw_min_avg, Adjusted R-squared: 0.0289\n",
      "Interaction: kw_min_avg_num_keywords, Adjusted R-squared: 0.0289\n",
      "Interaction: num_hrefs_data_channel, Adjusted R-squared: 0.0289\n",
      "Interaction: data_channel_num_hrefs, Adjusted R-squared: 0.0289\n",
      "Interaction: num_imgs_data_channel, Adjusted R-squared: 0.0289\n",
      "Interaction: data_channel_num_imgs, Adjusted R-squared: 0.0289\n",
      "Interaction: num_imgs_kw_min_avg, Adjusted R-squared: 0.0289\n",
      "Interaction: kw_min_avg_num_imgs, Adjusted R-squared: 0.0289\n",
      "Interaction: global_subjectivity_n_tokens_title, Adjusted R-squared: 0.0289\n",
      "Interaction: n_tokens_title_global_subjectivity, Adjusted R-squared: 0.0289\n",
      "Interaction: data_channel_kw_min_avg, Adjusted R-squared: 0.0289\n",
      "Interaction: kw_min_avg_data_channel, Adjusted R-squared: 0.0289\n",
      "Interaction: global_subjectivity_kw_min_avg, Adjusted R-squared: 0.0289\n",
      "Interaction: kw_min_avg_global_subjectivity, Adjusted R-squared: 0.0289\n",
      "Interaction: num_self_hrefs_num_keywords, Adjusted R-squared: 0.0289\n",
      "Interaction: num_keywords_num_self_hrefs, Adjusted R-squared: 0.0289\n",
      "Interaction: num_self_hrefs_data_channel, Adjusted R-squared: 0.0289\n",
      "Interaction: data_channel_num_self_hrefs, Adjusted R-squared: 0.0289\n",
      "Interaction: kw_max_avg_kw_min_avg, Adjusted R-squared: 0.0289\n",
      "Interaction: kw_min_avg_kw_max_avg, Adjusted R-squared: 0.0289\n",
      "Interaction: num_keywords_n_tokens_title, Adjusted R-squared: 0.0289\n",
      "Interaction: n_tokens_title_num_keywords, Adjusted R-squared: 0.0289\n",
      "Interaction: n_tokens_title_title_sentiment_polarity, Adjusted R-squared: 0.0289\n",
      "Interaction: title_sentiment_polarity_n_tokens_title, Adjusted R-squared: 0.0289\n",
      "Interaction: num_self_hrefs_self_reference_min_shares, Adjusted R-squared: 0.0289\n",
      "Interaction: self_reference_min_shares_num_self_hrefs, Adjusted R-squared: 0.0289\n",
      "Interaction: global_subjectivity_average_token_length, Adjusted R-squared: 0.0289\n",
      "Interaction: average_token_length_global_subjectivity, Adjusted R-squared: 0.0289\n",
      "Interaction: data_channel_average_token_length, Adjusted R-squared: 0.0289\n",
      "Interaction: average_token_length_data_channel, Adjusted R-squared: 0.0289\n",
      "Interaction: data_channel_n_tokens_title, Adjusted R-squared: 0.0289\n",
      "Interaction: n_tokens_title_data_channel, Adjusted R-squared: 0.0289\n",
      "Interaction: num_imgs_title_sentiment_polarity, Adjusted R-squared: 0.0288\n",
      "Interaction: title_sentiment_polarity_num_imgs, Adjusted R-squared: 0.0288\n",
      "Interaction: average_token_length_kw_min_avg, Adjusted R-squared: 0.0288\n",
      "Interaction: kw_min_avg_average_token_length, Adjusted R-squared: 0.0288\n",
      "Interaction: kw_max_avg_average_token_length, Adjusted R-squared: 0.0288\n",
      "Interaction: average_token_length_kw_max_avg, Adjusted R-squared: 0.0288\n",
      "Interaction: self_reference_min_shares_n_tokens_title, Adjusted R-squared: 0.0288\n",
      "Interaction: n_tokens_title_self_reference_min_shares, Adjusted R-squared: 0.0288\n",
      "Interaction: self_reference_min_shares_average_token_length, Adjusted R-squared: 0.0288\n",
      "Interaction: average_token_length_self_reference_min_shares, Adjusted R-squared: 0.0288\n",
      "Interaction: self_reference_min_shares_data_channel, Adjusted R-squared: 0.0288\n",
      "Interaction: data_channel_self_reference_min_shares, Adjusted R-squared: 0.0288\n",
      "Interaction: num_hrefs_kw_max_avg, Adjusted R-squared: 0.0288\n",
      "Interaction: kw_max_avg_num_hrefs, Adjusted R-squared: 0.0288\n",
      "Interaction: self_reference_min_shares_kw_max_avg, Adjusted R-squared: 0.0288\n",
      "Interaction: kw_max_avg_self_reference_min_shares, Adjusted R-squared: 0.0288\n",
      "Interaction: global_subjectivity_num_imgs, Adjusted R-squared: 0.0288\n",
      "Interaction: num_imgs_global_subjectivity, Adjusted R-squared: 0.0288\n",
      "Interaction: num_keywords_global_subjectivity, Adjusted R-squared: 0.0288\n",
      "Interaction: global_subjectivity_num_keywords, Adjusted R-squared: 0.0288\n",
      "Interaction: num_keywords_title_sentiment_polarity, Adjusted R-squared: 0.0288\n",
      "Interaction: title_sentiment_polarity_num_keywords, Adjusted R-squared: 0.0288\n",
      "Interaction: num_hrefs_average_token_length, Adjusted R-squared: 0.0288\n",
      "Interaction: average_token_length_num_hrefs, Adjusted R-squared: 0.0288\n",
      "Interaction: num_self_hrefs_kw_min_avg, Adjusted R-squared: 0.0288\n",
      "Interaction: kw_min_avg_num_self_hrefs, Adjusted R-squared: 0.0288\n",
      "Interaction: self_reference_min_shares_kw_min_avg, Adjusted R-squared: 0.0288\n",
      "Interaction: kw_min_avg_self_reference_min_shares, Adjusted R-squared: 0.0288\n",
      "Interaction: num_hrefs_global_subjectivity, Adjusted R-squared: 0.0288\n",
      "Interaction: global_subjectivity_num_hrefs, Adjusted R-squared: 0.0288\n",
      "Interaction: n_tokens_title_kw_min_avg, Adjusted R-squared: 0.0288\n",
      "Interaction: kw_min_avg_n_tokens_title, Adjusted R-squared: 0.0288\n",
      "Interaction: kw_max_avg_global_subjectivity, Adjusted R-squared: 0.0288\n",
      "Interaction: global_subjectivity_kw_max_avg, Adjusted R-squared: 0.0288\n",
      "Interaction: num_hrefs_title_sentiment_polarity, Adjusted R-squared: 0.0288\n",
      "Interaction: title_sentiment_polarity_num_hrefs, Adjusted R-squared: 0.0288\n",
      "Interaction: num_self_hrefs_kw_max_avg, Adjusted R-squared: 0.0288\n",
      "Interaction: kw_max_avg_num_self_hrefs, Adjusted R-squared: 0.0288\n",
      "Interaction: title_sentiment_polarity_kw_min_avg, Adjusted R-squared: 0.0288\n",
      "Interaction: kw_min_avg_title_sentiment_polarity, Adjusted R-squared: 0.0288\n",
      "Interaction: num_hrefs_kw_min_avg, Adjusted R-squared: 0.0288\n",
      "Interaction: kw_min_avg_num_hrefs, Adjusted R-squared: 0.0288\n"
     ]
    }
   ],
   "source": [
    "# Create interaction terms\n",
    "interaction_terms = []\n",
    "for predictor1 in selected_features:\n",
    "    for predictor2 in selected_features:\n",
    "        if predictor1 != predictor2:\n",
    "            interaction_name = f\"{predictor1}_{predictor2}\"\n",
    "            train_df_selected[interaction_name] = train_df_selected[predictor1] * train_df_selected[predictor2]\n",
    "            interaction_terms.append(interaction_name)\n",
    "\n",
    "# Prepare data with interaction terms\n",
    "data_with_interactions = pd.concat([train_df_selected, train_df_selected[interaction_terms]], axis=1).copy()\n",
    "\n",
    "# Run multilinear regression with interaction terms\n",
    "results = []\n",
    "for interaction in interaction_terms:\n",
    "    formula = f'shares ~ {\" + \".join(selected_features + [interaction])}'\n",
    "    ml_model = smf.ols(formula=formula, data=data_with_interactions).fit()\n",
    "    results.append((interaction, ml_model.rsquared_adj))\n",
    "\n",
    "results.sort(key=lambda x: x[1], reverse=True)\n",
    "for interaction_name, adj_r2 in results:\n",
    "    print(f\"Interaction: {interaction_name}, Adjusted R-squared: {adj_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "26859fd8-ebf9-460b-b2b9-acb94fdab1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction: self_reference_min_shares_global_subjectivity\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 shares   R-squared:                       0.031\n",
      "Model:                            OLS   Adj. R-squared:                  0.031\n",
      "Method:                 Least Squares   F-statistic:                     63.96\n",
      "Date:                Fri, 11 Aug 2023   Prob (F-statistic):          1.46e-166\n",
      "Time:                        11:34:45   Log-Likelihood:            -2.6440e+05\n",
      "No. Observations:               25756   AIC:                         5.288e+05\n",
      "Df Residuals:                   25742   BIC:                         5.289e+05\n",
      "Df Model:                          13                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "====================================================================================================================\n",
      "                                                       coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "Intercept                                         1791.7596    877.754      2.041      0.041      71.313    3512.206\n",
      "num_self_hrefs                                     -60.5472     12.320     -4.914      0.000     -84.696     -36.398\n",
      "num_hrefs                                           39.5657      4.469      8.854      0.000      30.807      48.324\n",
      "num_keywords                                        91.5751     24.370      3.758      0.000      43.808     139.342\n",
      "self_reference_min_shares                           -0.0772      0.013     -5.752      0.000      -0.103      -0.051\n",
      "kw_max_avg                                           0.0815      0.008     10.453      0.000       0.066       0.097\n",
      "global_subjectivity                               4578.3279    526.128      8.702      0.000    3547.088    5609.568\n",
      "num_imgs                                            29.0827      5.662      5.136      0.000      17.985      40.181\n",
      "data_channel                                       141.9827     23.560      6.026      0.000      95.803     188.162\n",
      "average_token_length                              -784.8161    161.347     -4.864      0.000   -1101.064    -468.568\n",
      "n_tokens_title                                      51.7184     20.744      2.493      0.013      11.059      92.378\n",
      "title_sentiment_polarity                           696.5217    164.405      4.237      0.000     374.279    1018.765\n",
      "kw_min_avg                                           0.2073      0.041      5.061      0.000       0.127       0.288\n",
      "self_reference_min_shares_global_subjectivity[0]     0.0997      0.014      7.252      0.000       0.073       0.127\n",
      "self_reference_min_shares_global_subjectivity[1]     0.0997      0.014      7.252      0.000       0.073       0.127\n",
      "==============================================================================\n",
      "Omnibus:                    38031.287   Durbin-Watson:                   1.997\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):         15651852.115\n",
      "Skew:                           9.091   Prob(JB):                         0.00\n",
      "Kurtosis:                     122.391   Cond. No.                     3.06e+15\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 1.58e-18. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n",
      "Interaction: kw_max_avg_num_imgs\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 shares   R-squared:                       0.031\n",
      "Model:                            OLS   Adj. R-squared:                  0.030\n",
      "Method:                 Least Squares   F-statistic:                     62.40\n",
      "Date:                Fri, 11 Aug 2023   Prob (F-statistic):          2.44e-162\n",
      "Time:                        11:34:45   Log-Likelihood:            -2.6441e+05\n",
      "No. Observations:               25756   AIC:                         5.289e+05\n",
      "Df Residuals:                   25742   BIC:                         5.290e+05\n",
      "Df Model:                          13                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=============================================================================================\n",
      "                                coef    std err          t      P>|t|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------------\n",
      "Intercept                  1625.2494    877.449      1.852      0.064     -94.601    3345.100\n",
      "num_self_hrefs              -54.3202     12.374     -4.390      0.000     -78.574     -30.066\n",
      "num_hrefs                    38.5828      4.474      8.625      0.000      29.814      47.351\n",
      "num_keywords                 88.6563     24.380      3.636      0.000      40.870     136.443\n",
      "self_reference_min_shares     0.0185      0.002      8.215      0.000       0.014       0.023\n",
      "kw_max_avg                    0.0548      0.009      5.999      0.000       0.037       0.073\n",
      "global_subjectivity        5423.6140    511.092     10.612      0.000    4421.845    6425.383\n",
      "num_imgs                     -7.6656      8.607     -0.891      0.373     -24.536       9.205\n",
      "data_channel                137.2753     23.607      5.815      0.000      91.004     183.547\n",
      "average_token_length       -796.8166    161.534     -4.933      0.000   -1113.432    -480.201\n",
      "n_tokens_title               53.1908     20.751      2.563      0.010      12.518      93.863\n",
      "title_sentiment_polarity    693.2208    164.469      4.215      0.000     370.852    1015.590\n",
      "kw_min_avg                    0.1984      0.041      4.837      0.000       0.118       0.279\n",
      "kw_max_avg_num_imgs[0]        0.0030      0.001      5.732      0.000       0.002       0.004\n",
      "kw_max_avg_num_imgs[1]        0.0030      0.001      5.732      0.000       0.002       0.004\n",
      "==============================================================================\n",
      "Omnibus:                    38116.752   Durbin-Watson:                   1.996\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):         15842044.232\n",
      "Skew:                           9.128   Prob(JB):                         0.00\n",
      "Kurtosis:                     123.119   Cond. No.                     1.15e+15\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 2.38e-16. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n",
      "Interaction: self_reference_min_shares_num_imgs\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 shares   R-squared:                       0.031\n",
      "Model:                            OLS   Adj. R-squared:                  0.031\n",
      "Method:                 Least Squares   F-statistic:                     63.51\n",
      "Date:                Fri, 11 Aug 2023   Prob (F-statistic):          2.43e-165\n",
      "Time:                        11:34:45   Log-Likelihood:            -2.6440e+05\n",
      "No. Observations:               25756   AIC:                         5.288e+05\n",
      "Df Residuals:                   25742   BIC:                         5.290e+05\n",
      "Df Model:                          13                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=========================================================================================================\n",
      "                                            coef    std err          t      P>|t|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "Intercept                              1436.8589    875.346      1.641      0.101    -278.868    3152.586\n",
      "num_self_hrefs                          -54.1126     12.394     -4.366      0.000     -78.405     -29.821\n",
      "num_hrefs                                39.0016      4.471      8.724      0.000      30.239      47.764\n",
      "num_keywords                             86.6735     24.382      3.555      0.000      38.883     134.464\n",
      "self_reference_min_shares                 0.0101      0.003      3.523      0.000       0.004       0.016\n",
      "kw_max_avg                                0.0796      0.008     10.182      0.000       0.064       0.095\n",
      "global_subjectivity                    5380.5100    511.323     10.523      0.000    4378.288    6382.732\n",
      "num_imgs                                 18.5613      6.079      3.053      0.002       6.646      30.477\n",
      "data_channel                            139.9525     23.584      5.934      0.000      93.727     186.178\n",
      "average_token_length                   -779.0322    161.394     -4.827      0.000   -1095.373    -462.692\n",
      "n_tokens_title                           54.5925     20.747      2.631      0.009      13.928      95.257\n",
      "title_sentiment_polarity                686.7355    164.437      4.176      0.000     364.430    1009.041\n",
      "kw_min_avg                                0.1975      0.041      4.816      0.000       0.117       0.278\n",
      "self_reference_min_shares_num_imgs[0] -1.435e+10    1.1e+11     -0.131      0.896   -2.29e+11    2.01e+11\n",
      "self_reference_min_shares_num_imgs[1]  1.435e+10    1.1e+11      0.131      0.896   -2.01e+11    2.29e+11\n",
      "==============================================================================\n",
      "Omnibus:                    38066.583   Durbin-Watson:                   1.996\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):         15726817.777\n",
      "Skew:                           9.106   Prob(JB):                         0.00\n",
      "Kurtosis:                     122.678   Cond. No.                     6.20e+14\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 2.01e-15. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlgAAAGwCAYAAAB1mRuuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABbbUlEQVR4nO3deVxU5f4H8M+ADotsosDAFRFxxQ03EHNNEvewxTQzXDI1yYXKpUzUMsuyzN1uv6uWmUtu1yUTcUtFURFxXxBFBURFGEHWmfP7g8vRie0MzDAz8Hm/XvOKOeeZw/dAXT73eZ7zPDJBEAQQERERkc6YGboAIiIioqqGAYuIiIhIxxiwiIiIiHSMAYuIiIhIxxiwiIiIiHSMAYuIiIhIxxiwiIiIiHSshqELqK7UajUSExNha2sLmUxm6HKIiIhIAkEQ8PTpU7i5ucHMrOR+KgYsA0lMTIS7u7uhyyAiIqJyuHv3LurVq1fieQYsA7G1tQVQ8Auys7MzcDVEREQkhVKphLu7u/h3vCQMWAZSOCxoZ2fHgEVERGRiyprew0nuRERERDrGgEVERESkYwxYRERERDrGgEVERESkYwxYRERERDrGgEVERESkYwxYRERERDrGgEVERESkYwxYRERERDrGldyJiIioWlOpBUTFpyLlaTacbS3h6+kIc7PSV2ovCwMWERERVUlSgtO+i0mYu+syktKzxWOu9pYIG+iNPi1dy/29GbCIiIjIpBUXpMIvJ5cZnPZdTMKE9dEQ/nG95PRsTFgfjZXvtCt3yGLAIiIiIpNVXA+Ug3VNpD3LK9L2xeD0ircCc3ddLhKuAEAAIAMwd9dlvOKtKNdwIQMWERERmaSSeqCKC1eAZnCytaypEcqKa5uUno2o+FT4e9XRujY+RUhEREQmR6UWSuyBKk1hcIqMeyypfcrTkkNYaRiwiIiIyORExaeW2gNVNmnRzNnWslxXZ8AiIiIik6FSC4iMe4w/LyZV6Dr+DevC1d4SJc2ukqFgUryvp2O5rs85WERERGQSipvQri0ZAIW9JTp51UHYQG9MWB8NGTT7swpDV9hA73Kvh8UeLCIiIjJqKrWAHw/cwPj10RUOV8Dz4NSnpStWvtMOCnvNYUCFvWWFlmgA2INFRERERqhwbavwy8nYfu4+npTwZGBxCnuk/rlcg6KYBUT7tHTFK94KruROREREVdOLoWpHTCJSM3PLdZ3CICU1OJmbycq1FENpGLCIiIjI4HQxv+pdfw/0bemqEaR0HZykYsAiIiIigyppwVBt9W3parBA9U8MWERERGQw5V0w9EWFTwaWd0kFfeBThERERGQwFV8wtEBFllTQB/ZgERERkcGUdyuaQq7FPBloDBiwiIiIyGDKuxUNAEwNaIyQlxsbVc9VIQYsIiIiMhhfT0e42lsiOT1b8jwsY+21ehEDFhERERmMuZmsxC1rXuRYqyYG+/wLAd4KnSwEqm8MWERERGRQhVvW/HMdLFMLVS9iwCIiIiKD09eWNYbCgEVEREQGVbhFTmGwGtDazWSDVSEGLCIiIjKY4rbIMYVJ7GXhQqNERERkEIVb5PxzodHk9GxMWB+NfReTDFRZxTFgERERUaUrbYucwmNzd12GSl3RHQoNgwGLiIiIKl1ZW+QIAJLSsxEVn1p5RekQAxYRERFVOqlb5FR0Kx1DYcAiIiKiSid1i5yKbKVjSAxYREREVOkKt8gpaTEGGQqeJvT1dKzMsnSGAYuIiIgqXeEWOQCKhKzC92EDvU12PSyjCVhHjx7FwIED4ebmBplMhh07dojn8vLyMH36dLRq1Qq1atWCm5sb3n33XSQmJmpco0GDBpDJZBqvr7/+WqNNbGwsunbtCktLS7i7u2PhwoVFatmyZQuaNWsGS0tLtGrVCnv37tU4LwgCZs+eDVdXV1hZWSEgIAA3btzQ3Q+DiIioGijcIkdhrzkMqLC3xMp32pn0OlhGs9BoZmYm2rRpg9GjR+O1117TOPfs2TNER0fj888/R5s2bfDkyRNMnjwZgwYNwpkzZzTazps3D2PHjhXf29rail8rlUr07t0bAQEBWLVqFS5cuIDRo0fDwcEB77//PgDgxIkTGDZsGBYsWIABAwZgw4YNCAoKQnR0NFq2bAkAWLhwIZYsWYJ169bB09MTn3/+OQIDA3H58mVYWprmWDEREZEhVLUtckSCEQIgbN++vdQ2UVFRAgDhzp074jEPDw/hhx9+KPEzK1asEGrXri3k5OSIx6ZPny40bdpUfD9kyBChf//+Gp/z8/MTxo0bJwiCIKjVakGhUAjffvuteD4tLU2wsLAQfv/9dym3JwiCIKSnpwsAhPT0dMmfISIiIsOS+vfbaIYItZWeng6ZTAYHBweN419//TXq1KmDtm3b4ttvv0V+fr54LjIyEt26dYNcLhePBQYG4tq1a3jy5InYJiAgQOOagYGBiIyMBADEx8cjOTlZo429vT38/PzENsXJycmBUqnUeBEREVHVZDRDhNrIzs7G9OnTMWzYMNjZ2YnHJ02ahHbt2sHR0REnTpzAzJkzkZSUhO+//x4AkJycDE9PT41rubi4iOdq166N5ORk8diLbZKTk8V2L36uuDbFWbBgAebOnVvOOyYiIiJTYnIBKy8vD0OGDIEgCFi5cqXGudDQUPHr1q1bQy6XY9y4cViwYAEsLCwqu1QNM2fO1KhPqVTC3d3dgBURERGRvpjUEGFhuLpz5w7Cw8M1eq+K4+fnh/z8fNy+fRsAoFAo8ODBA402he8VCkWpbV48/+LnimtTHAsLC9jZ2Wm8iIiIqGoymYBVGK5u3LiBAwcOoE6dOmV+JiYmBmZmZnB2dgYA+Pv74+jRo8jLyxPbhIeHo2nTpqhdu7bYJiIiQuM64eHh8Pf3BwB4enpCoVBotFEqlTh16pTYhoiIiMqmUguIjHuMnTH3ERn32GQ3di6O0QwRZmRk4ObNm+L7+Ph4xMTEwNHREa6urnjjjTcQHR2N3bt3Q6VSifOdHB0dIZfLERkZiVOnTqFnz56wtbVFZGQkpk6dinfeeUcMT2+//Tbmzp2LMWPGYPr06bh48SJ+/PFH/PDDD+L3nTx5Mrp3745Fixahf//+2LhxI86cOYOffvoJACCTyTBlyhR8+eWXaNy4sbhMg5ubG4KCgirvB0ZERGSiVGoByw7exJrj8UjLet7p4WpvibCB3ia9/lUhmSAIRhEXDx8+jJ49exY5HhwcjDlz5hSZnF7o0KFD6NGjB6Kjo/HBBx/g6tWryMnJgaenJ0aMGIHQ0FCN+VexsbGYOHEiTp8+jbp16+LDDz/E9OnTNa65ZcsWzJo1C7dv30bjxo2xcOFC9OvXTzwvCALCwsLw008/IS0tDV26dMGKFSvQpEkTyferVCphb2+P9PR0DhcSEVG1se9iEmZsu4C0Z3lFzhWufGXMi4xK/fttNAGrumHAIiKi6mbfxSSMXx9dahsZClZyPzb9ZaNcbFTq32+TmYNFREREpkulFjB31+Uy2wkAktKzERWfqv+i9IgBi4iIiPQuKj4VSenZktunPJXe1hgxYBEREZHeaRuYnG1Ne29fBiwiIiLSO20Ck6t9wYbPpowBi4iIiPTO19MRrvaWKGvaugxA2EBvo5zgrg0GLCIiItI7czMZwgZ6A0CJIau2dU2jXqJBGwxYREREVCn6tHTFynfaQWGvOVzoYFUTUwMa48ysV6pEuAKMaCV3IiIiqvr6tHTFK94KRMWnIuVpNpxtC+ZbmfqQ4D+xB4uIiIgqjUotVPlwBbAHi4iIiCrJvotJmLvrssZ6WFVp/8EXsQeLiIiI9G7fxSRMWB9dZLHR5PRsTFgfjX0XkwxUmX4wYBEREZFeFW6TU9zmx4XH5u66DJW66myPzIBFREREelXWNjlVZf/BFzFgERERkV5J3SbH1PcffBEDFhEREemV1G1yTH3/wRcxYBEREZFe+Xo6QmFXcniSoWrsP/giBiwiIiLSq/DLycjOVxV7rnAFrKqw/+CLuA4WERER6U3h8gwlPR/oYF0TC15rxXWwiIiIiKQobXmGQhY1zPCKt6LSaqosDFhERESkF2UtzwAAycqcKrU8QyEGLCIiItKLn/+Ok9SuKi3PUIgBi4iIiHRub2wiIq4+lNS2Ki3PUIgBi4iIiHRKpRYwa+dFSW3r1JJXqeUZCjFgERERkc6o1ALWHo9HamaepPav+rhVqeUZCnGZBiIiItKJfReTMHfX5TIntr+oKj5BCDBgERERkQ6Utd5Vcarq8CDAIUIiIiKqIJVawIxtF7QKVwDwxastq+TwIMCARURERBW07OANpD2TNueq0MvNnNCvddVavf1FDFhERERUbiq1gNVHb2n9ubFdvfRQjfFgwCIiIqJyO3nrMZ7lFr+Rc0mq8tyrQgxYREREVG6RcY+1/kxVnntViAGLiIiIKkC7qe3junlW6blXhRiwiIiIqNz8G9aV1M7WsgZWvN0WM/t567ki48B1sIiIiKjcOnnVgYN1zVKfIqxlYY6zs16BvEb16depPndKREREOmduJsNXQa1KbbPozTbVKlwBRhSwjh49ioEDB8LNzQ0ymQw7duzQOC8IAmbPng1XV1dYWVkhICAAN27c0GiTmpqK4cOHw87ODg4ODhgzZgwyMjI02sTGxqJr166wtLSEu7s7Fi5cWKSWLVu2oFmzZrC0tESrVq2wd+9erWshIiKq6lRqAYvDryN0c0yx5x2samDVO+3Qp2XVn3P1T0YTsDIzM9GmTRssX7682PMLFy7EkiVLsGrVKpw6dQq1atVCYGAgsrOf73c0fPhwXLp0CeHh4di9ezeOHj2K999/XzyvVCrRu3dveHh44OzZs/j2228xZ84c/PTTT2KbEydOYNiwYRgzZgzOnTuHoKAgBAUF4eLFi1rVQkREVJXtu5iEVnP+wuKIG8jOVxfbJi0rv5KrMh4yQRC0Xdle72QyGbZv346goCAABT1Gbm5u+Oijj/Dxxx8DANLT0+Hi4oK1a9di6NChuHLlCry9vXH69Gl06NABALBv3z7069cP9+7dg5ubG1auXInPPvsMycnJkMvlAIAZM2Zgx44duHr1KgDgrbfeQmZmJnbv3i3W06lTJ/j4+GDVqlWSapFCqVTC3t4e6enpsLOz08nPjYiIqDLsu5iE8eujJbV1tbfEsekvV5llGaT+/TaaHqzSxMfHIzk5GQEBAeIxe3t7+Pn5ITIyEgAQGRkJBwcHMVwBQEBAAMzMzHDq1CmxTbdu3cRwBQCBgYG4du0anjx5IrZ58fsUtin8PlJqKU5OTg6USqXGi4iIyNSo1ALm/PeS5PZJ6dmIik/VY0XGySQCVnJyMgDAxcVF47iLi4t4Ljk5Gc7Ozhrna9SoAUdHR402xV3jxe9RUpsXz5dVS3EWLFgAe3t78eXu7l7GXRMRERmfqPhUJCtztPpMytPqN4XGJAJWVTBz5kykp6eLr7t37xq6JCIiIq2VJyw521rqoRLjZhIBS6FQAAAePHigcfzBgwfiOYVCgZSUFI3z+fn5SE1N1WhT3DVe/B4ltXnxfFm1FMfCwgJ2dnYaLyIiIlOjbVhytbes8vsOFsckApanpycUCgUiIiLEY0qlEqdOnYK/vz8AwN/fH2lpaTh79qzY5uDBg1Cr1fDz8xPbHD16FHl5zxdDCw8PR9OmTVG7dm2xzYvfp7BN4feRUgsREVFV9SRTu+HBsIHeVWaCuzaMJmBlZGQgJiYGMTExAAomk8fExCAhIQEymQxTpkzBl19+if/+97+4cOEC3n33Xbi5uYlPGjZv3hx9+vTB2LFjERUVhePHjyMkJARDhw6Fm5sbAODtt9+GXC7HmDFjcOnSJWzatAk//vgjQkNDxTomT56Mffv2YdGiRbh69SrmzJmDM2fOICQkBAAk1UJERFQV7buYhA82nJPUtpaFebVdAwswomUaDh8+jJ49exY5HhwcjLVr10IQBISFheGnn35CWloaunTpghUrVqBJkyZi29TUVISEhGDXrl0wMzPD66+/jiVLlsDGxkZsExsbi4kTJ+L06dOoW7cuPvzwQ0yfPl3je27ZsgWzZs3C7du30bhxYyxcuBD9+vUTz0uppSxcpoGIiEyJSi2g7by/oMxWldk2pKcXpr7StEr2XEn9+200Aau6YcAiIiJT8uOB6/jhgLRdSz7v3xxjujbUc0WGUaXWwSIiIiLDUakFrDl+W3L7O6nP9FeMiWDAIiIiolJFxaciLSuv7Ib/4+ForcdqTAMDFhEREZVKm7WvzGTACP8G+ivGRDBgERERUam0WftqbFdPyGswXvAnQERERKVq71EbUp4HHNPFEzP7eeu9HlPAgEVERESlGvpTJKQsORDQ3KXsRtUEAxYRERGV6L11UYhOSJPUtjpu6lwSBiwiIiIq1u6Y+zhw5aHk9tVxU+eSMGARERFRESq1gKmbz0tu72Bds1pu6lwSBiwiIiIqYmnEdeSppW/2MqqzZ5XcGqe8GLCIiIhIg0ot4N/H4iW3t6pphpCXG+mxItPDgEVEREQaouJTkZlT9qbOhRa+0Ya9V//AgEVEREQakpXSnwZsX98BA9u46bEa08SARURERBpSM3IktTOXAZvHd9ZzNaaJAYuIiIg0ONaSS2r3rr8HhwZLwIBFREREGhT2VpLa9W7hqudKTBcDFhEREWnw9XSEq33pi4a62lty3atSMGARERGRBnMzGQa1Kbl3SgYgbKA3hwdLwYBFREREGvZdTMLqoyWvg/V+N0/0acnhwdIwYBEREZFIpRYwY9uFUttsPnMPKi1Wea+OGLCIiIhIdPLWY6Q9yyu1zZNneTh563ElVWSaGLCIiIhItP7kHUntIuMYsErDgEVEREQACoYHj15/KLE1hwhLw4BFREREAP63B2GutD0I/RvW1XM1po0Bi4iIiAAAKU+l7UFoLTdHJ686eq7GtDFgEREREQAg/mGGpHbvd23INbDKoHXAio6OxoULzx/f3LlzJ4KCgvDpp58iNzdXp8URERFR5VCpBayLvC2pbccGXMG9LFoHrHHjxuH69esAgFu3bmHo0KGwtrbGli1bMG3aNJ0XSERERPoXFZ+KJ8/yJbV9lJmj52pMn9YB6/r16/Dx8QEAbNmyBd26dcOGDRuwdu1abN26Vdf1ERERUSVIVkqbfwUAzral71NI5QhYgiBArVYDAA4cOIB+/foBANzd3fHo0SPdVkdERESVIjVDWq+UnWUNbvIsgdYBq0OHDvjyyy/x66+/4siRI+jfvz8AID4+Hi4uLjovkIiIiPTv3pNnktq93u5fnOAugdYBa/HixYiOjkZISAg+++wzNGrUCADwxx9/oHPnzjovkIiIiPRLpRaw+ew9SW17t+Amz1LU0PYDrVu31niKsNC3334Lc3NznRRFRERElefkrcfIzCl7gVHHWnIOD0pUrnWw0tLS8PPPP2PmzJlITU0FAFy+fBkpKSk6LY6IiIj0T+q+gn6ejhwelEjrHqzY2Fj06tULDg4OuH37NsaOHQtHR0ds27YNCQkJ+OWXX/RRJxEREenJrYdPJbXzcqql50qqDq17sEJDQzFq1CjcuHEDlpbPH9Ps168fjh49qtPiiIiISL9UagGRt6T1YHH/Qem0DlinT5/GuHHjihz/17/+heTkZJ0UVZwGDRpAJpMVeU2cOBEA0KNHjyLnxo8fr3GNhIQE9O/fH9bW1nB2dsYnn3yC/HzNRdUOHz6Mdu3awcLCAo0aNcLatWuL1LJ8+XI0aNAAlpaW8PPzQ1RUlN7um4iISJ+kLjBqY1GD+w9qQeuAZWFhAaVSWeT49evX4eTkpJOiinP69GkkJSWJr/DwcADAm2++KbYZO3asRpuFCxeK51QqFfr374/c3FycOHEC69atw9q1azF79myxTXx8PPr374+ePXsiJiYGU6ZMwXvvvYe//vpLbLNp0yaEhoYiLCwM0dHRaNOmDQIDAzn/jIiITJLUDZ6HdKjH+Vda0DpgDRo0CPPmzUNeXh4AQCaTISEhAdOnT8frr7+u8wILOTk5QaFQiK/du3fDy8sL3bt3F9tYW1trtLGzsxPP7d+/H5cvX8b69evh4+ODvn374osvvsDy5cvFPRRXrVoFT09PLFq0CM2bN0dISAjeeOMN/PDDD+J1vv/+e4wdOxajRo2Ct7c3Vq1aBWtra/znP//R270TERHpi9RV2V/xVui5kqpF64C1aNEiZGRkwNnZGVlZWejevTsaNWoEW1tbzJ8/Xx81FpGbm4v169dj9OjRkMmep+nffvsNdevWRcuWLTFz5kw8e/Z80bTIyEi0atVKYzHUwMBAKJVKXLp0SWwTEBCg8b0CAwMRGRkpft+zZ89qtDEzM0NAQIDYpiQ5OTlQKpUaLyIiIkNr71EbsjI6psxkBe1IOq2fIrS3t0d4eDiOHTuG2NhYZGRkoF27dkWCiT7t2LEDaWlpGDlypHjs7bffhoeHB9zc3BAbG4vp06fj2rVr2LZtGwAgOTm5yErzhe8L546V1EapVCIrKwtPnjyBSqUqts3Vq1dLrXnBggWYO3duue6XiIhIX1YevglBKL2NWgDO3nkCf87BkkzrgFWoS5cu6NKliy5rkez//u//0LdvX7i5uYnH3n//ffHrVq1awdXVFb169UJcXBy8vLwMUaaGmTNnIjQ0VHyvVCrh7u5uwIqIiKi6U6kFLI64Iamt1LlaVEBSwFqyZInkC06aNKncxUhx584dHDhwQOyZKomfnx8A4ObNm/Dy8oJCoSjytN+DBw8AAAqFQvxn4bEX29jZ2cHKygrm5uYwNzcvtk3hNUpiYWEBCwuLsm+QiIiokry58liZvVeFpM7VogKSAtaLk7xLI5PJ9B6w1qxZA2dnZ3GT6ZLExMQAAFxdC/ZM8vf3x/z585GSkgJnZ2cAQHh4OOzs7ODt7S222bt3r8Z1wsPD4e/vDwCQy+Vo3749IiIiEBQUBABQq9WIiIhASEiIrm6RiIhI77JyVYi+K20+sFVNM26RoyVJASs+Pl7fdUiiVquxZs0aBAcHo0aN56XHxcVhw4YN6NevH+rUqYPY2FhMnToV3bp1Q+vWrQEAvXv3hre3N0aMGIGFCxciOTkZs2bNwsSJE8WepfHjx2PZsmWYNm0aRo8ejYMHD2Lz5s3Ys2eP+L1CQ0MRHByMDh06wNfXF4sXL0ZmZiZGjRpVuT8MIiKiChj36xnJbZu42HKJBi2Vew6WIRw4cAAJCQkYPXq0xnG5XI4DBw6IYcfd3R2vv/46Zs2aJbYxNzfH7t27MWHCBPj7+6NWrVoIDg7GvHnzxDaenp7Ys2cPpk6dih9//BH16tXDzz//jMDAQLHNW2+9hYcPH2L27NlITk6Gj48P9u3bV2TiOxERkbFSqQWclLh6OwAMaO2qx2qqJpkglD36Ghoaii+++AK1atXSmKhdnO+//15nxVVlSqUS9vb2SE9P11ivi4iISN8i4x5j2L9PSm5//cu+kNfQemWnKknq329JPVjnzp0TFxY9d+6cbiokIiIig9DmicDRLzVguCoHSQHr0KFDxX5NREREpufAZWl7B7vYyTF7YAs9V1M1aR1JR48ejadPnxY5npmZWWRuFBERERmXvbGJ2BUrLWD9Pa2XnqupurQOWOvWrUNWVlaR41lZWfjll190UhQRERHpnkot4OMt5yW3P3vniR6rqdokP0WoVCohCAIEQcDTp09hafl8wTGVSoW9e/eK60sRERGR8Tl56zGe5aklt+fq7eUnOWA5ODhAJpNBJpOhSZMmRc7LZDLutUdERGTEFv1V+r65/8TV28tPcsA6dOgQBEHAyy+/jK1bt8LR8fmKrnK5XNxomYiIiIzP3thERN9Nl9zezrIGV2+vAMkBq3v37gAKVnV3d3eHmRkf2SQiIjIF2s69AoD5g1tx9fYK0Holdw8PD6SlpSEqKgopKSlQqzXHct99912dFUdEREQVp+3cq5ZudhjYhqNSFaF1wNq1axeGDx+OjIwM2NnZQSZ7nm5lMhkDFhERkZE5EfdIq/af9ffWUyXVh9bjfB999BFGjx6NjIwMpKWl4cmTJ+IrNTVVHzUSERFRBdx/UnR5pZJw7pVuaB2w7t+/j0mTJsHa2lof9RAREZGO3X/yTHJbzr3SDa0DVmBgIM6cOaOPWoiIiEjHVGoBlxKVkto2qGPNuVc6ovUcrP79++OTTz7B5cuX0apVK9SsWVPj/KBBg3RWHBEREVVMVHyq5Anu84Na6bma6kPrgDV27FgAwLx584qck8lkUKlUFa+KiIiIdELqauzWcnN08qqj52qqD60D1j+XZSAiIiLjVdfGQlK797s25NwrHeJqoURERFWZIK1ZxwZ8clCXtO7BAoDMzEwcOXIECQkJyM3N1Tg3adIknRRGREREFffzsThJ7VIycvRcSfWidcA6d+4c+vXrh2fPniEzMxOOjo549OgRrK2t4ezszIBFRERkJHLz1Th0Tdoio6kMWDql9RDh1KlTMXDgQDx58gRWVlY4efIk7ty5g/bt2+O7777TR41ERERUDp9ui5Xc1rGWXI+VVD9aB6yYmBh89NFHMDMzg7m5OXJycuDu7o6FCxfi008/1UeNREREpCWVWsDO8/clt1fYW+mxmupH64BVs2ZNmJkVfMzZ2RkJCQkAAHt7e9y9e1e31REREVG5nLz1GHkSV06yqGHG7XF0TOs5WG3btsXp06fRuHFjdO/eHbNnz8ajR4/w66+/omXLlvqokYiIiLQUGfdYcltfz9pcokHHtO7B+uqrr+Dq6goAmD9/PmrXro0JEybg4cOH+Omnn3ReIBEREZWHxPUZAPRo4qzHOqonrXuwOnToIH7t7OyMffv26bQgIiIiqrjdsdLmX8lkwAj/BvotphriQqNERERVzH+j7+H2Y2lb5PRtoYC8BuOArmndg+Xp6QmZrORx2lu3blWoICIiIio/lVrAtO0XJLdv6GSjx2qqL60D1pQpUzTe5+Xl4dy5c9i3bx8++eQTXdVFRERE5RAVn4rsPG32DZY+V4uk0zpgTZ48udjjy5cvx5kzZypcEBEREZXfqiM3tGrv37Cuniqp3nQ26Nq3b19s3bpVV5cjIiIiLeXmq3HkuvTlGWzkNdDJq44eK6q+dBaw/vjjDzg6cpEyIiIiQ5m57bxW7Re+0ZrrX+lJuRYafXGSuyAISE5OxsOHD7FixQqdFkdERETSqNQCtkcnSm7v7WqLfq1d9VhR9aZ1wAoKCtJ4b2ZmBicnJ/To0QPNmjXTVV1ERESkhRM3H0Gbqe1bJ7ykt1qoHAErLCxMH3UQERFRBWyLvie5rUcdK1jJzfVYDWkdsO7fv4+tW7fi+vXrkMvlaNq0KYYMGYLatWvroz4iIiKS4NbDDMltvwpqrcdKCNBykvuKFSvg5eWFKVOmYP369fjPf/6DCRMmoF69evj9998BFMzJOnfunM4LnTNnDmQymcbrxSHJ7OxsTJw4EXXq1IGNjQ1ef/11PHjwQOMaCQkJ6N+/P6ytreHs7IxPPvkE+fn5Gm0OHz6Mdu3awcLCAo0aNcLatWuL1LJ8+XI0aNAAlpaW8PPzQ1RUlM7vl4iISCqVWsD5+0pJbeU1zPjkYCWQHLD27NmDSZMmISQkBPfv30daWhrS0tJw//59jBs3DsHBwTh27BiGDx+OXbt26aXYFi1aICkpSXwdO3ZMPDd16lTs2rULW7ZswZEjR5CYmIjXXntNPK9SqdC/f3/k5ubixIkTWLduHdauXYvZs2eLbeLj49G/f3/07NkTMTExmDJlCt577z389ddfYptNmzYhNDQUYWFhiI6ORps2bRAYGIiUlBS93DMREVFZ3lh5rOxG/9OjiROfHKwEMkEQJC3h2qNHD3Tp0gVffvllsednzZqFRYsWQaFQ4PDhw/Dw8NBpoXPmzMGOHTsQExNT5Fx6ejqcnJywYcMGvPHGGwCAq1evonnz5oiMjESnTp3w559/YsCAAUhMTISLiwsAYNWqVZg+fToePnwIuVyO6dOnY8+ePbh48aJ47aFDhyItLU3c1NrPzw8dO3bEsmXLAABqtRru7u748MMPMWPGDMn3o1QqYW9vj/T0dNjZ2ZX3x0JERNXc/D2X8O+/b0tuH9LTCx8H8qG08pL691tyD1Z0dDRGjBhR4vkRI0YgJycHR44c0Xm4KnTjxg24ubmhYcOGGD58OBISEgAAZ8+eRV5eHgICAsS2zZo1Q/369REZGQkAiIyMRKtWrcRwBQCBgYFQKpW4dOmS2ObFaxS2KbxGbm4uzp49q9HGzMwMAQEBYpuS5OTkQKlUaryIiIgqIjdfrVW4Arhye2WRHLBUKhVq1qxZ4vmaNWvCysoK9evX10lh/+Tn54e1a9di3759WLlyJeLj49G1a1c8ffoUycnJkMvlcHBw0PiMi4sLkpOTAQDJycka4arwfOG50toolUpkZWXh0aNHUKlUxbYpvEZJFixYAHt7e/Hl7u6u9c+AiIjoRV2/OaBV+xpm4PyrSiI5YLVo0QI7d+4s8fyOHTvQokULnRRVnL59++LNN99E69atERgYiL179yItLQ2bN2/W2/fUpZkzZyI9PV183b1719AlERGRCftv9D08eJqn1WcmdPPi/KtKIjlgTZw4EZ999hlWrFih8eRdfn4+li9fjlmzZuGDDz7QS5HFcXBwQJMmTXDz5k0oFArk5uYiLS1No82DBw+gUCgAAAqFoshThYXvy2pjZ2cHKysr1K1bF+bm5sW2KbxGSSwsLGBnZ6fxIiIiKg+VWsDkzdptiwMAU3o31UM1VBzJASs4OBgffPABQkJCUKdOHbRr1w5t27ZFnTp1MGnSJIwbNw4jR47UY6maMjIyEBcXB1dXV7Rv3x41a9ZERESEeP7atWtISEiAv78/AMDf3x8XLlzQeNovPDwcdnZ28Pb2Ftu8eI3CNoXXkMvlaN++vUYbtVqNiIgIsQ0REZG+HbvxEJKeUHvBD0N82HtViSQ/RVjo5MmT+P3333Hjxg0AQOPGjTFs2DB06tRJLwUW+vjjjzFw4EB4eHggMTERYWFhiImJweXLl+Hk5IQJEyZg7969WLt2Lezs7PDhhx8CAE6cOAGgYA6Zj48P3NzcsHDhQiQnJ2PEiBF477338NVXXwEoWKahZcuWmDhxIkaPHo2DBw9i0qRJ2LNnDwIDAwEULNMQHByM1atXw9fXF4sXL8bmzZtx9erVInOzSsOnCImIqLyafrYXOSrpf77dHa3w97SX9VhR9SH177fWK7l36tRJ72GqOPfu3cOwYcPw+PFjODk5oUuXLjh58iScnJwAAD/88APMzMzw+uuvIycnB4GBgRqbT5ubm2P37t2YMGEC/P39UatWLQQHB2PevHliG09PT+zZswdTp07Fjz/+iHr16uHnn38WwxUAvPXWW3j48CFmz56N5ORk+Pj4YN++fVqFKyIiovKau+uCVuGqpjkYrgxA6x4s0g32YBERkbZy89VoMutPrT5z/cu+kNfQauMWKoXO18EiIiIiw1p34rZW7X0bODBcGQh/6kRERCZiScR1rdqvf48PYBkKAxYREZEJ+GL3RTzNUUlu38zFhr1XBlSun3x+fj4OHDiA1atX4+nTpwCAxMREZGRk6LQ4IiIiKph79X/H7mj1me0Tu+ipGpJC66cI79y5gz59+iAhIQE5OTl45ZVXYGtri2+++QY5OTlYtWqVPuokIiKqtn7++5ZW7V3s5LCSm+upGpJC6x6syZMno0OHDnjy5AmsrKzE44MHDy6ySCcRERFVnLZzr7573Uc/hZBkWvdg/f333zhx4gTkcrnG8QYNGuD+/fs6K4yIiIiArFwVsvOlr6hkJgM6N66rx4pICq17sNRqNVSqopPs7t27B1tbW50URURERAXeX3daq/aL3uSWOMZA64DVu3dvLF68WHwvk8mQkZGBsLAw9OvXT5e1ERERVWsqtYDjcY8lt3eykWNwu3/psSKSSushwkWLFiEwMBDe3t7Izs7G22+/jRs3bqBu3br4/fff9VEjERFRtRQVnwq1Fu1Pfhqgt1pIO1oHrHr16uH8+fPYuHEjYmNjkZGRgTFjxmD48OEak96JiIioYu48zpTc1rOOFYcGjYjWAQsAatSogXfeeUfXtRAREdEL5u26JLntUN/6eqyEtCUpYP33v/+VfMFBgwaVuxgiIiIqsDvmPp7lSR8gHPVSQz1WQ9qSFLCCgoIkXUwmkxX7hCERERFJp1ILCNkYI7m9ZU0zbotjZCQFLLVamyl2REREVBGL9l3Rqv3Enuy9MjaMu0REREZEpRaw4mi8Vp8Z162xnqqh8ipXwIqIiMCAAQPg5eUFLy8vDBgwAAcOHNB1bURERNXOa8uOaNXe2UbO4UEjpPVvZMWKFejTpw9sbW0xefJkTJ48GXZ2dujXrx+WL1+ujxqJiIiqhaxcFc4nSl+aAQDCQ3vopxiqEJkgCNI3OELBOlgzZsxASEiIxvHly5fjq6++4n6EEimVStjb2yM9PR12dnaGLoeIiIzA8J9O4PitJ5Lb21vVwPmwQD1WRP8k9e+31j1YaWlp6NOnT5HjvXv3Rnp6uraXIyIiIvxvWxwtwhUAnP7sFT1VQxWldcAaNGgQtm/fXuT4zp07MWDAAJ0URUREVN0cu/FQq/aBzZw598qIab2Su7e3N+bPn4/Dhw/D398fAHDy5EkcP34cH330EZYsWSK2nTRpku4qJSIiqsImb4zWqv2KdzvoqRLSBa3nYHl6ekq7sEyGW7dulauo6oBzsIiIqFBWrgrNZ++T3P6Drg0xrX9zPVZEJZH691vrHqz4eO3W5iAiIqLSdfpqv1btP+rbTE+VkK5w8JaIiMiAMrLzkZ4tfceUBo6WMDeT6bEi0gWte7AEQcAff/yBQ4cOISUlpcg2Otu2bdNZcURERFVd+y/+0qr9zpBueqqEdEnrgDVlyhSsXr0aPXv2hIuLC2QypmgiIqLySH+WhxyV9PY1zAB765r6K4h0RuuA9euvv2Lbtm3o16+fPuohIiKqNgavOKZV+6hPue6VqdB6Dpa9vT0aNuSu3URERBWhUgu49eiZ5PYyAI42cv0VRDqldcCaM2cO5s6di6ysLH3UQ0REVC0cuabdwqJDOv5LT5WQPmg9RDhkyBD8/vvvcHZ2RoMGDVCzpuZYcHS0dgulERERVUdh/72oVfs5A1vpqRLSB60DVnBwMM6ePYt33nmHk9yJiIjKQaUWcPeJ9JEgJxs5rOTmeqyIdE3rgLVnzx789ddf6NKliz7qISIiqvJeXXJIq/aHPu6pp0pIX7Seg+Xu7s6tXYiIiMopK1eFi8nSe6/MANhYat0fQtoSBOD2bZ1dTuuAtWjRIkybNg23dVgEERFRdaHNnoMA4ONur6dKqrnHj4E//wTmzAH69gXq1gU8PQuO64DWkfidd97Bs2fP4OXlBWtr6yKT3FNTU3VSGBERUVUzfZv2D4KtGeWnh0qqmZwcICYGOHUKiIoq+OfNm0XbyeXA1avASy9V+FtqHbAWL15c4W9aHgsWLMC2bdtw9epVWFlZoXPnzvjmm2/QtGlTsU2PHj1w5MgRjc+NGzcOq1atEt8nJCRgwoQJOHToEGxsbBAcHIwFCxagRo3nP4rDhw8jNDQUly5dgru7O2bNmoWRI0dqXHf58uX49ttvkZycjDZt2mDp0qXw9fXVz80TEZHJy81XY1NUklafcbKWc+V2bQkCEBdXEKIKXzExQG5u0baNGwN+fs9fbdoUhCwdKNdThIZw5MgRTJw4ER07dkR+fj4+/fRT9O7dG5cvX0atWrXEdmPHjsW8efPE99bW1uLXKpUK/fv3h0KhwIkTJ5CUlIR3330XNWvWxFdffQUAiI+PR//+/TF+/Hj89ttviIiIwHvvvQdXV1cEBgYCADZt2oTQ0FCsWrUKfn5+WLx4MQIDA3Ht2jU4OztX0k+EiIhMSZNZf2r9mZOzAvRQSRXz+PHzXqnCHqriRtPq1CkIUb6+z//p6Ki3smSCIAjl/XB2djZy/5EIK2sC/MOHD+Hs7IwjR46gW7eCjS979OgBHx+fEnvZ/vzzTwwYMACJiYlwcXEBAKxatQrTp0/Hw4cPIZfLMX36dOzZswcXLz5fn2To0KFIS0vDvn0F4+Z+fn7o2LEjli1bBgBQq9Vwd3fHhx9+iBkzZkiqX6lUwt7eHunp6XxogIioiuv1zX7EPcnT6jMLBrfCML/6eqrIRGkz1Ne2rWbvVMOGgA6WlpL691vrHqzMzExMnz4dmzdvxuNiJoKpVFrsWlkB6enpAADHf6TP3377DevXr4dCocDAgQPx+eefi71YkZGRaNWqlRiuACAwMBATJkzApUuX0LZtW0RGRiIgQPP/MQQGBmLKlCkAgNzcXJw9exYzZ84Uz5uZmSEgIACRkZEl1puTk4OcnBzxvVKpLN+NExGRSem3+JDW4UoGMFwZyVBfeWkdsKZNm4ZDhw5h5cqVGDFiBJYvX4779+9j9erV+Prrr/VRYxFqtRpTpkzBSy+9hJYtW4rH3377bXh4eMDNzQ2xsbGYPn06rl27hm3btgEAkpOTNcIVAPF9cnJyqW2USiWysrLw5MkTqFSqYttcvXq1xJoXLFiAuXPnlv+miYjI5Axa9jcuJ0vfb7DQhTmBeqjGyBnpUF95aR2wdu3ahV9++QU9evTAqFGj0LVrVzRq1AgeHh747bffMHz4cH3UqWHixIm4ePEijh3T3IX8/fffF79u1aoVXF1d0atXL8TFxcHLy0vvdZVm5syZCA0NFd8rlUq4u7sbsCIiItKnjOx8xN7TfrTCw8Gq6q97ZQRDffqm9W8wNTUVDRs2BFAw36pwWYYuXbpgwoQJuq2uGCEhIdi9ezeOHj2KevXqldrWz6/g0dabN2/Cy8sLCoUCUVFRGm0ePHgAAFAoFOI/C4+92MbOzg5WVlYwNzeHubl5sW0Kr1EcCwsLWFhYSLtJIiIyed0XHijX5w5Oq2Krtpv4UF95aR2wGjZsiPj4eNSvXx/NmjXD5s2b4evri127dsHBwUEPJRYQBAEffvghtm/fjsOHD8PT07PMz8TExAAAXF1dAQD+/v6YP38+UlJSxKf9wsPDYWdnB29vb7HN3r17Na4THh4Of39/AIBcLkf79u0RERGBoKAgAAVDlhEREQgJCdHFrRIRkYnLylXh8TPt5yT/MMQH5mbG3ztTKm2H+gqH+Yx0qK+8tA5Yo0aNwvnz59G9e3fMmDEDAwcOxLJly5CXl4fvv/9eHzUCKBgW3LBhA3bu3AlbW1txzpS9vT2srKwQFxeHDRs2oF+/fqhTpw5iY2MxdepUdOvWDa1btwYA9O7dG97e3hgxYgQWLlyI5ORkzJo1CxMnThR7l8aPH49ly5Zh2rRpGD16NA4ePIjNmzdjz549Yi2hoaEIDg5Ghw4d4Ovri8WLFyMzMxOjRo3S2/0TEZHp0Ha1dqBgzavB7f6lh2r0qBoM9ZVXhZZpAIDbt28jOjoajRo1EoOMPshK+CWsWbMGI0eOxN27d/HOO+/g4sWLyMzMhLu7OwYPHoxZs2ZpPEZ5584dTJgwAYcPH0atWrUQHByMr7/+ushCo1OnTsXly5dRr149fP7550UWGl22bJm40KiPjw+WLFkiDklKwWUaiIiqpnf/fQxH49K1/lzcV/2Mu/eqmg71/ZPUv98VDlhUPgxYRERVz+6Y+wjZGKP151a90w59WrrqvqCKKM9Qn58f0LFjlRrq+yedr4MVGRmJx48fY8CAAeKxX375BWFhYcjMzERQUBCWLl3KidxERFQtqdRCucLVojfbGD5cvTjUVximONRXIZID1rx589CjRw8xYF24cAFjxozByJEj0bx5c3z77bdwc3PDnDlz9FUrERGR0fL6dG/Zjf7B1sIcr7cv/Yl4neNQX6WQHLBiYmLwxRdfiO83btwIPz8//Pvf/wYAuLu7IywsjAGLiIiqnQYz9pTdqBhnP++t40qKwaE+g5AcsJ48eaKxevmRI0fQt29f8X3Hjh1x9+5d3VZHRERk5Mobrt72dYe8hplui+FQn9GQHLBcXFwQHx8Pd3d35ObmIjo6WmPrl6dPn6JmzZp6KZKIiMgYlTdcyQB89VoFn7wXhILw9GLvFIf6jIbkgNWvXz/MmDED33zzDXbs2AFra2t07dpVPB8bG2vw7WiIiIgqS3nDFQDEf91f+w9xqM+kSA5YX3zxBV577TV0794dNjY2WLduHeQvpN///Oc/6N27EsaSiYiIDKwi4Sruq35lN9JmqK9du+cbH3Ooz2hIDlh169bF0aNHkZ6eDhsbG5ibm2uc37JlC2xsbHReIBERkbHIzVejyaw/y/35YrfCKRzqe3E1dA71mTytt8qxt7cv9rgjux+JiKgKm73zIn6JvFPuz9dzsCrYCodDfdWC1gGLiIioumk2609k56u1/pw8Pw/eKbfQNvEawlwygMbjONRXTTBgERERlULyfCtBQIMnifBJuo42SdfRNvEamqfcgoUqv2hbDvVVeQxYRERExUhOy0anryNKPO+QpYRP4nX4JF2DT2JBqKqd/bRoQw71VUsMWERERC9QqQU0nbUXL44IFg71+SReg0/SNbRJug7PJ0lFPptjXgOXXLxw3rUJzrk1xZIfxnOor5piwCIiIvqf/zsajy/2XJI81Herthti3JoixrUJYtya4oqzJ/LMCxbdvl2eta6oymDAIiKi6u3xY6QePIZfftwEn8TrOFfCUF+qlZ0YpGJcm+C8axOkW9kWe0mGK2LAIiKi6uOfC3ieOgXExcERwJQXm/1vqC/GtSli3JogxrUpEhwUkob6GK4IYMAiIqKqSosFPEsb6pPKwUKGmLkSVmmnaoEBi4iIqgYtFvCMa9ACO63cyxzqkyp61itwtOEyC/QcAxYREZmeEob6iihcwNPPD/D1xXmXpnh1f5JOn+rjkCAVhwGLiIiM24tDfYU9U1ru1Xc49gFGbjgDIJnhiioFAxYRERkXHe7Vt/9MIt7/45xeymS4otIwYBERkeFIHeqzsADatn0epnx9S13Ac9/p+xi/NUYvJR+f9jL+5Will2tT1cGARURElUMHQ31luZ+ahZcWHtR97QA2jPRD52Z19XJtqnoYsIiISD90ONRXlhNXH+Httad0VHhRHA4kbTFgERFRxelpqK8s64/cwKw/r1es9lJsfb8z2jesrbfrU9XFgEVERNrRZqivSZOCEKXlUF9pvvrvGfx04kGFriEFe62oIhiwiIiodJU41FeSGVuOY+PZNJ1cqyyb3/OHbyPd1E3VFwMWERE9Z6ChvuJM3XAY22MzdXY9KdhrRbrCgEVEVF0ZeKjvn3ZE3sGUnRd1ek2pDoX2gKdzLYN8b6qaGLCIiKqLx481Nz42wFDfixbujsaKY0k6v642uPQC6QsDFhFRVVTeoT4/P8DTU6dDfQBwMCYZozee1ek1K4JPB5K+MWAREZk6IxvqO3n9MYb+56ROr6krDFZUWRiwiIhMjZEM9W04ehOf7r2ms+vpE58MpMrGgEVEZMwMONS3+M/zWHzkXvlrNwJzAhthZM+mhi6DqiEGLCIiY/HPob5TpwrCVV5e0bblHOqbs/0U1p56pPvajcySoFYY1Km+ocugaowBqwKWL1+Ob7/9FsnJyWjTpg2WLl0KX19fQ5dFRKaiHEN9p+wb4v1bNZBuZVtw7h6AeynA1vBKLd1Y7Rj/EnwaOBi6DCIGrPLatGkTQkNDsWrVKvj5+WHx4sUIDAzEtWvX4OzsbOjyiMiA9kbdwwfbzmsck+fnwTvlFnwSr8En6Rp8Eq+jQVrRJQpyzGvikktDxLg2RYxbU5xza4q79i4FQ33PUPCyqpz7MAV1rc0Q8XEA7K1rGroUIg0yQRAEQxdhivz8/NCxY0csW7YMAKBWq+Hu7o4PP/wQM2bMKPPzSqUS9vb2SE9Ph52dnb7LJSIAbWfswZPK+EaCgAZPEuGTdF0MVN4P4iFX5xdpGuf4L8S4NsF51yaIcWuKK86eyDNnWChNazc5/vigF+Q1zAxdClVDUv9+swerHHJzc3H27FnMnDlTPGZmZoaAgABERkYW+5mcnBzk5OSI75VKpd7rJDJ2m4/dwrTdVwxdRoU5ZCn/F6SuwyfxOtokXUft7KdF2qVa2SHmf0GqMFSJQ31Uqs4NauLX91+BuZlu1+ci0hcGrHJ49OgRVCoVXFxcNI67uLjg6tWrxX5mwYIFmDt3bmWUR6QT6c/y0GbefkOXYXR0MtRHksx6pSHe69Xc0GUQlQsDViWZOXMmQkNDxfdKpRLu7u4GrIiqml8OX8fsfTcMXUbVUo6hvhi3pjjv2oRDfeWw4rU26Odbz9BlEOkEA1Y51K1bF+bm5njw4IHG8QcPHkChUBT7GQsLC1hYWFRGeWRibiZnIGDxEUOXQXg+1Nf2f8N8HOrTn9fa2OD7Yd0NXQaR3jBglYNcLkf79u0RERGBoKAgAAWT3CMiIhASEmLY4sigktOy0fnrCKgNXQiViUN9lae3F/DT2P6GLoOoUjFglVNoaCiCg4PRoUMH+Pr6YvHixcjMzMSoUaMMXRrp2P3ULHRZeBB83NaEcaivUgxoLsey4FcMXQaRUWDAKqe33noLDx8+xOzZs5GcnAwfHx/s27evyMR3Mm5ZuSp8ui0a22NSDF0K6VDtZ+lok3SdQ3061reJOVaO7mPoMohMAtfBMhCug1X50p/lYfjqv3HxQZahSyEd4lBfxfRwB9ZO5PAdkVRcB4uqrYzsfISsj8LRm084F6qq4VBfEbUBnPuaAYnI2DBgkcnKylXh853nsTsmCdkqQ1dD+iB1qO+xlZ24EroxDvWZA4hjCCKqVhiwyCTk5qvx779vYn3kbTxU5qFofwWZOnl+Hlo8iEObpOuVNtS3bHBrDPDjenREpHsMWGRUVGoBJ248wuYzd3A24QkysvPxLE+NfI71VS1aDPWhSRPAz6/g5esLizZt0E4uR7vKr5qISDIGLDK4wlD148HrOHsnjcshGKkKPYL/6BEQFQWcOlXwz6goIDW1aLu6dQFf3+eBqmNHwNGxYoUTERkAAxYZTG6+GjO2nseOmESomap0yrqmDAc/ehkKB8vK/+Y5OcC5c88D1alTQFxc0XYWFkDbts/DlJ8f4OlZ7Z/qI6KqgQGLKpVKLeBk3GMs/OsKzt9TGroco7Trgy5oVd/e0GVIIwjAzZvPg9SpU0BMDJCXV7Tti0N9fn5A69aAXF7pJRMRVQYGLKoUKrWApRE3sPJIHHKqyYQqS3MZ9k/tgfp1rQ1diu5wqI+ISBIGLNKZwt6p43EPcf9JFgRBwKOMXCQps3Hn8bMqMQw4vpsXQns3gbyGmaFL0T8O9RERlRsDFlVIYaj65eRtHLyagjyVaaYoGYBWbnb49b1OsLeueotRlolDfUREOsWAReWiUgtYdvAmVh+Nw7Nc01rl087SHGO7eGFcD6/q0RNVnBeH+gqH+548Kdqubl1xeQTxn7VrV369REQmhgGLJCnsqYq89Qg3UjJw5PpDZOcZ71wqcxnwr9pWmD2gBXo2c4a5WTUeruJQHxFRpWPAomKp1AKi4lORnJ6F4zcfYe/FZKPrqZIBqGVhjlb/ssf47l7o0tipegcpgEN9RERGggGLxDCV8jQbdWtZ4PTtVKw9cRtpWcX8UTYAMwBWNc0gr2mOZgpbhqkXcaiPiMgoMWBVIy8GKWdbS7T3qI2Vh+Ow5ni80YSpQg3rWmNox/oY+ZJn9Z0n9U+FQ32FQYpDfURERosBq5rYdzEJc3ddRlJ6tnhMBhjVtjRedWthzqAW6NyoLnunONRHRGTSGLCqgX0XkzBhfXSRMGUM4cpMBgxu64YFr7Wp3j1V2g71FQ7zcaiPiMgoMWBVcSq1gLm7LhtFmHqRhbkME3p44cNeTapfbxWH+oiIqjwGrCouKj5VY1iwstSQAW3c7WFRwxzZ+SpY1jCHk60l6jlaobNXXXRqWKd6BCsO9RERVUsMWFWYSi3g+M1Hlfb9GjvVQu+WiuoVoP6JQ31ERAQGrCqruEnt+uJYqya+fLUl+rV20/v3Mioc6iMiohIwYFVBJU1q14VacnN0bVwX7T0cUdfWAgo7S/h6Olb93ioO9RERkRYYsKoYXU9qt7esgVe8XfBSY6fqE6YADvUREVGFMGBVMdpMajeTAeoXkpiDVU0Ed/aAr2cdPMrIgbNtNQlUHOojIiIdY8CqYlKeSgtXIT29MKlXE5y980Rc2b1ahClBAG7c0Oyd4lAfERHpGANWFeNsaymp3UuNnCCvYQZ/rzp6rsjAyjPU5+cHdOzIoT4iIio3BqwqxtfTEa72lkhOzy52HpYMgMK+oLeqyuFQHxERGQkGLBP3zw2cfT0dETbQGxPWRxfZa7AwPoQN9Db9oUAO9RERkRFjwDJhxa115WpvibCB3lj5Trsi5xT/O9enpashyq0YDvUREZEJkQmCYGzb1FULSqUS9vb2SE9Ph52dndafL2mtq8J+qZXvtMMr3ooivVsm0XP14lBfYZjiUB8RERkBqX+/2YNlgkpb60pAQciau+syXvFWGP8kdg71ERFRFcSAZYLKWutKAJCUno2o+FTjC1gc6iMiomqAAcsESV3rSmo7vdFmqK9du4JV0DnUR0REVQADlgmSutaV1HY6waE+IiIiEQOWCTKKta441EdERFQiBiwTZG4mq9y1rrKzC3qjCsPUqVPArVtF2xUO9RVufMyhPiIiqqbMDF2AFLdv38aYMWPg6ekJKysreHl5ISwsDLm5uRptZDJZkdfJkyc1rrVlyxY0a9YMlpaWaNWqFfbu3atxXhAEzJ49G66urrCyskJAQABu3Lih0SY1NRXDhw+HnZ0dHBwcMGbMGGRkZOjvB1CMPi1dsfKddlDYaw4DKuwtsfKdduVf60oQgOvXgfXrgQ8/LAhKdnaAvz8wZQrw++/Pw1WTJsCIEcCyZcDp04BSCZw4AfzwAzBsGNCwIcMVERFVSybRg3X16lWo1WqsXr0ajRo1wsWLFzF27FhkZmbiu+++02h74MABtGjRQnxfp87zp+hOnDiBYcOGYcGCBRgwYAA2bNiAoKAgREdHo2XLlgCAhQsXYsmSJVi3bh08PT3x+eefIzAwEJcvX4alZUGYGT58OJKSkhAeHo68vDyMGjUK77//PjZs2FAJP43n+rR0rfhaVxzqIyIi0jmTXWj022+/xcqVK3Hrf70pt2/fhqenJ86dOwcfH59iP/PWW28hMzMTu3fvFo916tQJPj4+WLVqFQRBgJubGz766CN8/PHHAID09HS4uLhg7dq1GDp0KK5cuQJvb2+cPn0aHTp0AADs27cP/fr1w7179+Dm5iap/oouNFouHOojIiKqkCq/0Gh6ejocHYtO4h40aBCys7PRpEkTTJs2DYMGDRLPRUZGIjQ0VKN9YGAgduzYAQCIj49HcnIyAgICxPP29vbw8/NDZGQkhg4disjISDg4OIjhCgACAgJgZmaGU6dOYfDgwcXWm5OTg5ycHPG9Uqks131LVvhU34sbH/OpPiIiokphkgHr5s2bWLp0qcbwoI2NDRYtWoSXXnoJZmZm2Lp1K4KCgrBjxw4xZCUnJ8PFxUXjWi4uLkhOThbPFx4rrY2zs7PG+Ro1asDR0VFsU5wFCxZg7ty55bxjCTjUR0REZDQMGrBmzJiBb775ptQ2V65cQbNmzcT39+/fR58+ffDmm29i7Nix4vG6detq9E517NgRiYmJ+PbbbzV6sQxl5syZGvUplUq4u7uX72Ic6iMiIjJqBg1YH330EUaOHFlqm4YNG4pfJyYmomfPnujcuTN++umnMq/v5+eH8PBw8b1CocCDBw802jx48AAKhUI8X3jM1dVVo03hvC6FQoGUlBSNa+Tn5yM1NVX8fHEsLCxgYWFRZs1FcKiPiIjI5Bg0YDk5OcHJyUlS2/v376Nnz55o37491qxZAzOzsleYiImJ0QhK/v7+iIiIwJQpU8Rj4eHh8Pf3BwB4enpCoVAgIiJCDFRKpRKnTp3ChAkTxGukpaXh7NmzaN++PQDg4MGDUKvV8PPzk3QvkqjVwKBBBcsecKiPiIjIpJjEHKz79++jR48e8PDwwHfffYeHDx+K5wp7jdatWwe5XI62bdsCALZt24b//Oc/+Pnnn8W2kydPRvfu3bFo0SL0798fGzduxJkzZ8TeMJlMhilTpuDLL79E48aNxWUa3NzcEBQUBABo3rw5+vTpg7Fjx2LVqlXIy8tDSEgIhg4dKvkJQknMzIC7dwvCFYf6iIiITItgAtasWSOgYMHyIq9Ca9euFZo3by5YW1sLdnZ2gq+vr7Bly5Yi19q8ebPQpEkTQS6XCy1atBD27NmjcV6tVguff/654OLiIlhYWAi9evUSrl27ptHm8ePHwrBhwwQbGxvBzs5OGDVqlPD06VOt7ik9PV0AIKSnp5fc6OBBQTh9WhBycrS6NhEREemHpL/fgiCY7DpYps4g62ARERFRhUj9+20SW+UQERERmRIGLCIiIiIdY8AiIiIi0jEGLCIiIiIdY8AiIiIi0jEGLCIiIiIdY8AiIiIi0jEGLCIiIiIdY8AiIiIi0jEGLCIiIiIdY8AiIiIi0jEGLCIiIiIdY8AiIiIi0jEGLCIiIiIdY8AiIiIi0rEahi6AiqdSC4iKT0XK02w421rC19MR5mYyQ5dFREREEjBgGaF9F5Mwd9dlJKVni8dc7S0RNtAbfVq6GrAyIiIikoJDhEZm38UkTFgfrRGuACA5PRsT1kdj38UkA1VGREREUjFgGRGVWsDcXZchFHOu8NjcXZehUhfXgoiIiIwFA5YRiYpPLdJz9SIBQFJ6NqLiUyuvKCIiItIaA5YRSXlacrgqTzsiIiIyDAYsI+Jsa6nTdkRERGQYDFhGxNfTEa72lihpMQYZCp4m9PV0rMyyiIiISEsMWEbE3EyGsIHeAFAkZBW+DxvozfWwiIiIjBwDlpHp09IVK99pB4W95jCgwt4SK99px3WwiIiITAAXGjVCfVq64hVvBVdyJyIiMlEMWEbK3EwGf686hi6DiIiIyoFDhEREREQ6xoBFREREpGMMWEREREQ6xoBFREREpGMMWEREREQ6xoBFREREpGMMWEREREQ6xoBFREREpGMMWEREREQ6xpXcDUQQBACAUqk0cCVEREQkVeHf7cK/4yVhwDKQp0+fAgDc3d0NXAkRERFp6+nTp7C3ty/xvEwoK4KRXqjVaiQmJsLW1hYy2fNNnJVKJdzd3XH37l3Y2dkZsMLKU93umfdb9VW3e+b9Vn3V7Z5Lu19BEPD06VO4ubnBzKzkmVbswTIQMzMz1KtXr8TzdnZ21eJf4hdVt3vm/VZ91e2eeb9VX3W755Lut7Seq0Kc5E5ERESkYwxYRERERDrGgGVkLCwsEBYWBgsLC0OXUmmq2z3zfqu+6nbPvN+qr7rdsy7ul5PciYiIiHSMPVhEREREOsaARURERKRjDFhEREREOsaARURERKRjDFgmIicnBz4+PpDJZIiJiTF0OXozaNAg1K9fH5aWlnB1dcWIESOQmJho6LL04vbt2xgzZgw8PT1hZWUFLy8vhIWFITc319Cl6dX8+fPRuXNnWFtbw8HBwdDl6Nzy5cvRoEEDWFpaws/PD1FRUYYuSW+OHj2KgQMHws3NDTKZDDt27DB0SXq1YMECdOzYEba2tnB2dkZQUBCuXbtm6LL0ZuXKlWjdurW42Ka/vz/+/PNPQ5dVab7++mvIZDJMmTKlXJ9nwDIR06ZNg5ubm6HL0LuePXti8+bNuHbtGrZu3Yq4uDi88cYbhi5LL65evQq1Wo3Vq1fj0qVL+OGHH7Bq1Sp8+umnhi5Nr3Jzc/Hmm29iwoQJhi5F5zZt2oTQ0FCEhYUhOjoabdq0QWBgIFJSUgxdml5kZmaiTZs2WL58uaFLqRRHjhzBxIkTcfLkSYSHhyMvLw+9e/dGZmamoUvTi3r16uHrr7/G2bNncebMGbz88st49dVXcenSJUOXpnenT5/G6tWr0bp16/JfRCCjt3fvXqFZs2bCpUuXBADCuXPnDF1Spdm5c6cgk8mE3NxcQ5dSKRYuXCh4enoauoxKsWbNGsHe3t7QZeiUr6+vMHHiRPG9SqUS3NzchAULFhiwqsoBQNi+fbuhy6hUKSkpAgDhyJEjhi6l0tSuXVv4+eefDV2GXj19+lRo3LixEB4eLnTv3l2YPHlyua7DHiwj9+DBA4wdOxa//vorrK2tDV1OpUpNTcVvv/2Gzp07o2bNmoYup1Kkp6fD0dHR0GVQOeTm5uLs2bMICAgQj5mZmSEgIACRkZEGrIz0JT09HQCqxX+zKpUKGzduRGZmJvz9/Q1djl5NnDgR/fv31/hvuTwYsIyYIAgYOXIkxo8fjw4dOhi6nEozffp01KpVC3Xq1EFCQgJ27txp6JIqxc2bN7F06VKMGzfO0KVQOTx69AgqlQouLi4ax11cXJCcnGygqkhf1Go1pkyZgpdeegktW7Y0dDl6c+HCBdjY2MDCwgLjx4/H9u3b4e3tbeiy9Gbjxo2Ijo7GggULKnwtBiwDmDFjBmQyWamvq1evYunSpXj69Clmzpxp6JIrROr9Fvrkk09w7tw57N+/H+bm5nj33XchmNCGA9reLwDcv38fffr0wZtvvomxY8caqPLyK889E5myiRMn4uLFi9i4caOhS9Grpk2bIiYmBqdOncKECRMQHByMy5cvG7osvbh79y4mT56M3377DZaWlhW+HrfKMYCHDx/i8ePHpbZp2LAhhgwZgl27dkEmk4nHVSoVzM3NMXz4cKxbt07fpeqE1PuVy+VFjt+7dw/u7u44ceKEyXRLa3u/iYmJ6NGjBzp16oS1a9fCzMz0/n9PeX7Ha9euxZQpU5CWlqbn6ipHbm4urK2t8ccffyAoKEg8HhwcjLS0tCrfEyuTybB9+3aNe6+qQkJCsHPnThw9ehSenp6GLqdSBQQEwMvLC6tXrzZ0KTq3Y8cODB48GObm5uIxlUoFmUwGMzMz5OTkaJwrSw19FEmlc3JygpOTU5ntlixZgi+//FJ8n5iYiMDAQGzatAl+fn76LFGnpN5vcdRqNYCCZSpMhTb3e//+ffTs2RPt27fHmjVrTDJcARX7HVcVcrkc7du3R0REhBgy1Go1IiIiEBISYtjiSCcEQcCHH36I7du34/Dhw9UuXAEF/06b0v8ea6NXr164cOGCxrFRo0ahWbNmmD59ulbhCmDAMmr169fXeG9jYwMA8PLyQr169QxRkl6dOnUKp0+fRpcuXVC7dm3ExcXh888/h5eXl8n0Xmnj/v376NGjBzw8PPDdd9/h4cOH4jmFQmHAyvQrISEBqampSEhIgEqlEtd1a9SokfjvuKkKDQ1FcHAwOnToAF9fXyxevBiZmZkYNWqUoUvTi4yMDNy8eVN8Hx8fj5iYGDg6Ohb536+qYOLEidiwYQN27twJW1tbcW6dvb09rKysDFyd7s2cORN9+/ZF/fr18fTpU2zYsAGHDx/GX3/9ZejS9MLW1rbIfLrC+cDlmmens+caSe/i4+Or9DINsbGxQs+ePQVHR0fBwsJCaNCggTB+/Hjh3r17hi5NL9asWSMAKPZVlQUHBxd7z4cOHTJ0aTqxdOlSoX79+oJcLhd8fX2FkydPGrokvTl06FCxv8vg4GBDl6YXJf33umbNGkOXphejR48WPDw8BLlcLjg5OQm9evUS9u/fb+iyKlVFlmngHCwiIiIiHTPNCR9ERERERowBi4iIiEjHGLCIiIiIdIwBi4iIiEjHGLCIiIiIdIwBi4iIiEjHGLCIiIiIdIwBi4iIiEjHGLCISOcOHz4MmUxmchs5y2Qy7NixQ2fXa9CgARYvXqyz6xnK7du3IZPJxG2NTPX3S1SZGLCISCsymazU15w5cwxdYpnmzJkDHx+fIseTkpLQt2/fSq0lNTUVU6ZMgYeHB+RyOdzc3DB69GgkJCRUah2FRo4cKW5WXcjd3R1JSUnl24+NqJriZs9EpJWkpCTx602bNmH27Nm4du2aeMzGxgZnzpwxRGnIzc2FXC4v9+cre5Pt1NRUdOrUCXK5HKtWrUKLFi1w+/ZtzJo1Cx07dkRkZCQaNmxYqTUVx9zcvEpvQE6kD+zBIiKtKBQK8WVvbw+ZTKZxzMbGRmx79uxZdOjQAdbW1ujcubNGEAOAnTt3ol27drC0tETDhg0xd+5c5Ofni+cTEhLw6quvwsbGBnZ2dhgyZAgePHggni/sifr555/h6ekJS0tLAEBaWhree+89ODk5wc7ODi+//DLOnz8PAFi7di3mzp2L8+fPi71ua9euBVB0iPDevXsYNmwYHB0dUatWLXTo0AGnTp0CAMTFxeHVV1+Fi4sLbGxs0LFjRxw4cECrn+Vnn32GxMREHDhwAH379kX9+vXRrVs3/PXXX6hZsyYmTpwoti1uuNHHx0ejx/D7779Hq1atUKtWLbi7u+ODDz5ARkaGeH7t2rVwcHDAX3/9hebNm8PGxgZ9+vQRQ/OcOXOwbt067Ny5U/zZHD58uMgQYXGOHTuGrl27wsrKCu7u7pg0aRIyMzPF8ytWrEDjxo1haWkJFxcXvPHGG1r9rIhMDQMWEenNZ599hkWLFuHMmTOoUaMGRo8eLZ77+++/8e6772Ly5Mm4fPkyVq9ejbVr12L+/PkAALVajVdffRWpqak4cuQIwsPDcevWLbz11lsa3+PmzZvYunUrtm3bJgaAN998EykpKfjzzz9x9uxZtGvXDr169UJqaireeustfPTRR2jRogWSkpKQlJRU5JoAkJGRge7du+P+/fv473//i/Pnz2PatGlQq9Xi+X79+iEiIgLnzp1Dnz59MHDgQMlDe2q1Ghs3bsTw4cOL9A5ZWVnhgw8+wF9//YXU1FTJP28zMzMsWbIEly5dwrp163Dw4EFMmzZNo82zZ8/w3Xff4ddff8XRo0eRkJCAjz/+GADw8ccfY8iQIWLoSkpKQufOncv8vnFxcejTpw9ef/11xMbGYtOmTTh27BhCQkIAAGfOnMGkSZMwb948XLt2Dfv27UO3bt0k3xeRSRKIiMppzZo1gr29fZHjhw4dEgAIBw4cEI/t2bNHACBkZWUJgiAIvXr1Er766iuNz/3666+Cq6urIAiCsH//fsHc3FxISEgQz1+6dEkAIERFRQmCIAhhYWFCzZo1hZSUFLHN33//LdjZ2QnZ2dka1/by8hJWr14tfq5NmzZF6gYgbN++XRAEQVi9erVga2srPH78WOJPQxBatGghLF26VHzv4eEh/PDDD8W2TU5OFgCUeH7btm0CAOHUqVMlXqtNmzZCWFhYifVs2bJFqFOnjvh+zZo1AgDh5s2b4rHly5cLLi4u4vvg4GDh1Vdf1bhOfHy8AEA4d+6cIAjPf79PnjwRBEEQxowZI7z//vsan/n7778FMzMzISsrS9i6datgZ2cnKJXKEmslqmo4B4uI9KZ169bi166urgCAlJQU1K9fH+fPn8fx48fFHisAUKlUyM7OxrNnz3DlyhW4u7vD3d1dPO/t7Q0HBwdcuXIFHTt2BAB4eHjAyclJbHP+/HlkZGSgTp06GrVkZWUhLi5Ocu0xMTFo27YtHB0diz2fkZGBOXPmYM+ePUhKSkJ+fj6ysrK0npwuCEKp57WZU3bgwAEsWLAAV69ehVKpRH5+vvjztLa2BgBYW1vDy8tL/IyrqytSUlK0qvmfzp8/j9jYWPz222/iMUEQoFarER8fj1deeQUeHh5o2LAh+vTpgz59+mDw4MFiTURVEQMWEelNzZo1xa9lMhkAaAyxzZ07F6+99lqRzxXOpZKiVq1aGu8zMjLg6uqKw4cPF2nr4OAg+bpWVlalnv/4448RHh6O7777Do0aNYKVlRXeeOMN5ObmSrq+k5OTGBaLc+XKFdSoUQOenp4ACob//hnG8vLyxK9v376NAQMGYMKECZg/fz4cHR1x7NgxjBkzBrm5uWKYefF3AhT8XsoKeWXJyMjAuHHjMGnSpCLn6tevD7lcjujoaBw+fBj79+/H7NmzMWfOHJw+fVqr3wmRKWHAIiKDaNeuHa5du4ZGjRoVe7558+a4e/cu7t69K/ZiXb58GWlpafD29i71usnJyahRowYaNGhQbBu5XA6VSlVqfa1bt8bPP/+M1NTUYnuxjh8/jpEjR2Lw4MEACkLG7du3S73mi8zMzDBkyBD89ttvmDdvnsY8rKysLKxYsQKDBw+Gvb09gIJA9uITnEqlEvHx8eL7s2fPQq1WY9GiRTAzK5heu3nzZsn1FJLys/mndu3a4fLlyyX+LgGgRo0aCAgIQEBAAMLCwuDg4ICDBw8WG7CJqgJOcicig5g9ezZ++eUXzJ07F5cuXcKVK1ewceNGzJo1CwAQEBCAVq1aYfjw4YiOjkZUVBTeffdddO/eHR06dCjxugEBAfD390dQUBD279+P27dv48SJE/jss8/E5SMaNGiA+Ph4xMTE4NGjR8jJySlynWHDhkGhUCAoKAjHjx/HrVu3sHXrVkRGRgIAGjduLE6sP3/+PN5++22xd06q+fPnQ6FQ4JVXXsGff/6Ju3fv4ujRowgMDISZmRl+/PFHse3LL7+MX3/9FX///TcuXLiA4OBgmJubi+cbNWqEvLw8LF26FLdu3cKvv/6KVatWaVVP4c8mNjYW165dw6NHjzR6yUoyffp0nDhxAiEhIYiJicGNGzewc+dOcZL77t27sWTJEsTExODOnTv45ZdfoFar0bRpU63rIzIVDFhEZBCBgYHYvXs39u/fj44dO6JTp0744Ycf4OHhAaBg6Grnzp2oXbs2unXrhoCAADRs2BCbNm0q9boymQx79+5Ft27dMGrUKDRp0gRDhw7FnTt34OLiAgB4/fXX0adPH/Ts2RNOTk74/fffi1xHLpdj//79cHZ2Rr9+/dCqVSt8/fXXYqj5/vvvUbt2bXTu3BkDBw5EYGAg2rVrp9XPoG7dujh58iR69uyJcePGwdPTE927d4dKpUJMTIw4bw0AZs6cie7du2PAgAHo378/goKCNOZStWnTBt9//z2++eYbtGzZEr/99hsWLFigVT0AMHbsWDRt2hQdOnSAk5MTjh8/XuZnWrdujSNHjuD69evo2rUr2rZti9mzZ8PNzQ1AwdDstm3b8PLLL6N58+ZYtWoVfv/9d7Ro0ULr+ohMhUyo6OA7ERHpzP/93//hgw8+wKZNm4qsqE5EpoM9WERERmTMmDHYuHEjrly5gqysLEOXQ0TlxB4sIiIiIh1jDxYRERGRjjFgEREREekYAxYRERGRjjFgEREREekYAxYRERGRjjFgEREREekYAxYRERGRjjFgEREREekYAxYRERGRjv0/F9ABUU+08UUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Top 3 interaction terms\n",
    "top_interactions = [\n",
    "    \"self_reference_min_shares_global_subjectivity\",\n",
    "    \"kw_max_avg_num_imgs\",\n",
    "    \"self_reference_min_shares_num_imgs\"\n",
    "]\n",
    "\n",
    "# Prepare data with top interaction terms\n",
    "data_with_top_interactions = pd.concat([train_df_selected, train_df_selected[top_interactions]], axis=1)\n",
    "\n",
    "# Run multilinear regression with top interaction terms\n",
    "for interaction in top_interactions:\n",
    "    formula = f'shares ~ {\" + \".join(selected_features + [interaction])}'\n",
    "    ml_model = smf.ols(formula=formula, data=data_with_top_interactions).fit()\n",
    "    print(f\"Interaction: {interaction}\")\n",
    "    print(ml_model.summary())\n",
    "    # Create a QQ plot for the residuals\n",
    "    sm.qqplot(ml_model.resid, line='s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d6872d-f93b-48cb-9931-002d4cfa0014",
   "metadata": {},
   "source": [
    "#### Summary of Results & Analysis: Exploring Predictor Interactions with Random Forest\n",
    "I incorporated interaction terms into the model based on the top-performing interactions from the Random Forest analysis. The selected interaction terms were:\n",
    "\n",
    "* self_reference_min_shares_global_subjectivity\n",
    "* kw_max_avg_num_imgs\n",
    "* self_reference_min_shares_num_imgs\n",
    "\n",
    "These interactions were chosen due to their relatively higher adjusted R-squared values and their potential to reveal intricate relationships between predictors.\n",
    "\n",
    "The multilinear regression models with interaction terms exhibited limited improvement in explaining the variance in the target variable, \"shares.\" The adjusted R-squared values for these models was 0.031. While statistically significant, these values indicate that only about 3.1% of the variation in the target variable could be explained by the included predictors and their interactions.\n",
    "\n",
    "This modest increase in adjusted R-squared as compared to the multilinear model with statistically significant predictors suggests that while these interaction terms contribute to the model's predictive capacity, they do not strongly enhance its overall performance. It's important to note that the inclusion of interaction terms can introduce multicollinearity and increase model complexity, potentially leading to overfitting. Thus far, the model with the highest R-squared is the multilinear model with all predictors.\n",
    "\n",
    "**Evaluating Regression: Diagnostic Plot:**     \n",
    "Consistent with previous analyses, the QQ plots maintain a non-linear pattern resembling a backward L shape. This persistent pattern underscores potential deviations from the assumption of normality in the residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db279853-2c0b-4594-ba7e-cce9f85d3203",
   "metadata": {},
   "source": [
    "#### Multicollinearity Detection: VIF\n",
    "As part of the comprehensive analysis of my multilinear regression model, I further investigated the presence of multicollinearity among predictor variables. Multicollinearity can distort the interpretation of individual predictors' effects and affect the overall stability of the regression model.\n",
    "\n",
    "VIF, or Variance Inflation Factor, is a widely used metric to assess multicollinearity. It quantifies how much the variance of the estimated regression coefficient is increased due to multicollinearity. A VIF value above a certain threshold (commonly 5 or 10) indicates high multicollinearity. Because the dataset includes over 40 predictors, I'll use a threshold of 5.\n",
    "\n",
    "By identifying and addressing predictors with high VIF, I aimed to enhance the model's interpretability and minimize the risk of multicollinearity-induced instability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "21f3ce10-870f-4d1b-9da6-e4af5f786fac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>average_token_length</th>\n",
       "      <th>num_keywords</th>\n",
       "      <th>kw_min_min</th>\n",
       "      <th>...</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>data_channel</th>\n",
       "      <th>weekday_is</th>\n",
       "      <th>Intercept</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35308</th>\n",
       "      <td>12.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>0.820359</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.625850</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.332143</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>-0.1250</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2595</th>\n",
       "      <td>11.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>0.762500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.690141</td>\n",
       "      <td>5.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.194444</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>-0.1000</td>\n",
       "      <td>0.60000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6100</th>\n",
       "      <td>10.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.748031</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>-0.1000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11827</th>\n",
       "      <td>9.0</td>\n",
       "      <td>292.0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>4.602740</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.475000</td>\n",
       "      <td>-0.700000</td>\n",
       "      <td>-0.2000</td>\n",
       "      <td>0.49513</td>\n",
       "      <td>0.211039</td>\n",
       "      <td>0.00487</td>\n",
       "      <td>0.211039</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12758</th>\n",
       "      <td>11.0</td>\n",
       "      <td>306.0</td>\n",
       "      <td>0.741573</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.205882</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.473438</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.1875</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       n_tokens_title  n_tokens_content  n_non_stop_unique_tokens  num_hrefs   \n",
       "35308            12.0             294.0                  0.820359        7.0  \\\n",
       "2595             11.0             284.0                  0.762500        1.0   \n",
       "6100             10.0             254.0                  0.745455        7.0   \n",
       "11827             9.0             292.0                  0.750000        6.0   \n",
       "12758            11.0             306.0                  0.741573       15.0   \n",
       "\n",
       "       num_self_hrefs  num_imgs  num_videos  average_token_length   \n",
       "35308             5.0       2.0         1.0              4.625850  \\\n",
       "2595              0.0       1.0         1.0              4.690141   \n",
       "6100              5.0       1.0         0.0              4.748031   \n",
       "11827             5.0       0.0        17.0              4.602740   \n",
       "12758             2.0      15.0         0.0              4.205882   \n",
       "\n",
       "       num_keywords  kw_min_min  ...  avg_negative_polarity   \n",
       "35308           7.0        -1.0  ...              -0.332143  \\\n",
       "2595            5.0       217.0  ...              -0.194444   \n",
       "6100            6.0         4.0  ...              -0.133333   \n",
       "11827           5.0         4.0  ...              -0.475000   \n",
       "12758          10.0         4.0  ...              -0.473438   \n",
       "\n",
       "       min_negative_polarity  max_negative_polarity  title_subjectivity   \n",
       "35308              -0.400000                -0.1250             0.00000  \\\n",
       "2595               -0.300000                -0.1000             0.60000   \n",
       "6100               -0.166667                -0.1000             0.00000   \n",
       "11827              -0.700000                -0.2000             0.49513   \n",
       "12758              -1.000000                -0.1875             0.00000   \n",
       "\n",
       "       title_sentiment_polarity  abs_title_subjectivity   \n",
       "35308                  0.000000                 0.50000  \\\n",
       "2595                   0.200000                 0.10000   \n",
       "6100                   0.000000                 0.50000   \n",
       "11827                  0.211039                 0.00487   \n",
       "12758                  0.000000                 0.50000   \n",
       "\n",
       "       abs_title_sentiment_polarity  data_channel  weekday_is  Intercept  \n",
       "35308                      0.000000             2           3          1  \n",
       "2595                       0.200000             3           4          1  \n",
       "6100                       0.000000             3           2          1  \n",
       "11827                      0.211039             4           4          1  \n",
       "12758                      0.000000             7           3          1  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get variables for which to compute VIF and add intercept term\n",
    "# set all values in intercept term to 1\n",
    "x_df_train['Intercept'] = 1\n",
    "x_df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "17b424d0-5045-41cc-97a1-c5b2a6e1b30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       variables           VIF\n",
      "0                 n_tokens_title  1.086528e+00\n",
      "1               n_tokens_content  2.508867e+00\n",
      "2       n_non_stop_unique_tokens  3.120861e+03\n",
      "3                      num_hrefs  1.624994e+00\n",
      "4                 num_self_hrefs  1.346270e+00\n",
      "5                       num_imgs  1.620733e+00\n",
      "6                     num_videos  1.238578e+00\n",
      "7           average_token_length  1.258831e+00\n",
      "8                   num_keywords  1.434711e+00\n",
      "9                     kw_min_min  3.754937e+00\n",
      "10                    kw_max_min  1.621238e+00\n",
      "11                    kw_min_max  1.356872e+00\n",
      "12                    kw_max_max  4.452995e+00\n",
      "13                    kw_avg_max  2.917122e+00\n",
      "14                    kw_min_avg  1.327269e+00\n",
      "15                    kw_max_avg  1.794655e+00\n",
      "16     self_reference_min_shares  5.388084e+00\n",
      "17     self_reference_max_shares  8.086168e+00\n",
      "18    self_reference_avg_sharess  1.660755e+01\n",
      "19                        LDA_00  5.743000e+06\n",
      "20                        LDA_01  3.864430e+06\n",
      "21                        LDA_02  6.422232e+06\n",
      "22                        LDA_03  6.665449e+06\n",
      "23                        LDA_04  6.818902e+06\n",
      "24           global_subjectivity  1.628999e+00\n",
      "25     global_sentiment_polarity  7.620645e+00\n",
      "26    global_rate_positive_words  3.959683e+00\n",
      "27    global_rate_negative_words  6.166344e+00\n",
      "28           rate_positive_words  1.938518e+02\n",
      "29           rate_negative_words  1.968686e+02\n",
      "30         avg_positive_polarity  4.019695e+00\n",
      "31         min_positive_polarity  1.837747e+00\n",
      "32         max_positive_polarity  2.384099e+00\n",
      "33         avg_negative_polarity  6.749696e+00\n",
      "34         min_negative_polarity  4.734122e+00\n",
      "35         max_negative_polarity  2.824733e+00\n",
      "36            title_subjectivity  2.378571e+00\n",
      "37      title_sentiment_polarity  1.327079e+00\n",
      "38        abs_title_subjectivity  1.424850e+00\n",
      "39  abs_title_sentiment_polarity  2.412687e+00\n",
      "40                  data_channel  1.567655e+00\n",
      "41                    weekday_is  1.014917e+00\n",
      "42                     Intercept  8.064023e+07\n"
     ]
    }
   ],
   "source": [
    "# Compute and view VIF\n",
    "vif = pd.DataFrame()\n",
    "# using variance inflation factor from Statsmodel\n",
    "vif['variables'] = x_df_train.columns\n",
    "vif[\"VIF\"] = [variance_inflation_factor(x_df_train.values, i) for i in range(x_df_train.shape[1])]\n",
    "print(vif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63633e7b-d655-4369-95be-6d6bf276959b",
   "metadata": {},
   "source": [
    "Upon analysis, I identified the following predictors with VIF values exceeding the designated threshold of 5. These predictors were consequently excluded from the dataset:\n",
    "* self_reference_min_shares\n",
    "* self_reference_max_shares\n",
    "* LDA_00\n",
    "* LDA_02\n",
    "* LDA_03\n",
    "* LDA_04\n",
    "* global_sentiment_polarity\n",
    "* global_rate_negative_words\n",
    "* avg_negative_polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c897d66a-1f9c-40ad-9629-1b23d68f8507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>average_token_length</th>\n",
       "      <th>num_keywords</th>\n",
       "      <th>kw_min_min</th>\n",
       "      <th>...</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>data_channel</th>\n",
       "      <th>weekday_is</th>\n",
       "      <th>Intercept</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35308</th>\n",
       "      <td>12.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>0.820359</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.625850</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>-0.1250</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2595</th>\n",
       "      <td>11.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>0.762500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.690141</td>\n",
       "      <td>5.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>-0.1000</td>\n",
       "      <td>0.60000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6100</th>\n",
       "      <td>10.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.748031</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>-0.1000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11827</th>\n",
       "      <td>9.0</td>\n",
       "      <td>292.0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>4.602740</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.700000</td>\n",
       "      <td>-0.2000</td>\n",
       "      <td>0.49513</td>\n",
       "      <td>0.211039</td>\n",
       "      <td>0.00487</td>\n",
       "      <td>0.211039</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12758</th>\n",
       "      <td>11.0</td>\n",
       "      <td>306.0</td>\n",
       "      <td>0.741573</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.205882</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.1875</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       n_tokens_title  n_tokens_content  n_non_stop_unique_tokens  num_hrefs   \n",
       "35308            12.0             294.0                  0.820359        7.0  \\\n",
       "2595             11.0             284.0                  0.762500        1.0   \n",
       "6100             10.0             254.0                  0.745455        7.0   \n",
       "11827             9.0             292.0                  0.750000        6.0   \n",
       "12758            11.0             306.0                  0.741573       15.0   \n",
       "\n",
       "       num_self_hrefs  num_imgs  num_videos  average_token_length   \n",
       "35308             5.0       2.0         1.0              4.625850  \\\n",
       "2595              0.0       1.0         1.0              4.690141   \n",
       "6100              5.0       1.0         0.0              4.748031   \n",
       "11827             5.0       0.0        17.0              4.602740   \n",
       "12758             2.0      15.0         0.0              4.205882   \n",
       "\n",
       "       num_keywords  kw_min_min  ...  max_positive_polarity   \n",
       "35308           7.0        -1.0  ...                    1.0  \\\n",
       "2595            5.0       217.0  ...                    0.8   \n",
       "6100            6.0         4.0  ...                    0.7   \n",
       "11827           5.0         4.0  ...                    1.0   \n",
       "12758          10.0         4.0  ...                    1.0   \n",
       "\n",
       "       min_negative_polarity  max_negative_polarity  title_subjectivity   \n",
       "35308              -0.400000                -0.1250             0.00000  \\\n",
       "2595               -0.300000                -0.1000             0.60000   \n",
       "6100               -0.166667                -0.1000             0.00000   \n",
       "11827              -0.700000                -0.2000             0.49513   \n",
       "12758              -1.000000                -0.1875             0.00000   \n",
       "\n",
       "       title_sentiment_polarity  abs_title_subjectivity   \n",
       "35308                  0.000000                 0.50000  \\\n",
       "2595                   0.200000                 0.10000   \n",
       "6100                   0.000000                 0.50000   \n",
       "11827                  0.211039                 0.00487   \n",
       "12758                  0.000000                 0.50000   \n",
       "\n",
       "       abs_title_sentiment_polarity  data_channel  weekday_is  Intercept  \n",
       "35308                      0.000000             2           3          1  \n",
       "2595                       0.200000             3           4          1  \n",
       "6100                       0.000000             3           2          1  \n",
       "11827                      0.211039             4           4          1  \n",
       "12758                      0.000000             7           3          1  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop predictors with VIF greater than 5\n",
    "columns_to_drop = ['self_reference_min_shares', 'self_reference_max_shares',\n",
    "                'LDA_00', 'LDA_02', 'LDA_03', 'LDA_04', 'global_sentiment_polarity',\n",
    "                'global_rate_negative_words', 'avg_negative_polarity']\n",
    "\n",
    "x_df_train = x_df_train.drop(columns_to_drop, axis=1)\n",
    "x_df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "14126f0b-889d-4021-9fd2-87ea18b2d1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       variables          VIF\n",
      "0                 n_tokens_title     1.073025\n",
      "1               n_tokens_content     1.999601\n",
      "2       n_non_stop_unique_tokens     1.335710\n",
      "3                      num_hrefs     1.574064\n",
      "4                 num_self_hrefs     1.261591\n",
      "5                       num_imgs     1.321363\n",
      "6                     num_videos     1.126530\n",
      "7           average_token_length     1.177110\n",
      "8                   num_keywords     1.393974\n",
      "9                     kw_min_min     3.745997\n",
      "10                    kw_max_min     1.610859\n",
      "11                    kw_min_max     1.341448\n",
      "12                    kw_max_max     4.346782\n",
      "13                    kw_avg_max     2.529692\n",
      "14                    kw_min_avg     1.314823\n",
      "15                    kw_max_avg     1.746513\n",
      "16    self_reference_avg_sharess     1.037253\n",
      "17                        LDA_01     1.174310\n",
      "18           global_subjectivity     1.462172\n",
      "19    global_rate_positive_words     1.867828\n",
      "20           rate_positive_words   193.340158\n",
      "21           rate_negative_words   193.069220\n",
      "22         avg_positive_polarity     2.548287\n",
      "23         min_positive_polarity     1.773965\n",
      "24         max_positive_polarity     2.364588\n",
      "25         min_negative_polarity     1.825794\n",
      "26         max_negative_polarity     1.184199\n",
      "27            title_subjectivity     2.372375\n",
      "28      title_sentiment_polarity     1.297834\n",
      "29        abs_title_subjectivity     1.419227\n",
      "30  abs_title_sentiment_polarity     2.401057\n",
      "31                  data_channel     1.209873\n",
      "32                    weekday_is     1.013317\n",
      "33                     Intercept  9020.129130\n"
     ]
    }
   ],
   "source": [
    "# Compute and view VIF to ensure all interactions are below 5\n",
    "vif2 = pd.DataFrame()\n",
    "# using variance inflation factor from Statsmodel\n",
    "vif2['variables'] = x_df_train.columns\n",
    "vif2[\"VIF\"] = [variance_inflation_factor(x_df_train.values, i) for i in range(x_df_train.shape[1])]\n",
    "print(vif2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc583560-d3b4-43d2-a0c6-7a7469c8e5f2",
   "metadata": {},
   "source": [
    "The rate_positive_words and rate_negative_words have a threshold over 5 and will be dropped from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4db33302-2420-4e20-b12c-a497a513f7e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>average_token_length</th>\n",
       "      <th>num_keywords</th>\n",
       "      <th>kw_min_min</th>\n",
       "      <th>...</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>data_channel</th>\n",
       "      <th>weekday_is</th>\n",
       "      <th>Intercept</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35308</th>\n",
       "      <td>12.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>0.820359</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.625850</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>-0.1250</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2595</th>\n",
       "      <td>11.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>0.762500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.690141</td>\n",
       "      <td>5.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>-0.1000</td>\n",
       "      <td>0.60000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6100</th>\n",
       "      <td>10.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.748031</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>-0.1000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11827</th>\n",
       "      <td>9.0</td>\n",
       "      <td>292.0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>4.602740</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.700000</td>\n",
       "      <td>-0.2000</td>\n",
       "      <td>0.49513</td>\n",
       "      <td>0.211039</td>\n",
       "      <td>0.00487</td>\n",
       "      <td>0.211039</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12758</th>\n",
       "      <td>11.0</td>\n",
       "      <td>306.0</td>\n",
       "      <td>0.741573</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.205882</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.1875</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       n_tokens_title  n_tokens_content  n_non_stop_unique_tokens  num_hrefs   \n",
       "35308            12.0             294.0                  0.820359        7.0  \\\n",
       "2595             11.0             284.0                  0.762500        1.0   \n",
       "6100             10.0             254.0                  0.745455        7.0   \n",
       "11827             9.0             292.0                  0.750000        6.0   \n",
       "12758            11.0             306.0                  0.741573       15.0   \n",
       "\n",
       "       num_self_hrefs  num_imgs  num_videos  average_token_length   \n",
       "35308             5.0       2.0         1.0              4.625850  \\\n",
       "2595              0.0       1.0         1.0              4.690141   \n",
       "6100              5.0       1.0         0.0              4.748031   \n",
       "11827             5.0       0.0        17.0              4.602740   \n",
       "12758             2.0      15.0         0.0              4.205882   \n",
       "\n",
       "       num_keywords  kw_min_min  ...  max_positive_polarity   \n",
       "35308           7.0        -1.0  ...                    1.0  \\\n",
       "2595            5.0       217.0  ...                    0.8   \n",
       "6100            6.0         4.0  ...                    0.7   \n",
       "11827           5.0         4.0  ...                    1.0   \n",
       "12758          10.0         4.0  ...                    1.0   \n",
       "\n",
       "       min_negative_polarity  max_negative_polarity  title_subjectivity   \n",
       "35308              -0.400000                -0.1250             0.00000  \\\n",
       "2595               -0.300000                -0.1000             0.60000   \n",
       "6100               -0.166667                -0.1000             0.00000   \n",
       "11827              -0.700000                -0.2000             0.49513   \n",
       "12758              -1.000000                -0.1875             0.00000   \n",
       "\n",
       "       title_sentiment_polarity  abs_title_subjectivity   \n",
       "35308                  0.000000                 0.50000  \\\n",
       "2595                   0.200000                 0.10000   \n",
       "6100                   0.000000                 0.50000   \n",
       "11827                  0.211039                 0.00487   \n",
       "12758                  0.000000                 0.50000   \n",
       "\n",
       "       abs_title_sentiment_polarity  data_channel  weekday_is  Intercept  \n",
       "35308                      0.000000             2           3          1  \n",
       "2595                       0.200000             3           4          1  \n",
       "6100                       0.000000             3           2          1  \n",
       "11827                      0.211039             4           4          1  \n",
       "12758                      0.000000             7           3          1  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop predictors with VIF greater than 5\n",
    "columns_to_drop = ['rate_positive_words', 'rate_negative_words']\n",
    "\n",
    "x_df_train = x_df_train.drop(columns_to_drop, axis=1)\n",
    "x_df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ac77cd4a-9f63-4427-874c-8037e4ea459c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       variables         VIF\n",
      "0                 n_tokens_title    1.072125\n",
      "1               n_tokens_content    1.979820\n",
      "2       n_non_stop_unique_tokens    1.003292\n",
      "3                      num_hrefs    1.569576\n",
      "4                 num_self_hrefs    1.260905\n",
      "5                       num_imgs    1.320565\n",
      "6                     num_videos    1.117591\n",
      "7           average_token_length    1.176463\n",
      "8                   num_keywords    1.393754\n",
      "9                     kw_min_min    3.745862\n",
      "10                    kw_max_min    1.610717\n",
      "11                    kw_min_max    1.341134\n",
      "12                    kw_max_max    4.346525\n",
      "13                    kw_avg_max    2.529553\n",
      "14                    kw_min_avg    1.313713\n",
      "15                    kw_max_avg    1.746246\n",
      "16    self_reference_avg_sharess    1.037203\n",
      "17                        LDA_01    1.171346\n",
      "18           global_subjectivity    1.455265\n",
      "19    global_rate_positive_words    1.450374\n",
      "20         avg_positive_polarity    2.546236\n",
      "21         min_positive_polarity    1.766104\n",
      "22         max_positive_polarity    2.347649\n",
      "23         min_negative_polarity    1.405993\n",
      "24         max_negative_polarity    1.131885\n",
      "25            title_subjectivity    2.369599\n",
      "26      title_sentiment_polarity    1.277577\n",
      "27        abs_title_subjectivity    1.416466\n",
      "28  abs_title_sentiment_polarity    2.400395\n",
      "29                  data_channel    1.204768\n",
      "30                    weekday_is    1.012599\n",
      "31                     Intercept  515.298531\n"
     ]
    }
   ],
   "source": [
    "# Compute and view VIF to ensure all interactions are below 5\n",
    "vif3 = pd.DataFrame()\n",
    "# using variance inflation factor from Statsmodel\n",
    "vif3['variables'] = x_df_train.columns\n",
    "vif3[\"VIF\"] = [variance_inflation_factor(x_df_train.values, i) for i in range(x_df_train.shape[1])]\n",
    "print(vif3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04425fd-802c-4820-a7f5-fdfec5f1b0b4",
   "metadata": {},
   "source": [
    "Now that VIF for all intereactions is below 5. I can rerun the multilinear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "396b954f-173d-472c-a319-98d7518ad761",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors_wo_multicol = [\n",
    "    'n_tokens_title', 'n_tokens_content', 'n_non_stop_unique_tokens',\n",
    "    'num_hrefs', 'num_self_hrefs', 'num_imgs',\n",
    "    'num_videos', 'average_token_length', 'num_keywords',\n",
    "    'kw_min_min', 'kw_max_min', 'kw_min_max',\n",
    "    'kw_max_max', 'kw_avg_max', 'kw_min_avg',\n",
    "    'kw_max_avg', 'self_reference_avg_sharess', 'LDA_01',\n",
    "    'global_subjectivity', 'global_rate_positive_words', 'avg_positive_polarity',\n",
    "    'min_positive_polarity', 'max_positive_polarity', 'min_negative_polarity',\n",
    "    'max_negative_polarity', 'title_subjectivity', 'title_sentiment_polarity',\n",
    "    'abs_title_subjectivity', 'abs_title_sentiment_polarity', 'data_channel',\n",
    "    'weekday_is'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0d49223f-1364-4766-96f8-9b481b81622a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_tokens_title+n_tokens_content+n_non_stop_unique_tokens+num_hrefs+num_self_hrefs+num_imgs+num_videos+average_token_length+num_keywords+kw_min_min+kw_max_min+kw_min_max+kw_max_max+kw_avg_max+kw_min_avg+kw_max_avg+self_reference_avg_sharess+LDA_01+global_subjectivity+global_rate_positive_words+avg_positive_polarity+min_positive_polarity+max_positive_polarity+min_negative_polarity+max_negative_polarity+title_subjectivity+title_sentiment_polarity+abs_title_subjectivity+abs_title_sentiment_polarity+data_channel+weekday_is\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>shares</td>      <th>  R-squared:         </th>  <td>   0.033</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.032</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   28.55</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 11 Aug 2023</td> <th>  Prob (F-statistic):</th>  <td>5.28e-163</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:34:50</td>     <th>  Log-Likelihood:    </th> <td>-2.6438e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 25756</td>      <th>  AIC:               </th>  <td>5.288e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 25724</td>      <th>  BIC:               </th>  <td>5.291e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    31</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                <td></td>                  <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                    <td> 1025.4081</td> <td>  982.844</td> <td>    1.043</td> <td> 0.297</td> <td> -901.021</td> <td> 2951.837</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n_tokens_title</th>               <td>   48.6666</td> <td>   21.340</td> <td>    2.281</td> <td> 0.023</td> <td>    6.839</td> <td>   90.494</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n_tokens_content</th>             <td>   -0.5247</td> <td>    0.131</td> <td>   -4.008</td> <td> 0.000</td> <td>   -0.781</td> <td>   -0.268</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n_non_stop_unique_tokens</th>     <td>    7.8771</td> <td>   10.716</td> <td>    0.735</td> <td> 0.462</td> <td>  -13.127</td> <td>   28.881</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_hrefs</th>                    <td>   42.6315</td> <td>    4.775</td> <td>    8.928</td> <td> 0.000</td> <td>   33.272</td> <td>   51.991</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_self_hrefs</th>               <td>  -55.0199</td> <td>   12.467</td> <td>   -4.413</td> <td> 0.000</td> <td>  -79.456</td> <td>  -30.583</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_imgs</th>                     <td>   36.5750</td> <td>    6.032</td> <td>    6.063</td> <td> 0.000</td> <td>   24.752</td> <td>   48.398</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_videos</th>                   <td>   17.1559</td> <td>   11.117</td> <td>    1.543</td> <td> 0.123</td> <td>   -4.633</td> <td>   38.945</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>average_token_length</th>         <td> -837.9062</td> <td>  164.887</td> <td>   -5.082</td> <td> 0.000</td> <td>-1161.095</td> <td> -514.718</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_keywords</th>                 <td>  149.7973</td> <td>   26.724</td> <td>    5.605</td> <td> 0.000</td> <td>   97.416</td> <td>  202.179</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_min_min</th>                   <td>    2.6478</td> <td>    1.197</td> <td>    2.212</td> <td> 0.027</td> <td>    0.301</td> <td>    4.994</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_max_min</th>                   <td>   -0.0361</td> <td>    0.015</td> <td>   -2.414</td> <td> 0.016</td> <td>   -0.065</td> <td>   -0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_min_max</th>                   <td>   -0.0025</td> <td>    0.001</td> <td>   -2.863</td> <td> 0.004</td> <td>   -0.004</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_max_max</th>                   <td>   -0.0005</td> <td>    0.000</td> <td>   -1.084</td> <td> 0.278</td> <td>   -0.001</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_avg_max</th>                   <td>    0.0031</td> <td>    0.001</td> <td>    5.855</td> <td> 0.000</td> <td>    0.002</td> <td>    0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_min_avg</th>                   <td>    0.1548</td> <td>    0.044</td> <td>    3.511</td> <td> 0.000</td> <td>    0.068</td> <td>    0.241</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_max_avg</th>                   <td>    0.0832</td> <td>    0.010</td> <td>    8.295</td> <td> 0.000</td> <td>    0.064</td> <td>    0.103</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>self_reference_avg_sharess</th>   <td>    0.0128</td> <td>    0.002</td> <td>    7.140</td> <td> 0.000</td> <td>    0.009</td> <td>    0.016</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LDA_01</th>                       <td>  -49.2722</td> <td>  213.758</td> <td>   -0.231</td> <td> 0.818</td> <td> -468.249</td> <td>  369.705</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>global_subjectivity</th>          <td> 4434.7686</td> <td>  594.895</td> <td>    7.455</td> <td> 0.000</td> <td> 3268.741</td> <td> 5600.796</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>global_rate_positive_words</th>   <td>-1926.7871</td> <td> 3236.531</td> <td>   -0.595</td> <td> 0.552</td> <td>-8270.569</td> <td> 4416.995</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>avg_positive_polarity</th>        <td> -371.0593</td> <td>  807.856</td> <td>   -0.459</td> <td> 0.646</td> <td>-1954.503</td> <td> 1212.385</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>min_positive_polarity</th>        <td>  -65.2270</td> <td>  811.878</td> <td>   -0.080</td> <td> 0.936</td> <td>-1656.554</td> <td> 1526.100</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>max_positive_polarity</th>        <td>   75.6728</td> <td>  313.062</td> <td>    0.242</td> <td> 0.809</td> <td> -537.946</td> <td>  689.291</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>min_negative_polarity</th>        <td> -460.6741</td> <td>  183.356</td> <td>   -2.512</td> <td> 0.012</td> <td> -820.062</td> <td> -101.286</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>max_negative_polarity</th>        <td> -875.9709</td> <td>  486.162</td> <td>   -1.802</td> <td> 0.072</td> <td>-1828.875</td> <td>   76.933</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>title_subjectivity</th>           <td>  213.7489</td> <td>  205.986</td> <td>    1.038</td> <td> 0.299</td> <td> -189.996</td> <td>  617.494</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>title_sentiment_polarity</th>     <td>  654.2681</td> <td>  185.005</td> <td>    3.536</td> <td> 0.000</td> <td>  291.647</td> <td> 1016.889</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>abs_title_subjectivity</th>       <td>  861.5931</td> <td>  272.856</td> <td>    3.158</td> <td> 0.002</td> <td>  326.780</td> <td> 1396.407</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>abs_title_sentiment_polarity</th> <td>  545.2446</td> <td>  296.749</td> <td>    1.837</td> <td> 0.066</td> <td>  -36.399</td> <td> 1126.889</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>data_channel</th>                 <td>  105.0672</td> <td>   25.286</td> <td>    4.155</td> <td> 0.000</td> <td>   55.505</td> <td>  154.630</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weekday_is</th>                   <td>    5.7123</td> <td>   24.711</td> <td>    0.231</td> <td> 0.817</td> <td>  -42.724</td> <td>   54.148</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>38064.341</td> <th>  Durbin-Watson:     </th>   <td>   1.995</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>15755288.077</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 9.104</td>   <th>  Prob(JB):          </th>   <td>    0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>122.790</td>  <th>  Cond. No.          </th>   <td>6.21e+07</td>  \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 6.21e+07. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                  &      shares      & \\textbf{  R-squared:         } &      0.033    \\\\\n",
       "\\textbf{Model:}                          &       OLS        & \\textbf{  Adj. R-squared:    } &      0.032    \\\\\n",
       "\\textbf{Method:}                         &  Least Squares   & \\textbf{  F-statistic:       } &      28.55    \\\\\n",
       "\\textbf{Date:}                           & Fri, 11 Aug 2023 & \\textbf{  Prob (F-statistic):} &  5.28e-163    \\\\\n",
       "\\textbf{Time:}                           &     11:34:50     & \\textbf{  Log-Likelihood:    } & -2.6438e+05   \\\\\n",
       "\\textbf{No. Observations:}               &       25756      & \\textbf{  AIC:               } &  5.288e+05    \\\\\n",
       "\\textbf{Df Residuals:}                   &       25724      & \\textbf{  BIC:               } &  5.291e+05    \\\\\n",
       "\\textbf{Df Model:}                       &          31      & \\textbf{                     } &               \\\\\n",
       "\\textbf{Covariance Type:}                &    nonrobust     & \\textbf{                     } &               \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                         & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                       &    1025.4081  &      982.844     &     1.043  &         0.297        &     -901.021    &     2951.837     \\\\\n",
       "\\textbf{n\\_tokens\\_title}                &      48.6666  &       21.340     &     2.281  &         0.023        &        6.839    &       90.494     \\\\\n",
       "\\textbf{n\\_tokens\\_content}              &      -0.5247  &        0.131     &    -4.008  &         0.000        &       -0.781    &       -0.268     \\\\\n",
       "\\textbf{n\\_non\\_stop\\_unique\\_tokens}    &       7.8771  &       10.716     &     0.735  &         0.462        &      -13.127    &       28.881     \\\\\n",
       "\\textbf{num\\_hrefs}                      &      42.6315  &        4.775     &     8.928  &         0.000        &       33.272    &       51.991     \\\\\n",
       "\\textbf{num\\_self\\_hrefs}                &     -55.0199  &       12.467     &    -4.413  &         0.000        &      -79.456    &      -30.583     \\\\\n",
       "\\textbf{num\\_imgs}                       &      36.5750  &        6.032     &     6.063  &         0.000        &       24.752    &       48.398     \\\\\n",
       "\\textbf{num\\_videos}                     &      17.1559  &       11.117     &     1.543  &         0.123        &       -4.633    &       38.945     \\\\\n",
       "\\textbf{average\\_token\\_length}          &    -837.9062  &      164.887     &    -5.082  &         0.000        &    -1161.095    &     -514.718     \\\\\n",
       "\\textbf{num\\_keywords}                   &     149.7973  &       26.724     &     5.605  &         0.000        &       97.416    &      202.179     \\\\\n",
       "\\textbf{kw\\_min\\_min}                    &       2.6478  &        1.197     &     2.212  &         0.027        &        0.301    &        4.994     \\\\\n",
       "\\textbf{kw\\_max\\_min}                    &      -0.0361  &        0.015     &    -2.414  &         0.016        &       -0.065    &       -0.007     \\\\\n",
       "\\textbf{kw\\_min\\_max}                    &      -0.0025  &        0.001     &    -2.863  &         0.004        &       -0.004    &       -0.001     \\\\\n",
       "\\textbf{kw\\_max\\_max}                    &      -0.0005  &        0.000     &    -1.084  &         0.278        &       -0.001    &        0.000     \\\\\n",
       "\\textbf{kw\\_avg\\_max}                    &       0.0031  &        0.001     &     5.855  &         0.000        &        0.002    &        0.004     \\\\\n",
       "\\textbf{kw\\_min\\_avg}                    &       0.1548  &        0.044     &     3.511  &         0.000        &        0.068    &        0.241     \\\\\n",
       "\\textbf{kw\\_max\\_avg}                    &       0.0832  &        0.010     &     8.295  &         0.000        &        0.064    &        0.103     \\\\\n",
       "\\textbf{self\\_reference\\_avg\\_sharess}   &       0.0128  &        0.002     &     7.140  &         0.000        &        0.009    &        0.016     \\\\\n",
       "\\textbf{LDA\\_01}                         &     -49.2722  &      213.758     &    -0.231  &         0.818        &     -468.249    &      369.705     \\\\\n",
       "\\textbf{global\\_subjectivity}            &    4434.7686  &      594.895     &     7.455  &         0.000        &     3268.741    &     5600.796     \\\\\n",
       "\\textbf{global\\_rate\\_positive\\_words}   &   -1926.7871  &     3236.531     &    -0.595  &         0.552        &    -8270.569    &     4416.995     \\\\\n",
       "\\textbf{avg\\_positive\\_polarity}         &    -371.0593  &      807.856     &    -0.459  &         0.646        &    -1954.503    &     1212.385     \\\\\n",
       "\\textbf{min\\_positive\\_polarity}         &     -65.2270  &      811.878     &    -0.080  &         0.936        &    -1656.554    &     1526.100     \\\\\n",
       "\\textbf{max\\_positive\\_polarity}         &      75.6728  &      313.062     &     0.242  &         0.809        &     -537.946    &      689.291     \\\\\n",
       "\\textbf{min\\_negative\\_polarity}         &    -460.6741  &      183.356     &    -2.512  &         0.012        &     -820.062    &     -101.286     \\\\\n",
       "\\textbf{max\\_negative\\_polarity}         &    -875.9709  &      486.162     &    -1.802  &         0.072        &    -1828.875    &       76.933     \\\\\n",
       "\\textbf{title\\_subjectivity}             &     213.7489  &      205.986     &     1.038  &         0.299        &     -189.996    &      617.494     \\\\\n",
       "\\textbf{title\\_sentiment\\_polarity}      &     654.2681  &      185.005     &     3.536  &         0.000        &      291.647    &     1016.889     \\\\\n",
       "\\textbf{abs\\_title\\_subjectivity}        &     861.5931  &      272.856     &     3.158  &         0.002        &      326.780    &     1396.407     \\\\\n",
       "\\textbf{abs\\_title\\_sentiment\\_polarity} &     545.2446  &      296.749     &     1.837  &         0.066        &      -36.399    &     1126.889     \\\\\n",
       "\\textbf{data\\_channel}                   &     105.0672  &       25.286     &     4.155  &         0.000        &       55.505    &      154.630     \\\\\n",
       "\\textbf{weekday\\_is}                     &       5.7123  &       24.711     &     0.231  &         0.817        &      -42.724    &       54.148     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 38064.341 & \\textbf{  Durbin-Watson:     } &      1.995    \\\\\n",
       "\\textbf{Prob(Omnibus):} &    0.000  & \\textbf{  Jarque-Bera (JB):  } & 15755288.077  \\\\\n",
       "\\textbf{Skew:}          &    9.104  & \\textbf{  Prob(JB):          } &       0.00    \\\\\n",
       "\\textbf{Kurtosis:}      &  122.790  & \\textbf{  Cond. No.          } &   6.21e+07    \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 6.21e+07. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 shares   R-squared:                       0.033\n",
       "Model:                            OLS   Adj. R-squared:                  0.032\n",
       "Method:                 Least Squares   F-statistic:                     28.55\n",
       "Date:                Fri, 11 Aug 2023   Prob (F-statistic):          5.28e-163\n",
       "Time:                        11:34:50   Log-Likelihood:            -2.6438e+05\n",
       "No. Observations:               25756   AIC:                         5.288e+05\n",
       "Df Residuals:                   25724   BIC:                         5.291e+05\n",
       "Df Model:                          31                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "================================================================================================\n",
       "                                   coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------------------\n",
       "Intercept                     1025.4081    982.844      1.043      0.297    -901.021    2951.837\n",
       "n_tokens_title                  48.6666     21.340      2.281      0.023       6.839      90.494\n",
       "n_tokens_content                -0.5247      0.131     -4.008      0.000      -0.781      -0.268\n",
       "n_non_stop_unique_tokens         7.8771     10.716      0.735      0.462     -13.127      28.881\n",
       "num_hrefs                       42.6315      4.775      8.928      0.000      33.272      51.991\n",
       "num_self_hrefs                 -55.0199     12.467     -4.413      0.000     -79.456     -30.583\n",
       "num_imgs                        36.5750      6.032      6.063      0.000      24.752      48.398\n",
       "num_videos                      17.1559     11.117      1.543      0.123      -4.633      38.945\n",
       "average_token_length          -837.9062    164.887     -5.082      0.000   -1161.095    -514.718\n",
       "num_keywords                   149.7973     26.724      5.605      0.000      97.416     202.179\n",
       "kw_min_min                       2.6478      1.197      2.212      0.027       0.301       4.994\n",
       "kw_max_min                      -0.0361      0.015     -2.414      0.016      -0.065      -0.007\n",
       "kw_min_max                      -0.0025      0.001     -2.863      0.004      -0.004      -0.001\n",
       "kw_max_max                      -0.0005      0.000     -1.084      0.278      -0.001       0.000\n",
       "kw_avg_max                       0.0031      0.001      5.855      0.000       0.002       0.004\n",
       "kw_min_avg                       0.1548      0.044      3.511      0.000       0.068       0.241\n",
       "kw_max_avg                       0.0832      0.010      8.295      0.000       0.064       0.103\n",
       "self_reference_avg_sharess       0.0128      0.002      7.140      0.000       0.009       0.016\n",
       "LDA_01                         -49.2722    213.758     -0.231      0.818    -468.249     369.705\n",
       "global_subjectivity           4434.7686    594.895      7.455      0.000    3268.741    5600.796\n",
       "global_rate_positive_words   -1926.7871   3236.531     -0.595      0.552   -8270.569    4416.995\n",
       "avg_positive_polarity         -371.0593    807.856     -0.459      0.646   -1954.503    1212.385\n",
       "min_positive_polarity          -65.2270    811.878     -0.080      0.936   -1656.554    1526.100\n",
       "max_positive_polarity           75.6728    313.062      0.242      0.809    -537.946     689.291\n",
       "min_negative_polarity         -460.6741    183.356     -2.512      0.012    -820.062    -101.286\n",
       "max_negative_polarity         -875.9709    486.162     -1.802      0.072   -1828.875      76.933\n",
       "title_subjectivity             213.7489    205.986      1.038      0.299    -189.996     617.494\n",
       "title_sentiment_polarity       654.2681    185.005      3.536      0.000     291.647    1016.889\n",
       "abs_title_subjectivity         861.5931    272.856      3.158      0.002     326.780    1396.407\n",
       "abs_title_sentiment_polarity   545.2446    296.749      1.837      0.066     -36.399    1126.889\n",
       "data_channel                   105.0672     25.286      4.155      0.000      55.505     154.630\n",
       "weekday_is                       5.7123     24.711      0.231      0.817     -42.724      54.148\n",
       "==============================================================================\n",
       "Omnibus:                    38064.341   Durbin-Watson:                   1.995\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):         15755288.077\n",
       "Skew:                           9.104   Prob(JB):                         0.00\n",
       "Kurtosis:                     122.790   Cond. No.                     6.21e+07\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 6.21e+07. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_multicol_model = smf.ols(formula='shares ~ {}'.format(list_to_string(predictors_wo_multicol)), data=train_df).fit()\n",
    "no_multicol_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "56e9d57e-3aba-4256-ba38-a9f8f5d63b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a QQ plot for the residuals\n",
    "sm.qqplot(no_multicol_model.resid, line='s');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fa4ad3-05ee-411c-b45c-fde027091e16",
   "metadata": {},
   "source": [
    "#### Summary of Results & Analysis: Multicollinearity Detection\n",
    "The removal of predictors exhibiting multicollinearity tendencies led to a marginal reduction in the model's R-squared value (0.033) compared to the R-squared value achieved by the multilinear model that incorporated all predictors (0.038). It's important to highlight that the QQ-plot of residuals still indicates a non-linear pattern, suggesting potential deviations from the assumption of normality in the residuals. I'll attempt to address this in the next section using polynomial regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5149fc8-411e-4d2d-b881-0fd26a17e224",
   "metadata": {},
   "source": [
    "### Polynomial Regression\n",
    "In this section, I delve into the application of polynomial regression as a tool to address the observed nonlinearity in the residuals. The objective is to uncover potential curvilinear relationships between predictor variables and the response variable, shedding light on intricate dynamics influencing the number of shares an article receives.\n",
    "\n",
    "The previous analysis indicated that a linear regression model might not fully capture the complexity of the relationships within the data. As a remedy, polynomial regression was explored. By incorporating polynomial terms, I aim to enhance the model's ability to capture nuanced patterns and better explain the variance in the response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a78d23e2-ceae-4397-b5dd-90b7ce1a73a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_regression_summary(data, predictors, target, degree):\n",
    "    # Construct the polynomial formula\n",
    "    poly_formula = f'{target} ~ ' + ' + '.join([f'{col} + np.power({col}, {degree})' for col in predictors])\n",
    "    \n",
    "    # Fit the polynomial regression model\n",
    "    poly_est = smf.ols(poly_formula, data).fit()\n",
    "    \n",
    "    # Return the fitted model\n",
    "    return poly_est"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da07e8a4-d7a1-4f37-8765-70026c63cde8",
   "metadata": {},
   "source": [
    "#### Initial Polynomial Regression with All Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3be26bb0-9ffe-4692-ad81-566287d114e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>shares</td>      <th>  R-squared:         </th>  <td>   0.043</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.042</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   47.71</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 11 Aug 2023</td> <th>  Prob (F-statistic):</th>  <td>1.94e-221</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:34:51</td>     <th>  Log-Likelihood:    </th> <td>-2.6425e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 25756</td>      <th>  AIC:               </th>  <td>5.286e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 25731</td>      <th>  BIC:               </th>  <td>5.288e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    24</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                     <td></td>                       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                              <td> 1.211e+04</td> <td> 6088.454</td> <td>    1.989</td> <td> 0.047</td> <td>  174.676</td> <td>  2.4e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>global_subjectivity</th>                    <td>-1644.6432</td> <td> 3028.143</td> <td>   -0.543</td> <td> 0.587</td> <td>-7579.973</td> <td> 4290.687</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(global_subjectivity, 2)</th>       <td> 5962.2558</td> <td> 3230.700</td> <td>    1.845</td> <td> 0.065</td> <td> -370.098</td> <td> 1.23e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_max_avg</th>                             <td>    0.1012</td> <td>    0.012</td> <td>    8.178</td> <td> 0.000</td> <td>    0.077</td> <td>    0.125</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_max_avg, 2)</th>                <td>-3.975e-07</td> <td> 9.66e-08</td> <td>   -4.113</td> <td> 0.000</td> <td>-5.87e-07</td> <td>-2.08e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_hrefs</th>                              <td>   33.2455</td> <td>    6.969</td> <td>    4.771</td> <td> 0.000</td> <td>   19.587</td> <td>   46.904</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_hrefs, 2)</th>                 <td>    0.0254</td> <td>    0.069</td> <td>    0.370</td> <td> 0.711</td> <td>   -0.109</td> <td>    0.160</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>self_reference_min_shares</th>              <td>    0.0709</td> <td>    0.005</td> <td>   14.113</td> <td> 0.000</td> <td>    0.061</td> <td>    0.081</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(self_reference_min_shares, 2)</th> <td>-1.047e-07</td> <td> 8.79e-09</td> <td>  -11.911</td> <td> 0.000</td> <td>-1.22e-07</td> <td>-8.75e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>data_channel</th>                           <td> -228.5836</td> <td>  138.153</td> <td>   -1.655</td> <td> 0.098</td> <td> -499.370</td> <td>   42.203</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(data_channel, 2)</th>              <td>   41.3909</td> <td>   16.429</td> <td>    2.519</td> <td> 0.012</td> <td>    9.189</td> <td>   73.592</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_imgs</th>                               <td>   33.9166</td> <td>   10.537</td> <td>    3.219</td> <td> 0.001</td> <td>   13.264</td> <td>   54.569</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_imgs, 2)</th>                  <td>   -0.3318</td> <td>    0.202</td> <td>   -1.645</td> <td> 0.100</td> <td>   -0.727</td> <td>    0.064</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>average_token_length</th>                   <td>-4079.4339</td> <td> 2504.552</td> <td>   -1.629</td> <td> 0.103</td> <td>-8988.497</td> <td>  829.629</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(average_token_length, 2)</th>      <td>  357.5119</td> <td>  261.833</td> <td>    1.365</td> <td> 0.172</td> <td> -155.695</td> <td>  870.719</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_self_hrefs</th>                         <td>  -25.6834</td> <td>   18.693</td> <td>   -1.374</td> <td> 0.169</td> <td>  -62.322</td> <td>   10.955</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_self_hrefs, 2)</th>            <td>   -0.1466</td> <td>    0.472</td> <td>   -0.311</td> <td> 0.756</td> <td>   -1.071</td> <td>    0.778</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>title_sentiment_polarity</th>               <td>  403.7566</td> <td>  176.308</td> <td>    2.290</td> <td> 0.022</td> <td>   58.182</td> <td>  749.331</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(title_sentiment_polarity, 2)</th>  <td> 1021.8826</td> <td>  274.691</td> <td>    3.720</td> <td> 0.000</td> <td>  483.472</td> <td> 1560.293</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_min_avg</th>                             <td>   -1.0617</td> <td>    0.124</td> <td>   -8.582</td> <td> 0.000</td> <td>   -1.304</td> <td>   -0.819</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_min_avg, 2)</th>                <td>    0.0005</td> <td> 4.36e-05</td> <td>   10.421</td> <td> 0.000</td> <td>    0.000</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_keywords</th>                           <td>  362.2013</td> <td>  162.149</td> <td>    2.234</td> <td> 0.026</td> <td>   44.380</td> <td>  680.022</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_keywords, 2)</th>              <td>  -19.6971</td> <td>   11.197</td> <td>   -1.759</td> <td> 0.079</td> <td>  -41.645</td> <td>    2.251</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n_tokens_title</th>                         <td> -247.0095</td> <td>  149.314</td> <td>   -1.654</td> <td> 0.098</td> <td> -539.674</td> <td>   45.655</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(n_tokens_title, 2)</th>            <td>   14.5613</td> <td>    7.004</td> <td>    2.079</td> <td> 0.038</td> <td>    0.833</td> <td>   28.290</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>38019.676</td> <th>  Durbin-Watson:     </th>   <td>   1.995</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>15661143.516</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 9.084</td>   <th>  Prob(JB):          </th>   <td>    0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>122.429</td>  <th>  Cond. No.          </th>   <td>1.68e+12</td>  \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.68e+12. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                            &      shares      & \\textbf{  R-squared:         } &      0.043    \\\\\n",
       "\\textbf{Model:}                                    &       OLS        & \\textbf{  Adj. R-squared:    } &      0.042    \\\\\n",
       "\\textbf{Method:}                                   &  Least Squares   & \\textbf{  F-statistic:       } &      47.71    \\\\\n",
       "\\textbf{Date:}                                     & Fri, 11 Aug 2023 & \\textbf{  Prob (F-statistic):} &  1.94e-221    \\\\\n",
       "\\textbf{Time:}                                     &     11:34:51     & \\textbf{  Log-Likelihood:    } & -2.6425e+05   \\\\\n",
       "\\textbf{No. Observations:}                         &       25756      & \\textbf{  AIC:               } &  5.286e+05    \\\\\n",
       "\\textbf{Df Residuals:}                             &       25731      & \\textbf{  BIC:               } &  5.288e+05    \\\\\n",
       "\\textbf{Df Model:}                                 &          24      & \\textbf{                     } &               \\\\\n",
       "\\textbf{Covariance Type:}                          &    nonrobust     & \\textbf{                     } &               \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                                   & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                                 &    1.211e+04  &     6088.454     &     1.989  &         0.047        &      174.676    &      2.4e+04     \\\\\n",
       "\\textbf{global\\_subjectivity}                      &   -1644.6432  &     3028.143     &    -0.543  &         0.587        &    -7579.973    &     4290.687     \\\\\n",
       "\\textbf{np.power(global\\_subjectivity, 2)}         &    5962.2558  &     3230.700     &     1.845  &         0.065        &     -370.098    &     1.23e+04     \\\\\n",
       "\\textbf{kw\\_max\\_avg}                              &       0.1012  &        0.012     &     8.178  &         0.000        &        0.077    &        0.125     \\\\\n",
       "\\textbf{np.power(kw\\_max\\_avg, 2)}                 &   -3.975e-07  &     9.66e-08     &    -4.113  &         0.000        &    -5.87e-07    &    -2.08e-07     \\\\\n",
       "\\textbf{num\\_hrefs}                                &      33.2455  &        6.969     &     4.771  &         0.000        &       19.587    &       46.904     \\\\\n",
       "\\textbf{np.power(num\\_hrefs, 2)}                   &       0.0254  &        0.069     &     0.370  &         0.711        &       -0.109    &        0.160     \\\\\n",
       "\\textbf{self\\_reference\\_min\\_shares}              &       0.0709  &        0.005     &    14.113  &         0.000        &        0.061    &        0.081     \\\\\n",
       "\\textbf{np.power(self\\_reference\\_min\\_shares, 2)} &   -1.047e-07  &     8.79e-09     &   -11.911  &         0.000        &    -1.22e-07    &    -8.75e-08     \\\\\n",
       "\\textbf{data\\_channel}                             &    -228.5836  &      138.153     &    -1.655  &         0.098        &     -499.370    &       42.203     \\\\\n",
       "\\textbf{np.power(data\\_channel, 2)}                &      41.3909  &       16.429     &     2.519  &         0.012        &        9.189    &       73.592     \\\\\n",
       "\\textbf{num\\_imgs}                                 &      33.9166  &       10.537     &     3.219  &         0.001        &       13.264    &       54.569     \\\\\n",
       "\\textbf{np.power(num\\_imgs, 2)}                    &      -0.3318  &        0.202     &    -1.645  &         0.100        &       -0.727    &        0.064     \\\\\n",
       "\\textbf{average\\_token\\_length}                    &   -4079.4339  &     2504.552     &    -1.629  &         0.103        &    -8988.497    &      829.629     \\\\\n",
       "\\textbf{np.power(average\\_token\\_length, 2)}       &     357.5119  &      261.833     &     1.365  &         0.172        &     -155.695    &      870.719     \\\\\n",
       "\\textbf{num\\_self\\_hrefs}                          &     -25.6834  &       18.693     &    -1.374  &         0.169        &      -62.322    &       10.955     \\\\\n",
       "\\textbf{np.power(num\\_self\\_hrefs, 2)}             &      -0.1466  &        0.472     &    -0.311  &         0.756        &       -1.071    &        0.778     \\\\\n",
       "\\textbf{title\\_sentiment\\_polarity}                &     403.7566  &      176.308     &     2.290  &         0.022        &       58.182    &      749.331     \\\\\n",
       "\\textbf{np.power(title\\_sentiment\\_polarity, 2)}   &    1021.8826  &      274.691     &     3.720  &         0.000        &      483.472    &     1560.293     \\\\\n",
       "\\textbf{kw\\_min\\_avg}                              &      -1.0617  &        0.124     &    -8.582  &         0.000        &       -1.304    &       -0.819     \\\\\n",
       "\\textbf{np.power(kw\\_min\\_avg, 2)}                 &       0.0005  &     4.36e-05     &    10.421  &         0.000        &        0.000    &        0.001     \\\\\n",
       "\\textbf{num\\_keywords}                             &     362.2013  &      162.149     &     2.234  &         0.026        &       44.380    &      680.022     \\\\\n",
       "\\textbf{np.power(num\\_keywords, 2)}                &     -19.6971  &       11.197     &    -1.759  &         0.079        &      -41.645    &        2.251     \\\\\n",
       "\\textbf{n\\_tokens\\_title}                          &    -247.0095  &      149.314     &    -1.654  &         0.098        &     -539.674    &       45.655     \\\\\n",
       "\\textbf{np.power(n\\_tokens\\_title, 2)}             &      14.5613  &        7.004     &     2.079  &         0.038        &        0.833    &       28.290     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 38019.676 & \\textbf{  Durbin-Watson:     } &      1.995    \\\\\n",
       "\\textbf{Prob(Omnibus):} &    0.000  & \\textbf{  Jarque-Bera (JB):  } & 15661143.516  \\\\\n",
       "\\textbf{Skew:}          &    9.084  & \\textbf{  Prob(JB):          } &       0.00    \\\\\n",
       "\\textbf{Kurtosis:}      &  122.429  & \\textbf{  Cond. No.          } &   1.68e+12    \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 1.68e+12. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 shares   R-squared:                       0.043\n",
       "Model:                            OLS   Adj. R-squared:                  0.042\n",
       "Method:                 Least Squares   F-statistic:                     47.71\n",
       "Date:                Fri, 11 Aug 2023   Prob (F-statistic):          1.94e-221\n",
       "Time:                        11:34:51   Log-Likelihood:            -2.6425e+05\n",
       "No. Observations:               25756   AIC:                         5.286e+05\n",
       "Df Residuals:                   25731   BIC:                         5.288e+05\n",
       "Df Model:                          24                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==========================================================================================================\n",
       "                                             coef    std err          t      P>|t|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------------------------------\n",
       "Intercept                               1.211e+04   6088.454      1.989      0.047     174.676     2.4e+04\n",
       "global_subjectivity                    -1644.6432   3028.143     -0.543      0.587   -7579.973    4290.687\n",
       "np.power(global_subjectivity, 2)        5962.2558   3230.700      1.845      0.065    -370.098    1.23e+04\n",
       "kw_max_avg                                 0.1012      0.012      8.178      0.000       0.077       0.125\n",
       "np.power(kw_max_avg, 2)                -3.975e-07   9.66e-08     -4.113      0.000   -5.87e-07   -2.08e-07\n",
       "num_hrefs                                 33.2455      6.969      4.771      0.000      19.587      46.904\n",
       "np.power(num_hrefs, 2)                     0.0254      0.069      0.370      0.711      -0.109       0.160\n",
       "self_reference_min_shares                  0.0709      0.005     14.113      0.000       0.061       0.081\n",
       "np.power(self_reference_min_shares, 2) -1.047e-07   8.79e-09    -11.911      0.000   -1.22e-07   -8.75e-08\n",
       "data_channel                            -228.5836    138.153     -1.655      0.098    -499.370      42.203\n",
       "np.power(data_channel, 2)                 41.3909     16.429      2.519      0.012       9.189      73.592\n",
       "num_imgs                                  33.9166     10.537      3.219      0.001      13.264      54.569\n",
       "np.power(num_imgs, 2)                     -0.3318      0.202     -1.645      0.100      -0.727       0.064\n",
       "average_token_length                   -4079.4339   2504.552     -1.629      0.103   -8988.497     829.629\n",
       "np.power(average_token_length, 2)        357.5119    261.833      1.365      0.172    -155.695     870.719\n",
       "num_self_hrefs                           -25.6834     18.693     -1.374      0.169     -62.322      10.955\n",
       "np.power(num_self_hrefs, 2)               -0.1466      0.472     -0.311      0.756      -1.071       0.778\n",
       "title_sentiment_polarity                 403.7566    176.308      2.290      0.022      58.182     749.331\n",
       "np.power(title_sentiment_polarity, 2)   1021.8826    274.691      3.720      0.000     483.472    1560.293\n",
       "kw_min_avg                                -1.0617      0.124     -8.582      0.000      -1.304      -0.819\n",
       "np.power(kw_min_avg, 2)                    0.0005   4.36e-05     10.421      0.000       0.000       0.001\n",
       "num_keywords                             362.2013    162.149      2.234      0.026      44.380     680.022\n",
       "np.power(num_keywords, 2)                -19.6971     11.197     -1.759      0.079     -41.645       2.251\n",
       "n_tokens_title                          -247.0095    149.314     -1.654      0.098    -539.674      45.655\n",
       "np.power(n_tokens_title, 2)               14.5613      7.004      2.079      0.038       0.833      28.290\n",
       "==============================================================================\n",
       "Omnibus:                    38019.676   Durbin-Watson:                   1.995\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):         15661143.516\n",
       "Skew:                           9.084   Prob(JB):                         0.00\n",
       "Kurtosis:                     122.429   Cond. No.                     1.68e+12\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.68e+12. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "degree = 2\n",
    "polynomial_regression_summary(train_df, predictors, 'shares', degree).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "60e8cd64-6f26-499a-afa9-cb389f7cab55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>shares</td>      <th>  R-squared:         </th>  <td>   0.033</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.033</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   80.46</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 11 Aug 2023</td> <th>  Prob (F-statistic):</th>  <td>1.51e-179</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:34:51</td>     <th>  Log-Likelihood:    </th> <td>-2.6438e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 25756</td>      <th>  AIC:               </th>  <td>5.288e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 25744</td>      <th>  BIC:               </th>  <td>5.289e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    11</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                     <td></td>                       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                              <td>    0.0269</td> <td>    0.009</td> <td>    3.056</td> <td> 0.002</td> <td>    0.010</td> <td>    0.044</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>global_subjectivity</th>                    <td>    0.0251</td> <td>    0.004</td> <td>    6.099</td> <td> 0.000</td> <td>    0.017</td> <td>    0.033</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(global_subjectivity, 3)</th>       <td>    0.0281</td> <td>    0.002</td> <td>   11.948</td> <td> 0.000</td> <td>    0.023</td> <td>    0.033</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_max_avg</th>                             <td>    0.0931</td> <td>    0.009</td> <td>   10.372</td> <td> 0.000</td> <td>    0.076</td> <td>    0.111</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_max_avg, 3)</th>                <td> -9.02e-13</td> <td> 2.91e-13</td> <td>   -3.096</td> <td> 0.002</td> <td>-1.47e-12</td> <td>-3.31e-13</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_hrefs</th>                              <td>   40.9514</td> <td>    4.003</td> <td>   10.229</td> <td> 0.000</td> <td>   33.105</td> <td>   48.798</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_hrefs, 3)</th>                 <td>   -0.0002</td> <td>    0.000</td> <td>   -1.020</td> <td> 0.308</td> <td>   -0.001</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>self_reference_min_shares</th>              <td>    0.0687</td> <td>    0.004</td> <td>   15.597</td> <td> 0.000</td> <td>    0.060</td> <td>    0.077</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(self_reference_min_shares, 3)</th> <td>-1.536e-13</td> <td> 1.17e-14</td> <td>  -13.119</td> <td> 0.000</td> <td>-1.77e-13</td> <td>-1.31e-13</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>data_channel</th>                           <td>   -0.1572</td> <td>    0.041</td> <td>   -3.808</td> <td> 0.000</td> <td>   -0.238</td> <td>   -0.076</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(data_channel, 3)</th>              <td>    2.8797</td> <td>    0.385</td> <td>    7.475</td> <td> 0.000</td> <td>    2.125</td> <td>    3.635</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_imgs</th>                               <td>   13.5360</td> <td>    1.331</td> <td>   10.172</td> <td> 0.000</td> <td>   10.928</td> <td>   16.144</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_imgs, 3)</th>                  <td>   -0.0008</td> <td>    0.001</td> <td>   -0.582</td> <td> 0.561</td> <td>   -0.004</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>average_token_length</th>                   <td>    0.1698</td> <td>    0.044</td> <td>    3.835</td> <td> 0.000</td> <td>    0.083</td> <td>    0.257</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(average_token_length, 3)</th>      <td>    5.2846</td> <td>    1.154</td> <td>    4.579</td> <td> 0.000</td> <td>    3.023</td> <td>    7.546</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_self_hrefs</th>                         <td>    6.2244</td> <td>    0.604</td> <td>   10.299</td> <td> 0.000</td> <td>    5.040</td> <td>    7.409</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_self_hrefs, 3)</th>            <td>   -0.0074</td> <td>    0.004</td> <td>   -1.890</td> <td> 0.059</td> <td>   -0.015</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>title_sentiment_polarity</th>               <td>    0.0509</td> <td>    0.005</td> <td>   10.622</td> <td> 0.000</td> <td>    0.041</td> <td>    0.060</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(title_sentiment_polarity, 3)</th>  <td>    0.0319</td> <td>    0.003</td> <td>   10.261</td> <td> 0.000</td> <td>    0.026</td> <td>    0.038</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_min_avg</th>                             <td>   -0.4656</td> <td>    0.077</td> <td>   -6.057</td> <td> 0.000</td> <td>   -0.616</td> <td>   -0.315</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_min_avg, 3)</th>                <td> 9.526e-08</td> <td> 8.75e-09</td> <td>   10.888</td> <td> 0.000</td> <td> 7.81e-08</td> <td> 1.12e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_keywords</th>                           <td>    0.2234</td> <td>    0.037</td> <td>    6.105</td> <td> 0.000</td> <td>    0.152</td> <td>    0.295</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_keywords, 3)</th>              <td>    0.8298</td> <td>    0.142</td> <td>    5.849</td> <td> 0.000</td> <td>    0.552</td> <td>    1.108</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n_tokens_title</th>                         <td>    0.1303</td> <td>    0.064</td> <td>    2.032</td> <td> 0.042</td> <td>    0.005</td> <td>    0.256</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(n_tokens_title, 3)</th>            <td>    0.3621</td> <td>    0.054</td> <td>    6.725</td> <td> 0.000</td> <td>    0.257</td> <td>    0.468</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>37893.971</td> <th>  Durbin-Watson:     </th>   <td>   1.997</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>15384038.916</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 9.029</td>   <th>  Prob(JB):          </th>   <td>    0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>121.360</td>  <th>  Cond. No.          </th>   <td>7.63e+17</td>  \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 7.63e+17. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                            &      shares      & \\textbf{  R-squared:         } &      0.033    \\\\\n",
       "\\textbf{Model:}                                    &       OLS        & \\textbf{  Adj. R-squared:    } &      0.033    \\\\\n",
       "\\textbf{Method:}                                   &  Least Squares   & \\textbf{  F-statistic:       } &      80.46    \\\\\n",
       "\\textbf{Date:}                                     & Fri, 11 Aug 2023 & \\textbf{  Prob (F-statistic):} &  1.51e-179    \\\\\n",
       "\\textbf{Time:}                                     &     11:34:51     & \\textbf{  Log-Likelihood:    } & -2.6438e+05   \\\\\n",
       "\\textbf{No. Observations:}                         &       25756      & \\textbf{  AIC:               } &  5.288e+05    \\\\\n",
       "\\textbf{Df Residuals:}                             &       25744      & \\textbf{  BIC:               } &  5.289e+05    \\\\\n",
       "\\textbf{Df Model:}                                 &          11      & \\textbf{                     } &               \\\\\n",
       "\\textbf{Covariance Type:}                          &    nonrobust     & \\textbf{                     } &               \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                                   & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                                 &       0.0269  &        0.009     &     3.056  &         0.002        &        0.010    &        0.044     \\\\\n",
       "\\textbf{global\\_subjectivity}                      &       0.0251  &        0.004     &     6.099  &         0.000        &        0.017    &        0.033     \\\\\n",
       "\\textbf{np.power(global\\_subjectivity, 3)}         &       0.0281  &        0.002     &    11.948  &         0.000        &        0.023    &        0.033     \\\\\n",
       "\\textbf{kw\\_max\\_avg}                              &       0.0931  &        0.009     &    10.372  &         0.000        &        0.076    &        0.111     \\\\\n",
       "\\textbf{np.power(kw\\_max\\_avg, 3)}                 &    -9.02e-13  &     2.91e-13     &    -3.096  &         0.002        &    -1.47e-12    &    -3.31e-13     \\\\\n",
       "\\textbf{num\\_hrefs}                                &      40.9514  &        4.003     &    10.229  &         0.000        &       33.105    &       48.798     \\\\\n",
       "\\textbf{np.power(num\\_hrefs, 3)}                   &      -0.0002  &        0.000     &    -1.020  &         0.308        &       -0.001    &        0.000     \\\\\n",
       "\\textbf{self\\_reference\\_min\\_shares}              &       0.0687  &        0.004     &    15.597  &         0.000        &        0.060    &        0.077     \\\\\n",
       "\\textbf{np.power(self\\_reference\\_min\\_shares, 3)} &   -1.536e-13  &     1.17e-14     &   -13.119  &         0.000        &    -1.77e-13    &    -1.31e-13     \\\\\n",
       "\\textbf{data\\_channel}                             &      -0.1572  &        0.041     &    -3.808  &         0.000        &       -0.238    &       -0.076     \\\\\n",
       "\\textbf{np.power(data\\_channel, 3)}                &       2.8797  &        0.385     &     7.475  &         0.000        &        2.125    &        3.635     \\\\\n",
       "\\textbf{num\\_imgs}                                 &      13.5360  &        1.331     &    10.172  &         0.000        &       10.928    &       16.144     \\\\\n",
       "\\textbf{np.power(num\\_imgs, 3)}                    &      -0.0008  &        0.001     &    -0.582  &         0.561        &       -0.004    &        0.002     \\\\\n",
       "\\textbf{average\\_token\\_length}                    &       0.1698  &        0.044     &     3.835  &         0.000        &        0.083    &        0.257     \\\\\n",
       "\\textbf{np.power(average\\_token\\_length, 3)}       &       5.2846  &        1.154     &     4.579  &         0.000        &        3.023    &        7.546     \\\\\n",
       "\\textbf{num\\_self\\_hrefs}                          &       6.2244  &        0.604     &    10.299  &         0.000        &        5.040    &        7.409     \\\\\n",
       "\\textbf{np.power(num\\_self\\_hrefs, 3)}             &      -0.0074  &        0.004     &    -1.890  &         0.059        &       -0.015    &        0.000     \\\\\n",
       "\\textbf{title\\_sentiment\\_polarity}                &       0.0509  &        0.005     &    10.622  &         0.000        &        0.041    &        0.060     \\\\\n",
       "\\textbf{np.power(title\\_sentiment\\_polarity, 3)}   &       0.0319  &        0.003     &    10.261  &         0.000        &        0.026    &        0.038     \\\\\n",
       "\\textbf{kw\\_min\\_avg}                              &      -0.4656  &        0.077     &    -6.057  &         0.000        &       -0.616    &       -0.315     \\\\\n",
       "\\textbf{np.power(kw\\_min\\_avg, 3)}                 &    9.526e-08  &     8.75e-09     &    10.888  &         0.000        &     7.81e-08    &     1.12e-07     \\\\\n",
       "\\textbf{num\\_keywords}                             &       0.2234  &        0.037     &     6.105  &         0.000        &        0.152    &        0.295     \\\\\n",
       "\\textbf{np.power(num\\_keywords, 3)}                &       0.8298  &        0.142     &     5.849  &         0.000        &        0.552    &        1.108     \\\\\n",
       "\\textbf{n\\_tokens\\_title}                          &       0.1303  &        0.064     &     2.032  &         0.042        &        0.005    &        0.256     \\\\\n",
       "\\textbf{np.power(n\\_tokens\\_title, 3)}             &       0.3621  &        0.054     &     6.725  &         0.000        &        0.257    &        0.468     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 37893.971 & \\textbf{  Durbin-Watson:     } &      1.997    \\\\\n",
       "\\textbf{Prob(Omnibus):} &    0.000  & \\textbf{  Jarque-Bera (JB):  } & 15384038.916  \\\\\n",
       "\\textbf{Skew:}          &    9.029  & \\textbf{  Prob(JB):          } &       0.00    \\\\\n",
       "\\textbf{Kurtosis:}      &  121.360  & \\textbf{  Cond. No.          } &   7.63e+17    \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 7.63e+17. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 shares   R-squared:                       0.033\n",
       "Model:                            OLS   Adj. R-squared:                  0.033\n",
       "Method:                 Least Squares   F-statistic:                     80.46\n",
       "Date:                Fri, 11 Aug 2023   Prob (F-statistic):          1.51e-179\n",
       "Time:                        11:34:51   Log-Likelihood:            -2.6438e+05\n",
       "No. Observations:               25756   AIC:                         5.288e+05\n",
       "Df Residuals:                   25744   BIC:                         5.289e+05\n",
       "Df Model:                          11                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==========================================================================================================\n",
       "                                             coef    std err          t      P>|t|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------------------------------\n",
       "Intercept                                  0.0269      0.009      3.056      0.002       0.010       0.044\n",
       "global_subjectivity                        0.0251      0.004      6.099      0.000       0.017       0.033\n",
       "np.power(global_subjectivity, 3)           0.0281      0.002     11.948      0.000       0.023       0.033\n",
       "kw_max_avg                                 0.0931      0.009     10.372      0.000       0.076       0.111\n",
       "np.power(kw_max_avg, 3)                 -9.02e-13   2.91e-13     -3.096      0.002   -1.47e-12   -3.31e-13\n",
       "num_hrefs                                 40.9514      4.003     10.229      0.000      33.105      48.798\n",
       "np.power(num_hrefs, 3)                    -0.0002      0.000     -1.020      0.308      -0.001       0.000\n",
       "self_reference_min_shares                  0.0687      0.004     15.597      0.000       0.060       0.077\n",
       "np.power(self_reference_min_shares, 3) -1.536e-13   1.17e-14    -13.119      0.000   -1.77e-13   -1.31e-13\n",
       "data_channel                              -0.1572      0.041     -3.808      0.000      -0.238      -0.076\n",
       "np.power(data_channel, 3)                  2.8797      0.385      7.475      0.000       2.125       3.635\n",
       "num_imgs                                  13.5360      1.331     10.172      0.000      10.928      16.144\n",
       "np.power(num_imgs, 3)                     -0.0008      0.001     -0.582      0.561      -0.004       0.002\n",
       "average_token_length                       0.1698      0.044      3.835      0.000       0.083       0.257\n",
       "np.power(average_token_length, 3)          5.2846      1.154      4.579      0.000       3.023       7.546\n",
       "num_self_hrefs                             6.2244      0.604     10.299      0.000       5.040       7.409\n",
       "np.power(num_self_hrefs, 3)               -0.0074      0.004     -1.890      0.059      -0.015       0.000\n",
       "title_sentiment_polarity                   0.0509      0.005     10.622      0.000       0.041       0.060\n",
       "np.power(title_sentiment_polarity, 3)      0.0319      0.003     10.261      0.000       0.026       0.038\n",
       "kw_min_avg                                -0.4656      0.077     -6.057      0.000      -0.616      -0.315\n",
       "np.power(kw_min_avg, 3)                 9.526e-08   8.75e-09     10.888      0.000    7.81e-08    1.12e-07\n",
       "num_keywords                               0.2234      0.037      6.105      0.000       0.152       0.295\n",
       "np.power(num_keywords, 3)                  0.8298      0.142      5.849      0.000       0.552       1.108\n",
       "n_tokens_title                             0.1303      0.064      2.032      0.042       0.005       0.256\n",
       "np.power(n_tokens_title, 3)                0.3621      0.054      6.725      0.000       0.257       0.468\n",
       "==============================================================================\n",
       "Omnibus:                    37893.971   Durbin-Watson:                   1.997\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):         15384038.916\n",
       "Skew:                           9.029   Prob(JB):                         0.00\n",
       "Kurtosis:                     121.360   Cond. No.                     7.63e+17\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 7.63e+17. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "degree = 3\n",
    "polynomial_regression_summary(train_df, predictors, 'shares', degree).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "be33dc72-84fc-4883-908f-3a442a4c16e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>shares</td>      <th>  R-squared:         </th>  <td>  -0.120</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>  -0.120</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>  -917.0</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 11 Aug 2023</td> <th>  Prob (F-statistic):</th>   <td>  1.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:34:51</td>     <th>  Log-Likelihood:    </th> <td>-2.6627e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 25756</td>      <th>  AIC:               </th>  <td>5.325e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 25752</td>      <th>  BIC:               </th>  <td>5.326e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                     <td></td>                       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                              <td> 5.388e-16</td> <td> 2.12e-16</td> <td>    2.536</td> <td> 0.011</td> <td> 1.22e-16</td> <td> 9.55e-16</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>global_subjectivity</th>                    <td> 9.976e-12</td> <td> 3.93e-12</td> <td>    2.536</td> <td> 0.011</td> <td> 2.27e-12</td> <td> 1.77e-11</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(global_subjectivity, 4)</th>       <td> 2.619e-13</td> <td> 1.03e-13</td> <td>    2.536</td> <td> 0.011</td> <td> 5.95e-14</td> <td> 4.64e-13</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_max_avg</th>                             <td> 3.461e-12</td> <td> 1.36e-12</td> <td>    2.536</td> <td> 0.011</td> <td> 7.86e-13</td> <td> 6.14e-12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_max_avg, 4)</th>                <td> 2.383e-18</td> <td> 9.38e-19</td> <td>    2.542</td> <td> 0.011</td> <td> 5.45e-19</td> <td> 4.22e-18</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_hrefs</th>                              <td> 1.249e-13</td> <td> 4.93e-14</td> <td>    2.536</td> <td> 0.011</td> <td> 2.84e-14</td> <td> 2.21e-13</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_hrefs, 4)</th>                 <td> 2.133e-06</td> <td> 8.41e-07</td> <td>    2.536</td> <td> 0.011</td> <td> 4.85e-07</td> <td> 3.78e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>self_reference_min_shares</th>              <td> 9.073e-13</td> <td> 3.58e-13</td> <td>    2.536</td> <td> 0.011</td> <td> 2.06e-13</td> <td> 1.61e-12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(self_reference_min_shares, 4)</th> <td> 1.312e-20</td> <td> 9.71e-21</td> <td>    1.351</td> <td> 0.177</td> <td>-5.91e-21</td> <td> 3.22e-20</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>data_channel</th>                           <td> 1.705e-15</td> <td> 6.72e-16</td> <td>    2.536</td> <td> 0.011</td> <td> 3.87e-16</td> <td> 3.02e-15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(data_channel, 4)</th>              <td> 2.603e-13</td> <td> 1.03e-13</td> <td>    2.536</td> <td> 0.011</td> <td> 5.91e-14</td> <td> 4.61e-13</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_imgs</th>                               <td> 6.794e-15</td> <td> 2.68e-15</td> <td>    2.536</td> <td> 0.011</td> <td> 1.54e-15</td> <td>  1.2e-14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_imgs, 4)</th>                  <td> 8.904e-10</td> <td> 3.51e-10</td> <td>    2.536</td> <td> 0.011</td> <td> 2.02e-10</td> <td> 1.58e-09</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>average_token_length</th>                   <td> 3.079e-15</td> <td> 1.21e-15</td> <td>    2.536</td> <td> 0.011</td> <td> 6.99e-16</td> <td> 5.46e-15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(average_token_length, 4)</th>      <td>  6.17e-13</td> <td> 2.43e-13</td> <td>    2.536</td> <td> 0.011</td> <td>  1.4e-13</td> <td> 1.09e-12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_self_hrefs</th>                         <td> 6.678e-15</td> <td> 2.63e-15</td> <td>    2.536</td> <td> 0.011</td> <td> 1.52e-15</td> <td> 1.18e-14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_self_hrefs, 4)</th>            <td> 1.253e-09</td> <td> 4.94e-10</td> <td>    2.536</td> <td> 0.011</td> <td> 2.85e-10</td> <td> 2.22e-09</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>title_sentiment_polarity</th>               <td>  2.79e-17</td> <td>  1.1e-17</td> <td>    2.536</td> <td> 0.011</td> <td> 6.34e-18</td> <td> 4.95e-17</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(title_sentiment_polarity, 4)</th>  <td> 1.604e-17</td> <td> 6.33e-18</td> <td>    2.536</td> <td> 0.011</td> <td> 3.64e-18</td> <td> 2.84e-17</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_min_avg</th>                             <td> 5.989e-13</td> <td> 2.36e-13</td> <td>    2.536</td> <td> 0.011</td> <td> 1.36e-13</td> <td> 1.06e-12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_min_avg, 4)</th>                <td> 5.604e-11</td> <td> 1.28e-12</td> <td>   43.913</td> <td> 0.000</td> <td> 5.35e-11</td> <td> 5.85e-11</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_keywords</th>                           <td> 4.588e-15</td> <td> 1.81e-15</td> <td>    2.536</td> <td> 0.011</td> <td> 1.04e-15</td> <td> 8.13e-15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_keywords, 4)</th>              <td> 2.913e-12</td> <td> 1.15e-12</td> <td>    2.536</td> <td> 0.011</td> <td> 6.62e-13</td> <td> 5.16e-12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n_tokens_title</th>                         <td> 5.508e-15</td> <td> 2.17e-15</td> <td>    2.536</td> <td> 0.011</td> <td> 1.25e-15</td> <td> 9.76e-15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(n_tokens_title, 4)</th>            <td> 6.751e-12</td> <td> 2.66e-12</td> <td>    2.536</td> <td> 0.011</td> <td> 1.53e-12</td> <td>  1.2e-11</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>37102.804</td> <th>  Durbin-Watson:     </th>   <td>   1.813</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>13908757.618</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 8.681</td>   <th>  Prob(JB):          </th>   <td>    0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>115.512</td>  <th>  Cond. No.          </th>   <td>5.11e+23</td>  \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 5.11e+23. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                            &      shares      & \\textbf{  R-squared:         } &     -0.120    \\\\\n",
       "\\textbf{Model:}                                    &       OLS        & \\textbf{  Adj. R-squared:    } &     -0.120    \\\\\n",
       "\\textbf{Method:}                                   &  Least Squares   & \\textbf{  F-statistic:       } &     -917.0    \\\\\n",
       "\\textbf{Date:}                                     & Fri, 11 Aug 2023 & \\textbf{  Prob (F-statistic):} &      1.00     \\\\\n",
       "\\textbf{Time:}                                     &     11:34:51     & \\textbf{  Log-Likelihood:    } & -2.6627e+05   \\\\\n",
       "\\textbf{No. Observations:}                         &       25756      & \\textbf{  AIC:               } &  5.325e+05    \\\\\n",
       "\\textbf{Df Residuals:}                             &       25752      & \\textbf{  BIC:               } &  5.326e+05    \\\\\n",
       "\\textbf{Df Model:}                                 &           3      & \\textbf{                     } &               \\\\\n",
       "\\textbf{Covariance Type:}                          &    nonrobust     & \\textbf{                     } &               \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                                   & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                                 &    5.388e-16  &     2.12e-16     &     2.536  &         0.011        &     1.22e-16    &     9.55e-16     \\\\\n",
       "\\textbf{global\\_subjectivity}                      &    9.976e-12  &     3.93e-12     &     2.536  &         0.011        &     2.27e-12    &     1.77e-11     \\\\\n",
       "\\textbf{np.power(global\\_subjectivity, 4)}         &    2.619e-13  &     1.03e-13     &     2.536  &         0.011        &     5.95e-14    &     4.64e-13     \\\\\n",
       "\\textbf{kw\\_max\\_avg}                              &    3.461e-12  &     1.36e-12     &     2.536  &         0.011        &     7.86e-13    &     6.14e-12     \\\\\n",
       "\\textbf{np.power(kw\\_max\\_avg, 4)}                 &    2.383e-18  &     9.38e-19     &     2.542  &         0.011        &     5.45e-19    &     4.22e-18     \\\\\n",
       "\\textbf{num\\_hrefs}                                &    1.249e-13  &     4.93e-14     &     2.536  &         0.011        &     2.84e-14    &     2.21e-13     \\\\\n",
       "\\textbf{np.power(num\\_hrefs, 4)}                   &    2.133e-06  &     8.41e-07     &     2.536  &         0.011        &     4.85e-07    &     3.78e-06     \\\\\n",
       "\\textbf{self\\_reference\\_min\\_shares}              &    9.073e-13  &     3.58e-13     &     2.536  &         0.011        &     2.06e-13    &     1.61e-12     \\\\\n",
       "\\textbf{np.power(self\\_reference\\_min\\_shares, 4)} &    1.312e-20  &     9.71e-21     &     1.351  &         0.177        &    -5.91e-21    &     3.22e-20     \\\\\n",
       "\\textbf{data\\_channel}                             &    1.705e-15  &     6.72e-16     &     2.536  &         0.011        &     3.87e-16    &     3.02e-15     \\\\\n",
       "\\textbf{np.power(data\\_channel, 4)}                &    2.603e-13  &     1.03e-13     &     2.536  &         0.011        &     5.91e-14    &     4.61e-13     \\\\\n",
       "\\textbf{num\\_imgs}                                 &    6.794e-15  &     2.68e-15     &     2.536  &         0.011        &     1.54e-15    &      1.2e-14     \\\\\n",
       "\\textbf{np.power(num\\_imgs, 4)}                    &    8.904e-10  &     3.51e-10     &     2.536  &         0.011        &     2.02e-10    &     1.58e-09     \\\\\n",
       "\\textbf{average\\_token\\_length}                    &    3.079e-15  &     1.21e-15     &     2.536  &         0.011        &     6.99e-16    &     5.46e-15     \\\\\n",
       "\\textbf{np.power(average\\_token\\_length, 4)}       &     6.17e-13  &     2.43e-13     &     2.536  &         0.011        &      1.4e-13    &     1.09e-12     \\\\\n",
       "\\textbf{num\\_self\\_hrefs}                          &    6.678e-15  &     2.63e-15     &     2.536  &         0.011        &     1.52e-15    &     1.18e-14     \\\\\n",
       "\\textbf{np.power(num\\_self\\_hrefs, 4)}             &    1.253e-09  &     4.94e-10     &     2.536  &         0.011        &     2.85e-10    &     2.22e-09     \\\\\n",
       "\\textbf{title\\_sentiment\\_polarity}                &     2.79e-17  &      1.1e-17     &     2.536  &         0.011        &     6.34e-18    &     4.95e-17     \\\\\n",
       "\\textbf{np.power(title\\_sentiment\\_polarity, 4)}   &    1.604e-17  &     6.33e-18     &     2.536  &         0.011        &     3.64e-18    &     2.84e-17     \\\\\n",
       "\\textbf{kw\\_min\\_avg}                              &    5.989e-13  &     2.36e-13     &     2.536  &         0.011        &     1.36e-13    &     1.06e-12     \\\\\n",
       "\\textbf{np.power(kw\\_min\\_avg, 4)}                 &    5.604e-11  &     1.28e-12     &    43.913  &         0.000        &     5.35e-11    &     5.85e-11     \\\\\n",
       "\\textbf{num\\_keywords}                             &    4.588e-15  &     1.81e-15     &     2.536  &         0.011        &     1.04e-15    &     8.13e-15     \\\\\n",
       "\\textbf{np.power(num\\_keywords, 4)}                &    2.913e-12  &     1.15e-12     &     2.536  &         0.011        &     6.62e-13    &     5.16e-12     \\\\\n",
       "\\textbf{n\\_tokens\\_title}                          &    5.508e-15  &     2.17e-15     &     2.536  &         0.011        &     1.25e-15    &     9.76e-15     \\\\\n",
       "\\textbf{np.power(n\\_tokens\\_title, 4)}             &    6.751e-12  &     2.66e-12     &     2.536  &         0.011        &     1.53e-12    &      1.2e-11     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 37102.804 & \\textbf{  Durbin-Watson:     } &      1.813    \\\\\n",
       "\\textbf{Prob(Omnibus):} &    0.000  & \\textbf{  Jarque-Bera (JB):  } & 13908757.618  \\\\\n",
       "\\textbf{Skew:}          &    8.681  & \\textbf{  Prob(JB):          } &       0.00    \\\\\n",
       "\\textbf{Kurtosis:}      &  115.512  & \\textbf{  Cond. No.          } &   5.11e+23    \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 5.11e+23. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 shares   R-squared:                      -0.120\n",
       "Model:                            OLS   Adj. R-squared:                 -0.120\n",
       "Method:                 Least Squares   F-statistic:                    -917.0\n",
       "Date:                Fri, 11 Aug 2023   Prob (F-statistic):               1.00\n",
       "Time:                        11:34:51   Log-Likelihood:            -2.6627e+05\n",
       "No. Observations:               25756   AIC:                         5.325e+05\n",
       "Df Residuals:                   25752   BIC:                         5.326e+05\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==========================================================================================================\n",
       "                                             coef    std err          t      P>|t|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------------------------------\n",
       "Intercept                               5.388e-16   2.12e-16      2.536      0.011    1.22e-16    9.55e-16\n",
       "global_subjectivity                     9.976e-12   3.93e-12      2.536      0.011    2.27e-12    1.77e-11\n",
       "np.power(global_subjectivity, 4)        2.619e-13   1.03e-13      2.536      0.011    5.95e-14    4.64e-13\n",
       "kw_max_avg                              3.461e-12   1.36e-12      2.536      0.011    7.86e-13    6.14e-12\n",
       "np.power(kw_max_avg, 4)                 2.383e-18   9.38e-19      2.542      0.011    5.45e-19    4.22e-18\n",
       "num_hrefs                               1.249e-13   4.93e-14      2.536      0.011    2.84e-14    2.21e-13\n",
       "np.power(num_hrefs, 4)                  2.133e-06   8.41e-07      2.536      0.011    4.85e-07    3.78e-06\n",
       "self_reference_min_shares               9.073e-13   3.58e-13      2.536      0.011    2.06e-13    1.61e-12\n",
       "np.power(self_reference_min_shares, 4)  1.312e-20   9.71e-21      1.351      0.177   -5.91e-21    3.22e-20\n",
       "data_channel                            1.705e-15   6.72e-16      2.536      0.011    3.87e-16    3.02e-15\n",
       "np.power(data_channel, 4)               2.603e-13   1.03e-13      2.536      0.011    5.91e-14    4.61e-13\n",
       "num_imgs                                6.794e-15   2.68e-15      2.536      0.011    1.54e-15     1.2e-14\n",
       "np.power(num_imgs, 4)                   8.904e-10   3.51e-10      2.536      0.011    2.02e-10    1.58e-09\n",
       "average_token_length                    3.079e-15   1.21e-15      2.536      0.011    6.99e-16    5.46e-15\n",
       "np.power(average_token_length, 4)        6.17e-13   2.43e-13      2.536      0.011     1.4e-13    1.09e-12\n",
       "num_self_hrefs                          6.678e-15   2.63e-15      2.536      0.011    1.52e-15    1.18e-14\n",
       "np.power(num_self_hrefs, 4)             1.253e-09   4.94e-10      2.536      0.011    2.85e-10    2.22e-09\n",
       "title_sentiment_polarity                 2.79e-17    1.1e-17      2.536      0.011    6.34e-18    4.95e-17\n",
       "np.power(title_sentiment_polarity, 4)   1.604e-17   6.33e-18      2.536      0.011    3.64e-18    2.84e-17\n",
       "kw_min_avg                              5.989e-13   2.36e-13      2.536      0.011    1.36e-13    1.06e-12\n",
       "np.power(kw_min_avg, 4)                 5.604e-11   1.28e-12     43.913      0.000    5.35e-11    5.85e-11\n",
       "num_keywords                            4.588e-15   1.81e-15      2.536      0.011    1.04e-15    8.13e-15\n",
       "np.power(num_keywords, 4)               2.913e-12   1.15e-12      2.536      0.011    6.62e-13    5.16e-12\n",
       "n_tokens_title                          5.508e-15   2.17e-15      2.536      0.011    1.25e-15    9.76e-15\n",
       "np.power(n_tokens_title, 4)             6.751e-12   2.66e-12      2.536      0.011    1.53e-12     1.2e-11\n",
       "==============================================================================\n",
       "Omnibus:                    37102.804   Durbin-Watson:                   1.813\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):         13908757.618\n",
       "Skew:                           8.681   Prob(JB):                         0.00\n",
       "Kurtosis:                     115.512   Cond. No.                     5.11e+23\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 5.11e+23. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "degree = 4\n",
    "polynomial_regression_summary(train_df, predictors, 'shares', degree).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "44f3ba48-a7aa-4a4d-a224-3f56ad41ed5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>shares</td>      <th>  R-squared:         </th>  <td>  -0.131</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>  -0.131</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>  -1494.</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 11 Aug 2023</td> <th>  Prob (F-statistic):</th>   <td>  1.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:34:51</td>     <th>  Log-Likelihood:    </th> <td>-2.6640e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 25756</td>      <th>  AIC:               </th>  <td>5.328e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 25753</td>      <th>  BIC:               </th>  <td>5.328e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                     <td></td>                       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                              <td> 5.663e-32</td> <td> 1.39e-33</td> <td>   40.697</td> <td> 0.000</td> <td> 5.39e-32</td> <td> 5.94e-32</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>global_subjectivity</th>                    <td>-1.002e-21</td> <td> 2.46e-23</td> <td>  -40.697</td> <td> 0.000</td> <td>-1.05e-21</td> <td>-9.54e-22</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(global_subjectivity, 5)</th>       <td>  -1.8e-23</td> <td> 4.42e-25</td> <td>  -40.697</td> <td> 0.000</td> <td>-1.89e-23</td> <td>-1.71e-23</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_max_avg</th>                             <td> 4.256e-28</td> <td> 1.05e-29</td> <td>   40.697</td> <td> 0.000</td> <td> 4.05e-28</td> <td> 4.46e-28</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_max_avg, 5)</th>                <td> 7.509e-24</td> <td> 3.17e-24</td> <td>    2.367</td> <td> 0.018</td> <td> 1.29e-24</td> <td> 1.37e-23</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_hrefs</th>                              <td> 7.504e-31</td> <td> 1.84e-32</td> <td>   40.697</td> <td> 0.000</td> <td> 7.14e-31</td> <td> 7.87e-31</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_hrefs, 5)</th>                 <td> 1.354e-23</td> <td> 3.33e-25</td> <td>   40.697</td> <td> 0.000</td> <td> 1.29e-23</td> <td> 1.42e-23</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>self_reference_min_shares</th>              <td> 3.377e-28</td> <td>  8.3e-30</td> <td>   40.697</td> <td> 0.000</td> <td> 3.21e-28</td> <td> 3.54e-28</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(self_reference_min_shares, 5)</th> <td> 2.042e-26</td> <td> 1.47e-26</td> <td>    1.390</td> <td> 0.165</td> <td>-8.37e-27</td> <td> 4.92e-26</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>data_channel</th>                           <td> 2.597e-31</td> <td> 6.38e-33</td> <td>   40.697</td> <td> 0.000</td> <td> 2.47e-31</td> <td> 2.72e-31</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(data_channel, 5)</th>              <td> 3.887e-28</td> <td> 9.55e-30</td> <td>   40.697</td> <td> 0.000</td> <td>  3.7e-28</td> <td> 4.07e-28</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_imgs</th>                               <td> 3.651e-31</td> <td> 8.97e-33</td> <td>   40.697</td> <td> 0.000</td> <td> 3.48e-31</td> <td> 3.83e-31</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_imgs, 5)</th>                  <td> 1.139e-24</td> <td>  2.8e-26</td> <td>   40.697</td> <td> 0.000</td> <td> 1.08e-24</td> <td> 1.19e-24</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>average_token_length</th>                   <td> 2.653e-31</td> <td> 6.52e-33</td> <td>   40.697</td> <td> 0.000</td> <td> 2.52e-31</td> <td> 2.78e-31</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(average_token_length, 5)</th>      <td> 1.335e-28</td> <td> 3.28e-30</td> <td>   40.697</td> <td> 0.000</td> <td> 1.27e-28</td> <td>  1.4e-28</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_self_hrefs</th>                         <td> 1.862e-31</td> <td> 4.58e-33</td> <td>   40.697</td> <td> 0.000</td> <td> 1.77e-31</td> <td> 1.95e-31</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_self_hrefs, 5)</th>            <td> 1.495e-25</td> <td> 3.67e-27</td> <td>   40.697</td> <td> 0.000</td> <td> 1.42e-25</td> <td> 1.57e-25</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>title_sentiment_polarity</th>               <td> 5.238e-33</td> <td> 1.29e-34</td> <td>   40.697</td> <td> 0.000</td> <td> 4.99e-33</td> <td> 5.49e-33</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(title_sentiment_polarity, 5)</th>  <td> 1.251e-33</td> <td> 3.07e-35</td> <td>   40.697</td> <td> 0.000</td> <td> 1.19e-33</td> <td> 1.31e-33</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_min_avg</th>                             <td> 1.676e-28</td> <td> 4.12e-30</td> <td>   40.697</td> <td> 0.000</td> <td>  1.6e-28</td> <td> 1.76e-28</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_min_avg, 5)</th>                <td> 1.631e-14</td> <td> 4.01e-16</td> <td>   40.697</td> <td> 0.000</td> <td> 1.55e-14</td> <td> 1.71e-14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_keywords</th>                           <td> 3.577e-31</td> <td> 8.79e-33</td> <td>   40.697</td> <td> 0.000</td> <td>  3.4e-31</td> <td> 3.75e-31</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_keywords, 5)</th>              <td> 1.269e-27</td> <td> 3.12e-29</td> <td>   40.697</td> <td> 0.000</td> <td> 1.21e-27</td> <td> 1.33e-27</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n_tokens_title</th>                         <td> 5.853e-31</td> <td> 1.44e-32</td> <td>   40.697</td> <td> 0.000</td> <td> 5.57e-31</td> <td> 6.14e-31</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(n_tokens_title, 5)</th>            <td> 9.586e-27</td> <td> 2.36e-28</td> <td>   40.697</td> <td> 0.000</td> <td> 9.12e-27</td> <td>    1e-26</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>37211.834</td> <th>  Durbin-Watson:     </th>   <td>   1.787</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>14087045.465</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 8.729</td>   <th>  Prob(JB):          </th>   <td>    0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>116.234</td>  <th>  Cond. No.          </th>   <td>1.75e+29</td>  \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.75e+29. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                            &      shares      & \\textbf{  R-squared:         } &     -0.131    \\\\\n",
       "\\textbf{Model:}                                    &       OLS        & \\textbf{  Adj. R-squared:    } &     -0.131    \\\\\n",
       "\\textbf{Method:}                                   &  Least Squares   & \\textbf{  F-statistic:       } &     -1494.    \\\\\n",
       "\\textbf{Date:}                                     & Fri, 11 Aug 2023 & \\textbf{  Prob (F-statistic):} &      1.00     \\\\\n",
       "\\textbf{Time:}                                     &     11:34:51     & \\textbf{  Log-Likelihood:    } & -2.6640e+05   \\\\\n",
       "\\textbf{No. Observations:}                         &       25756      & \\textbf{  AIC:               } &  5.328e+05    \\\\\n",
       "\\textbf{Df Residuals:}                             &       25753      & \\textbf{  BIC:               } &  5.328e+05    \\\\\n",
       "\\textbf{Df Model:}                                 &           2      & \\textbf{                     } &               \\\\\n",
       "\\textbf{Covariance Type:}                          &    nonrobust     & \\textbf{                     } &               \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                                   & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                                 &    5.663e-32  &     1.39e-33     &    40.697  &         0.000        &     5.39e-32    &     5.94e-32     \\\\\n",
       "\\textbf{global\\_subjectivity}                      &   -1.002e-21  &     2.46e-23     &   -40.697  &         0.000        &    -1.05e-21    &    -9.54e-22     \\\\\n",
       "\\textbf{np.power(global\\_subjectivity, 5)}         &     -1.8e-23  &     4.42e-25     &   -40.697  &         0.000        &    -1.89e-23    &    -1.71e-23     \\\\\n",
       "\\textbf{kw\\_max\\_avg}                              &    4.256e-28  &     1.05e-29     &    40.697  &         0.000        &     4.05e-28    &     4.46e-28     \\\\\n",
       "\\textbf{np.power(kw\\_max\\_avg, 5)}                 &    7.509e-24  &     3.17e-24     &     2.367  &         0.018        &     1.29e-24    &     1.37e-23     \\\\\n",
       "\\textbf{num\\_hrefs}                                &    7.504e-31  &     1.84e-32     &    40.697  &         0.000        &     7.14e-31    &     7.87e-31     \\\\\n",
       "\\textbf{np.power(num\\_hrefs, 5)}                   &    1.354e-23  &     3.33e-25     &    40.697  &         0.000        &     1.29e-23    &     1.42e-23     \\\\\n",
       "\\textbf{self\\_reference\\_min\\_shares}              &    3.377e-28  &      8.3e-30     &    40.697  &         0.000        &     3.21e-28    &     3.54e-28     \\\\\n",
       "\\textbf{np.power(self\\_reference\\_min\\_shares, 5)} &    2.042e-26  &     1.47e-26     &     1.390  &         0.165        &    -8.37e-27    &     4.92e-26     \\\\\n",
       "\\textbf{data\\_channel}                             &    2.597e-31  &     6.38e-33     &    40.697  &         0.000        &     2.47e-31    &     2.72e-31     \\\\\n",
       "\\textbf{np.power(data\\_channel, 5)}                &    3.887e-28  &     9.55e-30     &    40.697  &         0.000        &      3.7e-28    &     4.07e-28     \\\\\n",
       "\\textbf{num\\_imgs}                                 &    3.651e-31  &     8.97e-33     &    40.697  &         0.000        &     3.48e-31    &     3.83e-31     \\\\\n",
       "\\textbf{np.power(num\\_imgs, 5)}                    &    1.139e-24  &      2.8e-26     &    40.697  &         0.000        &     1.08e-24    &     1.19e-24     \\\\\n",
       "\\textbf{average\\_token\\_length}                    &    2.653e-31  &     6.52e-33     &    40.697  &         0.000        &     2.52e-31    &     2.78e-31     \\\\\n",
       "\\textbf{np.power(average\\_token\\_length, 5)}       &    1.335e-28  &     3.28e-30     &    40.697  &         0.000        &     1.27e-28    &      1.4e-28     \\\\\n",
       "\\textbf{num\\_self\\_hrefs}                          &    1.862e-31  &     4.58e-33     &    40.697  &         0.000        &     1.77e-31    &     1.95e-31     \\\\\n",
       "\\textbf{np.power(num\\_self\\_hrefs, 5)}             &    1.495e-25  &     3.67e-27     &    40.697  &         0.000        &     1.42e-25    &     1.57e-25     \\\\\n",
       "\\textbf{title\\_sentiment\\_polarity}                &    5.238e-33  &     1.29e-34     &    40.697  &         0.000        &     4.99e-33    &     5.49e-33     \\\\\n",
       "\\textbf{np.power(title\\_sentiment\\_polarity, 5)}   &    1.251e-33  &     3.07e-35     &    40.697  &         0.000        &     1.19e-33    &     1.31e-33     \\\\\n",
       "\\textbf{kw\\_min\\_avg}                              &    1.676e-28  &     4.12e-30     &    40.697  &         0.000        &      1.6e-28    &     1.76e-28     \\\\\n",
       "\\textbf{np.power(kw\\_min\\_avg, 5)}                 &    1.631e-14  &     4.01e-16     &    40.697  &         0.000        &     1.55e-14    &     1.71e-14     \\\\\n",
       "\\textbf{num\\_keywords}                             &    3.577e-31  &     8.79e-33     &    40.697  &         0.000        &      3.4e-31    &     3.75e-31     \\\\\n",
       "\\textbf{np.power(num\\_keywords, 5)}                &    1.269e-27  &     3.12e-29     &    40.697  &         0.000        &     1.21e-27    &     1.33e-27     \\\\\n",
       "\\textbf{n\\_tokens\\_title}                          &    5.853e-31  &     1.44e-32     &    40.697  &         0.000        &     5.57e-31    &     6.14e-31     \\\\\n",
       "\\textbf{np.power(n\\_tokens\\_title, 5)}             &    9.586e-27  &     2.36e-28     &    40.697  &         0.000        &     9.12e-27    &        1e-26     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 37211.834 & \\textbf{  Durbin-Watson:     } &      1.787    \\\\\n",
       "\\textbf{Prob(Omnibus):} &    0.000  & \\textbf{  Jarque-Bera (JB):  } & 14087045.465  \\\\\n",
       "\\textbf{Skew:}          &    8.729  & \\textbf{  Prob(JB):          } &       0.00    \\\\\n",
       "\\textbf{Kurtosis:}      &  116.234  & \\textbf{  Cond. No.          } &   1.75e+29    \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 1.75e+29. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 shares   R-squared:                      -0.131\n",
       "Model:                            OLS   Adj. R-squared:                 -0.131\n",
       "Method:                 Least Squares   F-statistic:                    -1494.\n",
       "Date:                Fri, 11 Aug 2023   Prob (F-statistic):               1.00\n",
       "Time:                        11:34:51   Log-Likelihood:            -2.6640e+05\n",
       "No. Observations:               25756   AIC:                         5.328e+05\n",
       "Df Residuals:                   25753   BIC:                         5.328e+05\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==========================================================================================================\n",
       "                                             coef    std err          t      P>|t|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------------------------------\n",
       "Intercept                               5.663e-32   1.39e-33     40.697      0.000    5.39e-32    5.94e-32\n",
       "global_subjectivity                    -1.002e-21   2.46e-23    -40.697      0.000   -1.05e-21   -9.54e-22\n",
       "np.power(global_subjectivity, 5)         -1.8e-23   4.42e-25    -40.697      0.000   -1.89e-23   -1.71e-23\n",
       "kw_max_avg                              4.256e-28   1.05e-29     40.697      0.000    4.05e-28    4.46e-28\n",
       "np.power(kw_max_avg, 5)                 7.509e-24   3.17e-24      2.367      0.018    1.29e-24    1.37e-23\n",
       "num_hrefs                               7.504e-31   1.84e-32     40.697      0.000    7.14e-31    7.87e-31\n",
       "np.power(num_hrefs, 5)                  1.354e-23   3.33e-25     40.697      0.000    1.29e-23    1.42e-23\n",
       "self_reference_min_shares               3.377e-28    8.3e-30     40.697      0.000    3.21e-28    3.54e-28\n",
       "np.power(self_reference_min_shares, 5)  2.042e-26   1.47e-26      1.390      0.165   -8.37e-27    4.92e-26\n",
       "data_channel                            2.597e-31   6.38e-33     40.697      0.000    2.47e-31    2.72e-31\n",
       "np.power(data_channel, 5)               3.887e-28   9.55e-30     40.697      0.000     3.7e-28    4.07e-28\n",
       "num_imgs                                3.651e-31   8.97e-33     40.697      0.000    3.48e-31    3.83e-31\n",
       "np.power(num_imgs, 5)                   1.139e-24    2.8e-26     40.697      0.000    1.08e-24    1.19e-24\n",
       "average_token_length                    2.653e-31   6.52e-33     40.697      0.000    2.52e-31    2.78e-31\n",
       "np.power(average_token_length, 5)       1.335e-28   3.28e-30     40.697      0.000    1.27e-28     1.4e-28\n",
       "num_self_hrefs                          1.862e-31   4.58e-33     40.697      0.000    1.77e-31    1.95e-31\n",
       "np.power(num_self_hrefs, 5)             1.495e-25   3.67e-27     40.697      0.000    1.42e-25    1.57e-25\n",
       "title_sentiment_polarity                5.238e-33   1.29e-34     40.697      0.000    4.99e-33    5.49e-33\n",
       "np.power(title_sentiment_polarity, 5)   1.251e-33   3.07e-35     40.697      0.000    1.19e-33    1.31e-33\n",
       "kw_min_avg                              1.676e-28   4.12e-30     40.697      0.000     1.6e-28    1.76e-28\n",
       "np.power(kw_min_avg, 5)                 1.631e-14   4.01e-16     40.697      0.000    1.55e-14    1.71e-14\n",
       "num_keywords                            3.577e-31   8.79e-33     40.697      0.000     3.4e-31    3.75e-31\n",
       "np.power(num_keywords, 5)               1.269e-27   3.12e-29     40.697      0.000    1.21e-27    1.33e-27\n",
       "n_tokens_title                          5.853e-31   1.44e-32     40.697      0.000    5.57e-31    6.14e-31\n",
       "np.power(n_tokens_title, 5)             9.586e-27   2.36e-28     40.697      0.000    9.12e-27       1e-26\n",
       "==============================================================================\n",
       "Omnibus:                    37211.834   Durbin-Watson:                   1.787\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):         14087045.465\n",
       "Skew:                           8.729   Prob(JB):                         0.00\n",
       "Kurtosis:                     116.234   Cond. No.                     1.75e+29\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.75e+29. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "degree = 5\n",
    "polynomial_regression_summary(train_df, predictors, 'shares', degree).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202da93f-907d-48aa-8abf-434af464f9cb",
   "metadata": {},
   "source": [
    "#### Summary of Results & Analysis: Initial Polynomial Regression\n",
    "The initial polynomial regression was performed using all available predictor variables. Degrees ranging from 2 to 5 were evaluated to find the best-fitting model. The highest achieved R-squared value was 0.043, which corresponded to a polynomial degree of 2. This modest improvement over the best R-squared value obtained from multilinear regression (0.038) suggested that polynomial terms helped capture some of the nonlinear trends in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf51ed5-ffa2-4248-91bd-0d23d75a5829",
   "metadata": {},
   "source": [
    "#### Polynomial Regression without Multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ab677a9a-91ab-4de0-b5ed-ceb6a250518d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>shares</td>      <th>  R-squared:         </th>  <td>   0.047</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.045</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   22.34</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 11 Aug 2023</td> <th>  Prob (F-statistic):</th>  <td>8.77e-223</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:34:51</td>     <th>  Log-Likelihood:    </th> <td>-2.6419e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 25756</td>      <th>  AIC:               </th>  <td>5.285e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 25698</td>      <th>  BIC:               </th>  <td>5.290e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    57</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                      <td></td>                         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                                 <td> 1.174e+04</td> <td> 6016.776</td> <td>    1.952</td> <td> 0.051</td> <td>  -50.617</td> <td> 2.35e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n_tokens_title</th>                            <td> -230.4734</td> <td>  149.368</td> <td>   -1.543</td> <td> 0.123</td> <td> -523.243</td> <td>   62.297</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(n_tokens_title, 2)</th>               <td>   13.8268</td> <td>    6.999</td> <td>    1.976</td> <td> 0.048</td> <td>    0.109</td> <td>   27.545</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n_tokens_content</th>                          <td>   -0.7626</td> <td>    0.266</td> <td>   -2.865</td> <td> 0.004</td> <td>   -1.284</td> <td>   -0.241</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(n_tokens_content, 2)</th>             <td>    0.0002</td> <td> 6.79e-05</td> <td>    3.001</td> <td> 0.003</td> <td> 7.07e-05</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n_non_stop_unique_tokens</th>                  <td> 1154.1138</td> <td>  633.546</td> <td>    1.822</td> <td> 0.069</td> <td>  -87.673</td> <td> 2395.901</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(n_non_stop_unique_tokens, 2)</th>     <td>   -1.7624</td> <td>    0.974</td> <td>   -1.809</td> <td> 0.070</td> <td>   -3.672</td> <td>    0.147</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_hrefs</th>                                 <td>   36.0616</td> <td>    7.482</td> <td>    4.820</td> <td> 0.000</td> <td>   21.396</td> <td>   50.727</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_hrefs, 2)</th>                    <td>    0.0145</td> <td>    0.070</td> <td>    0.207</td> <td> 0.836</td> <td>   -0.123</td> <td>    0.152</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_self_hrefs</th>                            <td>  -50.0096</td> <td>   18.904</td> <td>   -2.646</td> <td> 0.008</td> <td>  -87.062</td> <td>  -12.958</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_self_hrefs, 2)</th>               <td>    0.2250</td> <td>    0.474</td> <td>    0.475</td> <td> 0.635</td> <td>   -0.704</td> <td>    1.154</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_imgs</th>                                  <td>   58.1546</td> <td>   11.812</td> <td>    4.923</td> <td> 0.000</td> <td>   35.002</td> <td>   81.307</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_imgs, 2)</th>                     <td>   -0.6798</td> <td>    0.219</td> <td>   -3.106</td> <td> 0.002</td> <td>   -1.109</td> <td>   -0.251</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_videos</th>                                <td>   47.1148</td> <td>   19.798</td> <td>    2.380</td> <td> 0.017</td> <td>    8.310</td> <td>   85.919</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_videos, 2)</th>                   <td>   -1.0594</td> <td>    0.500</td> <td>   -2.118</td> <td> 0.034</td> <td>   -2.040</td> <td>   -0.079</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>average_token_length</th>                      <td>-4602.9170</td> <td> 2521.982</td> <td>   -1.825</td> <td> 0.068</td> <td>-9546.143</td> <td>  340.309</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(average_token_length, 2)</th>         <td>  412.9765</td> <td>  263.701</td> <td>    1.566</td> <td> 0.117</td> <td> -103.891</td> <td>  929.844</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_keywords</th>                              <td>  194.1481</td> <td>  176.736</td> <td>    1.099</td> <td> 0.272</td> <td> -152.264</td> <td>  540.561</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_keywords, 2)</th>                 <td>   -7.2808</td> <td>   12.013</td> <td>   -0.606</td> <td> 0.544</td> <td>  -30.827</td> <td>   16.266</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_min_min</th>                                <td>   13.9588</td> <td>   14.667</td> <td>    0.952</td> <td> 0.341</td> <td>  -14.790</td> <td>   42.708</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_min_min, 2)</th>                   <td>   -0.0530</td> <td>    0.066</td> <td>   -0.806</td> <td> 0.420</td> <td>   -0.182</td> <td>    0.076</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_max_min</th>                                <td>   -0.1173</td> <td>    0.024</td> <td>   -4.856</td> <td> 0.000</td> <td>   -0.165</td> <td>   -0.070</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_max_min, 2)</th>                   <td> 1.525e-06</td> <td> 2.56e-07</td> <td>    5.963</td> <td> 0.000</td> <td> 1.02e-06</td> <td> 2.03e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_min_max</th>                                <td>   -0.0103</td> <td>    0.003</td> <td>   -3.631</td> <td> 0.000</td> <td>   -0.016</td> <td>   -0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_min_max, 2)</th>                   <td> 7.503e-09</td> <td> 3.83e-09</td> <td>    1.961</td> <td> 0.050</td> <td> 1.93e-12</td> <td>  1.5e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_max_max</th>                                <td>    0.0012</td> <td>    0.001</td> <td>    1.028</td> <td> 0.304</td> <td>   -0.001</td> <td>    0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_max_max, 2)</th>                   <td>-1.278e-09</td> <td> 1.22e-09</td> <td>   -1.047</td> <td> 0.295</td> <td>-3.67e-09</td> <td> 1.12e-09</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_avg_max</th>                                <td>   -0.0007</td> <td>    0.002</td> <td>   -0.349</td> <td> 0.727</td> <td>   -0.005</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_avg_max, 2)</th>                   <td> 4.157e-09</td> <td> 2.94e-09</td> <td>    1.416</td> <td> 0.157</td> <td> -1.6e-09</td> <td> 9.91e-09</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_min_avg</th>                                <td>   -1.0743</td> <td>    0.127</td> <td>   -8.437</td> <td> 0.000</td> <td>   -1.324</td> <td>   -0.825</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_min_avg, 2)</th>                   <td>    0.0005</td> <td> 4.69e-05</td> <td>   10.160</td> <td> 0.000</td> <td>    0.000</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_max_avg</th>                                <td>    0.1836</td> <td>    0.021</td> <td>    8.950</td> <td> 0.000</td> <td>    0.143</td> <td>    0.224</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_max_avg, 2)</th>                   <td> -1.67e-06</td> <td> 2.47e-07</td> <td>   -6.769</td> <td> 0.000</td> <td>-2.15e-06</td> <td>-1.19e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>self_reference_avg_sharess</th>                <td>    0.0443</td> <td>    0.004</td> <td>   12.007</td> <td> 0.000</td> <td>    0.037</td> <td>    0.052</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(self_reference_avg_sharess, 2)</th>   <td>-7.327e-08</td> <td> 7.46e-09</td> <td>   -9.824</td> <td> 0.000</td> <td>-8.79e-08</td> <td>-5.86e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LDA_01</th>                                    <td> -917.9807</td> <td>  703.962</td> <td>   -1.304</td> <td> 0.192</td> <td>-2297.787</td> <td>  461.825</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(LDA_01, 2)</th>                       <td>  931.7250</td> <td>  905.135</td> <td>    1.029</td> <td> 0.303</td> <td> -842.391</td> <td> 2705.841</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>global_subjectivity</th>                       <td>  553.9343</td> <td> 3247.757</td> <td>    0.171</td> <td> 0.865</td> <td>-5811.853</td> <td> 6919.721</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(global_subjectivity, 2)</th>          <td> 3127.3381</td> <td> 3436.177</td> <td>    0.910</td> <td> 0.363</td> <td>-3607.762</td> <td> 9862.438</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>global_rate_positive_words</th>                <td>-3941.8216</td> <td> 3262.402</td> <td>   -1.208</td> <td> 0.227</td> <td>-1.03e+04</td> <td> 2452.671</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(global_rate_positive_words, 2)</th>   <td> -383.4732</td> <td>  323.026</td> <td>   -1.187</td> <td> 0.235</td> <td>-1016.621</td> <td>  249.675</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>avg_positive_polarity</th>                     <td>-2814.3820</td> <td> 3190.721</td> <td>   -0.882</td> <td> 0.378</td> <td>-9068.376</td> <td> 3439.612</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(avg_positive_polarity, 2)</th>        <td> 2534.8729</td> <td> 3924.548</td> <td>    0.646</td> <td> 0.518</td> <td>-5157.463</td> <td> 1.02e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>min_positive_polarity</th>                     <td>-1607.6553</td> <td> 1569.719</td> <td>   -1.024</td> <td> 0.306</td> <td>-4684.393</td> <td> 1469.082</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(min_positive_polarity, 2)</th>        <td> 1786.7249</td> <td> 3177.706</td> <td>    0.562</td> <td> 0.574</td> <td>-4441.758</td> <td> 8015.207</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>max_positive_polarity</th>                     <td> 1253.1561</td> <td> 1806.322</td> <td>    0.694</td> <td> 0.488</td> <td>-2287.337</td> <td> 4793.649</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(max_positive_polarity, 2)</th>        <td> -738.6140</td> <td> 1172.102</td> <td>   -0.630</td> <td> 0.529</td> <td>-3036.000</td> <td> 1558.772</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>min_negative_polarity</th>                     <td> -411.1482</td> <td>  643.966</td> <td>   -0.638</td> <td> 0.523</td> <td>-1673.358</td> <td>  851.062</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(min_negative_polarity, 2)</th>        <td>   50.8048</td> <td>  543.445</td> <td>    0.093</td> <td> 0.926</td> <td>-1014.379</td> <td> 1115.988</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>max_negative_polarity</th>                     <td> -343.6140</td> <td> 1086.866</td> <td>   -0.316</td> <td> 0.752</td> <td>-2473.933</td> <td> 1786.705</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(max_negative_polarity, 2)</th>        <td>  554.2873</td> <td> 1640.744</td> <td>    0.338</td> <td> 0.735</td> <td>-2661.663</td> <td> 3770.237</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>title_subjectivity</th>                        <td> 3267.4503</td> <td>  871.496</td> <td>    3.749</td> <td> 0.000</td> <td> 1559.268</td> <td> 4975.632</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(title_subjectivity, 2)</th>           <td>-2934.8242</td> <td>  871.794</td> <td>   -3.366</td> <td> 0.001</td> <td>-4643.589</td> <td>-1226.059</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>title_sentiment_polarity</th>                  <td>  651.5505</td> <td>  184.355</td> <td>    3.534</td> <td> 0.000</td> <td>  290.204</td> <td> 1012.897</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(title_sentiment_polarity, 2)</th>     <td> 1449.9901</td> <td>  409.755</td> <td>    3.539</td> <td> 0.000</td> <td>  646.847</td> <td> 2253.133</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>abs_title_subjectivity</th>                    <td> 3652.9724</td> <td> 1223.948</td> <td>    2.985</td> <td> 0.003</td> <td> 1253.966</td> <td> 6051.979</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(abs_title_subjectivity, 2)</th>       <td>-3260.7811</td> <td> 1554.657</td> <td>   -2.097</td> <td> 0.036</td> <td>-6307.996</td> <td> -213.566</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>abs_title_sentiment_polarity</th>              <td>-2000.4841</td> <td>  749.237</td> <td>   -2.670</td> <td> 0.008</td> <td>-3469.031</td> <td> -531.937</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(abs_title_sentiment_polarity, 2)</th> <td> 1449.9901</td> <td>  409.755</td> <td>    3.539</td> <td> 0.000</td> <td>  646.847</td> <td> 2253.133</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>data_channel</th>                              <td>   43.7705</td> <td>  148.397</td> <td>    0.295</td> <td> 0.768</td> <td> -247.097</td> <td>  334.638</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(data_channel, 2)</th>                 <td>    4.6444</td> <td>   17.581</td> <td>    0.264</td> <td> 0.792</td> <td>  -29.815</td> <td>   39.103</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weekday_is</th>                                <td> -412.0407</td> <td>  101.582</td> <td>   -4.056</td> <td> 0.000</td> <td> -611.147</td> <td> -212.934</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(weekday_is, 2)</th>                   <td>   56.2178</td> <td>   13.263</td> <td>    4.239</td> <td> 0.000</td> <td>   30.221</td> <td>   82.215</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>38036.285</td> <th>  Durbin-Watson:     </th>   <td>   1.995</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>15782195.727</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 9.089</td>   <th>  Prob(JB):          </th>   <td>    0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>122.899</td>  <th>  Cond. No.          </th>   <td>1.06e+16</td>  \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.06e+16. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                               &      shares      & \\textbf{  R-squared:         } &      0.047    \\\\\n",
       "\\textbf{Model:}                                       &       OLS        & \\textbf{  Adj. R-squared:    } &      0.045    \\\\\n",
       "\\textbf{Method:}                                      &  Least Squares   & \\textbf{  F-statistic:       } &      22.34    \\\\\n",
       "\\textbf{Date:}                                        & Fri, 11 Aug 2023 & \\textbf{  Prob (F-statistic):} &  8.77e-223    \\\\\n",
       "\\textbf{Time:}                                        &     11:34:51     & \\textbf{  Log-Likelihood:    } & -2.6419e+05   \\\\\n",
       "\\textbf{No. Observations:}                            &       25756      & \\textbf{  AIC:               } &  5.285e+05    \\\\\n",
       "\\textbf{Df Residuals:}                                &       25698      & \\textbf{  BIC:               } &  5.290e+05    \\\\\n",
       "\\textbf{Df Model:}                                    &          57      & \\textbf{                     } &               \\\\\n",
       "\\textbf{Covariance Type:}                             &    nonrobust     & \\textbf{                     } &               \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                                      & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                                    &    1.174e+04  &     6016.776     &     1.952  &         0.051        &      -50.617    &     2.35e+04     \\\\\n",
       "\\textbf{n\\_tokens\\_title}                             &    -230.4734  &      149.368     &    -1.543  &         0.123        &     -523.243    &       62.297     \\\\\n",
       "\\textbf{np.power(n\\_tokens\\_title, 2)}                &      13.8268  &        6.999     &     1.976  &         0.048        &        0.109    &       27.545     \\\\\n",
       "\\textbf{n\\_tokens\\_content}                           &      -0.7626  &        0.266     &    -2.865  &         0.004        &       -1.284    &       -0.241     \\\\\n",
       "\\textbf{np.power(n\\_tokens\\_content, 2)}              &       0.0002  &     6.79e-05     &     3.001  &         0.003        &     7.07e-05    &        0.000     \\\\\n",
       "\\textbf{n\\_non\\_stop\\_unique\\_tokens}                 &    1154.1138  &      633.546     &     1.822  &         0.069        &      -87.673    &     2395.901     \\\\\n",
       "\\textbf{np.power(n\\_non\\_stop\\_unique\\_tokens, 2)}    &      -1.7624  &        0.974     &    -1.809  &         0.070        &       -3.672    &        0.147     \\\\\n",
       "\\textbf{num\\_hrefs}                                   &      36.0616  &        7.482     &     4.820  &         0.000        &       21.396    &       50.727     \\\\\n",
       "\\textbf{np.power(num\\_hrefs, 2)}                      &       0.0145  &        0.070     &     0.207  &         0.836        &       -0.123    &        0.152     \\\\\n",
       "\\textbf{num\\_self\\_hrefs}                             &     -50.0096  &       18.904     &    -2.646  &         0.008        &      -87.062    &      -12.958     \\\\\n",
       "\\textbf{np.power(num\\_self\\_hrefs, 2)}                &       0.2250  &        0.474     &     0.475  &         0.635        &       -0.704    &        1.154     \\\\\n",
       "\\textbf{num\\_imgs}                                    &      58.1546  &       11.812     &     4.923  &         0.000        &       35.002    &       81.307     \\\\\n",
       "\\textbf{np.power(num\\_imgs, 2)}                       &      -0.6798  &        0.219     &    -3.106  &         0.002        &       -1.109    &       -0.251     \\\\\n",
       "\\textbf{num\\_videos}                                  &      47.1148  &       19.798     &     2.380  &         0.017        &        8.310    &       85.919     \\\\\n",
       "\\textbf{np.power(num\\_videos, 2)}                     &      -1.0594  &        0.500     &    -2.118  &         0.034        &       -2.040    &       -0.079     \\\\\n",
       "\\textbf{average\\_token\\_length}                       &   -4602.9170  &     2521.982     &    -1.825  &         0.068        &    -9546.143    &      340.309     \\\\\n",
       "\\textbf{np.power(average\\_token\\_length, 2)}          &     412.9765  &      263.701     &     1.566  &         0.117        &     -103.891    &      929.844     \\\\\n",
       "\\textbf{num\\_keywords}                                &     194.1481  &      176.736     &     1.099  &         0.272        &     -152.264    &      540.561     \\\\\n",
       "\\textbf{np.power(num\\_keywords, 2)}                   &      -7.2808  &       12.013     &    -0.606  &         0.544        &      -30.827    &       16.266     \\\\\n",
       "\\textbf{kw\\_min\\_min}                                 &      13.9588  &       14.667     &     0.952  &         0.341        &      -14.790    &       42.708     \\\\\n",
       "\\textbf{np.power(kw\\_min\\_min, 2)}                    &      -0.0530  &        0.066     &    -0.806  &         0.420        &       -0.182    &        0.076     \\\\\n",
       "\\textbf{kw\\_max\\_min}                                 &      -0.1173  &        0.024     &    -4.856  &         0.000        &       -0.165    &       -0.070     \\\\\n",
       "\\textbf{np.power(kw\\_max\\_min, 2)}                    &    1.525e-06  &     2.56e-07     &     5.963  &         0.000        &     1.02e-06    &     2.03e-06     \\\\\n",
       "\\textbf{kw\\_min\\_max}                                 &      -0.0103  &        0.003     &    -3.631  &         0.000        &       -0.016    &       -0.005     \\\\\n",
       "\\textbf{np.power(kw\\_min\\_max, 2)}                    &    7.503e-09  &     3.83e-09     &     1.961  &         0.050        &     1.93e-12    &      1.5e-08     \\\\\n",
       "\\textbf{kw\\_max\\_max}                                 &       0.0012  &        0.001     &     1.028  &         0.304        &       -0.001    &        0.004     \\\\\n",
       "\\textbf{np.power(kw\\_max\\_max, 2)}                    &   -1.278e-09  &     1.22e-09     &    -1.047  &         0.295        &    -3.67e-09    &     1.12e-09     \\\\\n",
       "\\textbf{kw\\_avg\\_max}                                 &      -0.0007  &        0.002     &    -0.349  &         0.727        &       -0.005    &        0.003     \\\\\n",
       "\\textbf{np.power(kw\\_avg\\_max, 2)}                    &    4.157e-09  &     2.94e-09     &     1.416  &         0.157        &     -1.6e-09    &     9.91e-09     \\\\\n",
       "\\textbf{kw\\_min\\_avg}                                 &      -1.0743  &        0.127     &    -8.437  &         0.000        &       -1.324    &       -0.825     \\\\\n",
       "\\textbf{np.power(kw\\_min\\_avg, 2)}                    &       0.0005  &     4.69e-05     &    10.160  &         0.000        &        0.000    &        0.001     \\\\\n",
       "\\textbf{kw\\_max\\_avg}                                 &       0.1836  &        0.021     &     8.950  &         0.000        &        0.143    &        0.224     \\\\\n",
       "\\textbf{np.power(kw\\_max\\_avg, 2)}                    &    -1.67e-06  &     2.47e-07     &    -6.769  &         0.000        &    -2.15e-06    &    -1.19e-06     \\\\\n",
       "\\textbf{self\\_reference\\_avg\\_sharess}                &       0.0443  &        0.004     &    12.007  &         0.000        &        0.037    &        0.052     \\\\\n",
       "\\textbf{np.power(self\\_reference\\_avg\\_sharess, 2)}   &   -7.327e-08  &     7.46e-09     &    -9.824  &         0.000        &    -8.79e-08    &    -5.86e-08     \\\\\n",
       "\\textbf{LDA\\_01}                                      &    -917.9807  &      703.962     &    -1.304  &         0.192        &    -2297.787    &      461.825     \\\\\n",
       "\\textbf{np.power(LDA\\_01, 2)}                         &     931.7250  &      905.135     &     1.029  &         0.303        &     -842.391    &     2705.841     \\\\\n",
       "\\textbf{global\\_subjectivity}                         &     553.9343  &     3247.757     &     0.171  &         0.865        &    -5811.853    &     6919.721     \\\\\n",
       "\\textbf{np.power(global\\_subjectivity, 2)}            &    3127.3381  &     3436.177     &     0.910  &         0.363        &    -3607.762    &     9862.438     \\\\\n",
       "\\textbf{global\\_rate\\_positive\\_words}                &   -3941.8216  &     3262.402     &    -1.208  &         0.227        &    -1.03e+04    &     2452.671     \\\\\n",
       "\\textbf{np.power(global\\_rate\\_positive\\_words, 2)}   &    -383.4732  &      323.026     &    -1.187  &         0.235        &    -1016.621    &      249.675     \\\\\n",
       "\\textbf{avg\\_positive\\_polarity}                      &   -2814.3820  &     3190.721     &    -0.882  &         0.378        &    -9068.376    &     3439.612     \\\\\n",
       "\\textbf{np.power(avg\\_positive\\_polarity, 2)}         &    2534.8729  &     3924.548     &     0.646  &         0.518        &    -5157.463    &     1.02e+04     \\\\\n",
       "\\textbf{min\\_positive\\_polarity}                      &   -1607.6553  &     1569.719     &    -1.024  &         0.306        &    -4684.393    &     1469.082     \\\\\n",
       "\\textbf{np.power(min\\_positive\\_polarity, 2)}         &    1786.7249  &     3177.706     &     0.562  &         0.574        &    -4441.758    &     8015.207     \\\\\n",
       "\\textbf{max\\_positive\\_polarity}                      &    1253.1561  &     1806.322     &     0.694  &         0.488        &    -2287.337    &     4793.649     \\\\\n",
       "\\textbf{np.power(max\\_positive\\_polarity, 2)}         &    -738.6140  &     1172.102     &    -0.630  &         0.529        &    -3036.000    &     1558.772     \\\\\n",
       "\\textbf{min\\_negative\\_polarity}                      &    -411.1482  &      643.966     &    -0.638  &         0.523        &    -1673.358    &      851.062     \\\\\n",
       "\\textbf{np.power(min\\_negative\\_polarity, 2)}         &      50.8048  &      543.445     &     0.093  &         0.926        &    -1014.379    &     1115.988     \\\\\n",
       "\\textbf{max\\_negative\\_polarity}                      &    -343.6140  &     1086.866     &    -0.316  &         0.752        &    -2473.933    &     1786.705     \\\\\n",
       "\\textbf{np.power(max\\_negative\\_polarity, 2)}         &     554.2873  &     1640.744     &     0.338  &         0.735        &    -2661.663    &     3770.237     \\\\\n",
       "\\textbf{title\\_subjectivity}                          &    3267.4503  &      871.496     &     3.749  &         0.000        &     1559.268    &     4975.632     \\\\\n",
       "\\textbf{np.power(title\\_subjectivity, 2)}             &   -2934.8242  &      871.794     &    -3.366  &         0.001        &    -4643.589    &    -1226.059     \\\\\n",
       "\\textbf{title\\_sentiment\\_polarity}                   &     651.5505  &      184.355     &     3.534  &         0.000        &      290.204    &     1012.897     \\\\\n",
       "\\textbf{np.power(title\\_sentiment\\_polarity, 2)}      &    1449.9901  &      409.755     &     3.539  &         0.000        &      646.847    &     2253.133     \\\\\n",
       "\\textbf{abs\\_title\\_subjectivity}                     &    3652.9724  &     1223.948     &     2.985  &         0.003        &     1253.966    &     6051.979     \\\\\n",
       "\\textbf{np.power(abs\\_title\\_subjectivity, 2)}        &   -3260.7811  &     1554.657     &    -2.097  &         0.036        &    -6307.996    &     -213.566     \\\\\n",
       "\\textbf{abs\\_title\\_sentiment\\_polarity}              &   -2000.4841  &      749.237     &    -2.670  &         0.008        &    -3469.031    &     -531.937     \\\\\n",
       "\\textbf{np.power(abs\\_title\\_sentiment\\_polarity, 2)} &    1449.9901  &      409.755     &     3.539  &         0.000        &      646.847    &     2253.133     \\\\\n",
       "\\textbf{data\\_channel}                                &      43.7705  &      148.397     &     0.295  &         0.768        &     -247.097    &      334.638     \\\\\n",
       "\\textbf{np.power(data\\_channel, 2)}                   &       4.6444  &       17.581     &     0.264  &         0.792        &      -29.815    &       39.103     \\\\\n",
       "\\textbf{weekday\\_is}                                  &    -412.0407  &      101.582     &    -4.056  &         0.000        &     -611.147    &     -212.934     \\\\\n",
       "\\textbf{np.power(weekday\\_is, 2)}                     &      56.2178  &       13.263     &     4.239  &         0.000        &       30.221    &       82.215     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 38036.285 & \\textbf{  Durbin-Watson:     } &      1.995    \\\\\n",
       "\\textbf{Prob(Omnibus):} &    0.000  & \\textbf{  Jarque-Bera (JB):  } & 15782195.727  \\\\\n",
       "\\textbf{Skew:}          &    9.089  & \\textbf{  Prob(JB):          } &       0.00    \\\\\n",
       "\\textbf{Kurtosis:}      &  122.899  & \\textbf{  Cond. No.          } &   1.06e+16    \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 1.06e+16. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 shares   R-squared:                       0.047\n",
       "Model:                            OLS   Adj. R-squared:                  0.045\n",
       "Method:                 Least Squares   F-statistic:                     22.34\n",
       "Date:                Fri, 11 Aug 2023   Prob (F-statistic):          8.77e-223\n",
       "Time:                        11:34:51   Log-Likelihood:            -2.6419e+05\n",
       "No. Observations:               25756   AIC:                         5.285e+05\n",
       "Df Residuals:                   25698   BIC:                         5.290e+05\n",
       "Df Model:                          57                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=============================================================================================================\n",
       "                                                coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------------------------------------\n",
       "Intercept                                  1.174e+04   6016.776      1.952      0.051     -50.617    2.35e+04\n",
       "n_tokens_title                             -230.4734    149.368     -1.543      0.123    -523.243      62.297\n",
       "np.power(n_tokens_title, 2)                  13.8268      6.999      1.976      0.048       0.109      27.545\n",
       "n_tokens_content                             -0.7626      0.266     -2.865      0.004      -1.284      -0.241\n",
       "np.power(n_tokens_content, 2)                 0.0002   6.79e-05      3.001      0.003    7.07e-05       0.000\n",
       "n_non_stop_unique_tokens                   1154.1138    633.546      1.822      0.069     -87.673    2395.901\n",
       "np.power(n_non_stop_unique_tokens, 2)        -1.7624      0.974     -1.809      0.070      -3.672       0.147\n",
       "num_hrefs                                    36.0616      7.482      4.820      0.000      21.396      50.727\n",
       "np.power(num_hrefs, 2)                        0.0145      0.070      0.207      0.836      -0.123       0.152\n",
       "num_self_hrefs                              -50.0096     18.904     -2.646      0.008     -87.062     -12.958\n",
       "np.power(num_self_hrefs, 2)                   0.2250      0.474      0.475      0.635      -0.704       1.154\n",
       "num_imgs                                     58.1546     11.812      4.923      0.000      35.002      81.307\n",
       "np.power(num_imgs, 2)                        -0.6798      0.219     -3.106      0.002      -1.109      -0.251\n",
       "num_videos                                   47.1148     19.798      2.380      0.017       8.310      85.919\n",
       "np.power(num_videos, 2)                      -1.0594      0.500     -2.118      0.034      -2.040      -0.079\n",
       "average_token_length                      -4602.9170   2521.982     -1.825      0.068   -9546.143     340.309\n",
       "np.power(average_token_length, 2)           412.9765    263.701      1.566      0.117    -103.891     929.844\n",
       "num_keywords                                194.1481    176.736      1.099      0.272    -152.264     540.561\n",
       "np.power(num_keywords, 2)                    -7.2808     12.013     -0.606      0.544     -30.827      16.266\n",
       "kw_min_min                                   13.9588     14.667      0.952      0.341     -14.790      42.708\n",
       "np.power(kw_min_min, 2)                      -0.0530      0.066     -0.806      0.420      -0.182       0.076\n",
       "kw_max_min                                   -0.1173      0.024     -4.856      0.000      -0.165      -0.070\n",
       "np.power(kw_max_min, 2)                    1.525e-06   2.56e-07      5.963      0.000    1.02e-06    2.03e-06\n",
       "kw_min_max                                   -0.0103      0.003     -3.631      0.000      -0.016      -0.005\n",
       "np.power(kw_min_max, 2)                    7.503e-09   3.83e-09      1.961      0.050    1.93e-12     1.5e-08\n",
       "kw_max_max                                    0.0012      0.001      1.028      0.304      -0.001       0.004\n",
       "np.power(kw_max_max, 2)                   -1.278e-09   1.22e-09     -1.047      0.295   -3.67e-09    1.12e-09\n",
       "kw_avg_max                                   -0.0007      0.002     -0.349      0.727      -0.005       0.003\n",
       "np.power(kw_avg_max, 2)                    4.157e-09   2.94e-09      1.416      0.157    -1.6e-09    9.91e-09\n",
       "kw_min_avg                                   -1.0743      0.127     -8.437      0.000      -1.324      -0.825\n",
       "np.power(kw_min_avg, 2)                       0.0005   4.69e-05     10.160      0.000       0.000       0.001\n",
       "kw_max_avg                                    0.1836      0.021      8.950      0.000       0.143       0.224\n",
       "np.power(kw_max_avg, 2)                    -1.67e-06   2.47e-07     -6.769      0.000   -2.15e-06   -1.19e-06\n",
       "self_reference_avg_sharess                    0.0443      0.004     12.007      0.000       0.037       0.052\n",
       "np.power(self_reference_avg_sharess, 2)   -7.327e-08   7.46e-09     -9.824      0.000   -8.79e-08   -5.86e-08\n",
       "LDA_01                                     -917.9807    703.962     -1.304      0.192   -2297.787     461.825\n",
       "np.power(LDA_01, 2)                         931.7250    905.135      1.029      0.303    -842.391    2705.841\n",
       "global_subjectivity                         553.9343   3247.757      0.171      0.865   -5811.853    6919.721\n",
       "np.power(global_subjectivity, 2)           3127.3381   3436.177      0.910      0.363   -3607.762    9862.438\n",
       "global_rate_positive_words                -3941.8216   3262.402     -1.208      0.227   -1.03e+04    2452.671\n",
       "np.power(global_rate_positive_words, 2)    -383.4732    323.026     -1.187      0.235   -1016.621     249.675\n",
       "avg_positive_polarity                     -2814.3820   3190.721     -0.882      0.378   -9068.376    3439.612\n",
       "np.power(avg_positive_polarity, 2)         2534.8729   3924.548      0.646      0.518   -5157.463    1.02e+04\n",
       "min_positive_polarity                     -1607.6553   1569.719     -1.024      0.306   -4684.393    1469.082\n",
       "np.power(min_positive_polarity, 2)         1786.7249   3177.706      0.562      0.574   -4441.758    8015.207\n",
       "max_positive_polarity                      1253.1561   1806.322      0.694      0.488   -2287.337    4793.649\n",
       "np.power(max_positive_polarity, 2)         -738.6140   1172.102     -0.630      0.529   -3036.000    1558.772\n",
       "min_negative_polarity                      -411.1482    643.966     -0.638      0.523   -1673.358     851.062\n",
       "np.power(min_negative_polarity, 2)           50.8048    543.445      0.093      0.926   -1014.379    1115.988\n",
       "max_negative_polarity                      -343.6140   1086.866     -0.316      0.752   -2473.933    1786.705\n",
       "np.power(max_negative_polarity, 2)          554.2873   1640.744      0.338      0.735   -2661.663    3770.237\n",
       "title_subjectivity                         3267.4503    871.496      3.749      0.000    1559.268    4975.632\n",
       "np.power(title_subjectivity, 2)           -2934.8242    871.794     -3.366      0.001   -4643.589   -1226.059\n",
       "title_sentiment_polarity                    651.5505    184.355      3.534      0.000     290.204    1012.897\n",
       "np.power(title_sentiment_polarity, 2)      1449.9901    409.755      3.539      0.000     646.847    2253.133\n",
       "abs_title_subjectivity                     3652.9724   1223.948      2.985      0.003    1253.966    6051.979\n",
       "np.power(abs_title_subjectivity, 2)       -3260.7811   1554.657     -2.097      0.036   -6307.996    -213.566\n",
       "abs_title_sentiment_polarity              -2000.4841    749.237     -2.670      0.008   -3469.031    -531.937\n",
       "np.power(abs_title_sentiment_polarity, 2)  1449.9901    409.755      3.539      0.000     646.847    2253.133\n",
       "data_channel                                 43.7705    148.397      0.295      0.768    -247.097     334.638\n",
       "np.power(data_channel, 2)                     4.6444     17.581      0.264      0.792     -29.815      39.103\n",
       "weekday_is                                 -412.0407    101.582     -4.056      0.000    -611.147    -212.934\n",
       "np.power(weekday_is, 2)                      56.2178     13.263      4.239      0.000      30.221      82.215\n",
       "==============================================================================\n",
       "Omnibus:                    38036.285   Durbin-Watson:                   1.995\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):         15782195.727\n",
       "Skew:                           9.089   Prob(JB):                         0.00\n",
       "Kurtosis:                     122.899   Cond. No.                     1.06e+16\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.06e+16. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "degree = 2\n",
    "polynomial_regression_summary(train_df, predictors_wo_multicol, 'shares', degree).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "db396b19-2fde-441f-b4f0-a971fe3c3d73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>shares</td>      <th>  R-squared:         </th>  <td>   0.028</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.027</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   43.33</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 11 Aug 2023</td> <th>  Prob (F-statistic):</th>  <td>2.03e-143</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:34:52</td>     <th>  Log-Likelihood:    </th> <td>-2.6445e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 25756</td>      <th>  AIC:               </th>  <td>5.289e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 25738</td>      <th>  BIC:               </th>  <td>5.291e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    17</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                      <td></td>                         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                                 <td>    0.0012</td> <td>    0.000</td> <td>    3.390</td> <td> 0.001</td> <td>    0.001</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n_tokens_title</th>                            <td>    0.0016</td> <td>    0.000</td> <td>    4.033</td> <td> 0.000</td> <td>    0.001</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(n_tokens_title, 3)</th>               <td>    0.2202</td> <td>    0.065</td> <td>    3.387</td> <td> 0.001</td> <td>    0.093</td> <td>    0.348</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n_tokens_content</th>                          <td>    0.0396</td> <td>    0.048</td> <td>    0.819</td> <td> 0.413</td> <td>   -0.055</td> <td>    0.134</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(n_tokens_content, 3)</th>             <td> 8.425e-09</td> <td>  7.1e-09</td> <td>    1.187</td> <td> 0.235</td> <td>-5.48e-09</td> <td> 2.23e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n_non_stop_unique_tokens</th>                  <td>-2.428e-06</td> <td> 6.84e-06</td> <td>   -0.355</td> <td> 0.723</td> <td>-1.58e-05</td> <td>  1.1e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(n_non_stop_unique_tokens, 3)</th>     <td> 9.781e-06</td> <td> 2.54e-05</td> <td>    0.385</td> <td> 0.700</td> <td>   -4e-05</td> <td> 5.95e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_hrefs</th>                                 <td>    0.0005</td> <td>    0.001</td> <td>    0.716</td> <td> 0.474</td> <td>   -0.001</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_hrefs, 3)</th>                    <td>    0.0007</td> <td>    0.000</td> <td>    3.320</td> <td> 0.001</td> <td>    0.000</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_self_hrefs</th>                            <td>    0.0001</td> <td>    0.000</td> <td>    0.761</td> <td> 0.447</td> <td>   -0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_self_hrefs, 3)</th>               <td>   -0.0011</td> <td>    0.004</td> <td>   -0.294</td> <td> 0.769</td> <td>   -0.009</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_imgs</th>                                  <td>    0.0003</td> <td>    0.000</td> <td>    0.953</td> <td> 0.340</td> <td>   -0.000</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_imgs, 3)</th>                     <td>    0.0014</td> <td>    0.002</td> <td>    0.876</td> <td> 0.381</td> <td>   -0.002</td> <td>    0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_videos</th>                                <td> 9.834e-05</td> <td> 5.49e-05</td> <td>    1.792</td> <td> 0.073</td> <td>-9.22e-06</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_videos, 3)</th>                   <td>-5.349e-05</td> <td>    0.004</td> <td>   -0.012</td> <td> 0.990</td> <td>   -0.009</td> <td>    0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>average_token_length</th>                      <td> 1.029e-05</td> <td> 3.42e-06</td> <td>    3.011</td> <td> 0.003</td> <td> 3.59e-06</td> <td>  1.7e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(average_token_length, 3)</th>         <td>   -0.0002</td> <td>    0.000</td> <td>   -1.596</td> <td> 0.110</td> <td>   -0.001</td> <td> 5.39e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_keywords</th>                              <td>    0.0002</td> <td>    0.000</td> <td>    0.982</td> <td> 0.326</td> <td>   -0.000</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_keywords, 3)</th>                 <td>    0.0398</td> <td>    0.045</td> <td>    0.885</td> <td> 0.376</td> <td>   -0.048</td> <td>    0.128</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_min_min</th>                                <td>-2.313e-05</td> <td> 1.49e-05</td> <td>   -1.550</td> <td> 0.121</td> <td>-5.24e-05</td> <td> 6.11e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_min_min, 3)</th>                   <td>    0.0001</td> <td>  1.5e-05</td> <td>    8.806</td> <td> 0.000</td> <td>    0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_max_min</th>                                <td>   -0.1456</td> <td>    0.019</td> <td>   -7.493</td> <td> 0.000</td> <td>   -0.184</td> <td>   -0.107</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_max_min, 3)</th>                   <td> 1.702e-11</td> <td> 1.73e-12</td> <td>    9.839</td> <td> 0.000</td> <td> 1.36e-11</td> <td> 2.04e-11</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_min_max</th>                                <td>   -0.0122</td> <td>    0.002</td> <td>   -5.875</td> <td> 0.000</td> <td>   -0.016</td> <td>   -0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_min_max, 3)</th>                   <td> 8.852e-15</td> <td> 3.88e-15</td> <td>    2.283</td> <td> 0.022</td> <td> 1.25e-15</td> <td> 1.65e-14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_max_max</th>                                <td>    0.0023</td> <td>    0.001</td> <td>    4.097</td> <td> 0.000</td> <td>    0.001</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_max_max, 3)</th>                   <td>-1.031e-15</td> <td> 7.26e-16</td> <td>   -1.420</td> <td> 0.156</td> <td>-2.45e-15</td> <td> 3.92e-16</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_avg_max</th>                                <td>   -0.0010</td> <td>    0.001</td> <td>   -0.912</td> <td> 0.362</td> <td>   -0.003</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_avg_max, 3)</th>                   <td> 7.543e-15</td> <td> 2.72e-15</td> <td>    2.773</td> <td> 0.006</td> <td> 2.21e-15</td> <td> 1.29e-14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_min_avg</th>                                <td>    0.0199</td> <td>    0.031</td> <td>    0.632</td> <td> 0.527</td> <td>   -0.042</td> <td>    0.081</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_min_avg, 3)</th>                   <td> 5.688e-08</td> <td> 6.24e-09</td> <td>    9.113</td> <td> 0.000</td> <td> 4.46e-08</td> <td> 6.91e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_max_avg</th>                                <td>    0.2142</td> <td>    0.014</td> <td>   14.974</td> <td> 0.000</td> <td>    0.186</td> <td>    0.242</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_max_avg, 3)</th>                   <td>-1.735e-11</td> <td> 1.72e-12</td> <td>  -10.109</td> <td> 0.000</td> <td>-2.07e-11</td> <td> -1.4e-11</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>self_reference_avg_sharess</th>                <td>    0.0346</td> <td>    0.003</td> <td>   11.979</td> <td> 0.000</td> <td>    0.029</td> <td>    0.040</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(self_reference_avg_sharess, 3)</th>   <td>-8.732e-14</td> <td> 9.39e-15</td> <td>   -9.302</td> <td> 0.000</td> <td>-1.06e-13</td> <td>-6.89e-14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LDA_01</th>                                    <td> 2.649e-06</td> <td> 5.36e-06</td> <td>    0.494</td> <td> 0.621</td> <td>-7.85e-06</td> <td> 1.31e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(LDA_01, 3)</th>                       <td> 9.926e-07</td> <td> 3.52e-06</td> <td>    0.282</td> <td> 0.778</td> <td> -5.9e-06</td> <td> 7.89e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>global_subjectivity</th>                       <td> 3.162e-06</td> <td> 1.18e-06</td> <td>    2.682</td> <td> 0.007</td> <td> 8.51e-07</td> <td> 5.47e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(global_subjectivity, 3)</th>          <td> 8.599e-07</td> <td> 2.13e-07</td> <td>    4.046</td> <td> 0.000</td> <td> 4.43e-07</td> <td> 1.28e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>global_rate_positive_words</th>                <td> 3.118e-07</td> <td> 3.59e-07</td> <td>    0.869</td> <td> 0.385</td> <td>-3.92e-07</td> <td> 1.02e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(global_rate_positive_words, 3)</th>   <td> 3.308e-10</td> <td> 1.17e-09</td> <td>    0.284</td> <td> 0.777</td> <td>-1.96e-09</td> <td> 2.62e-09</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>avg_positive_polarity</th>                     <td> 2.636e-06</td> <td>  1.4e-06</td> <td>    1.878</td> <td> 0.060</td> <td>-1.15e-07</td> <td> 5.39e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(avg_positive_polarity, 3)</th>        <td> 3.624e-07</td> <td> 1.53e-07</td> <td>    2.373</td> <td> 0.018</td> <td>  6.3e-08</td> <td> 6.62e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>min_positive_polarity</th>                     <td>-1.532e-06</td> <td> 2.66e-06</td> <td>   -0.577</td> <td> 0.564</td> <td>-6.74e-06</td> <td> 3.68e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(min_positive_polarity, 3)</th>        <td>-2.812e-07</td> <td> 2.67e-07</td> <td>   -1.054</td> <td> 0.292</td> <td>-8.04e-07</td> <td> 2.42e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>max_positive_polarity</th>                     <td> 1.387e-05</td> <td> 1.26e-05</td> <td>    1.105</td> <td> 0.269</td> <td>-1.07e-05</td> <td> 3.85e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(max_positive_polarity, 3)</th>        <td> 1.981e-05</td> <td> 2.08e-05</td> <td>    0.953</td> <td> 0.341</td> <td>-2.09e-05</td> <td> 6.05e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>min_negative_polarity</th>                     <td>-1.635e-05</td> <td> 1.35e-05</td> <td>   -1.216</td> <td> 0.224</td> <td>-4.27e-05</td> <td>    1e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(min_negative_polarity, 3)</th>        <td>-1.592e-05</td> <td> 1.28e-05</td> <td>   -1.246</td> <td> 0.213</td> <td> -4.1e-05</td> <td> 9.13e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>max_negative_polarity</th>                     <td> 1.568e-06</td> <td> 3.01e-06</td> <td>    0.520</td> <td> 0.603</td> <td>-4.34e-06</td> <td> 7.47e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(max_negative_polarity, 3)</th>        <td> 3.266e-07</td> <td> 6.04e-07</td> <td>    0.541</td> <td> 0.589</td> <td>-8.58e-07</td> <td> 1.51e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>title_subjectivity</th>                        <td> 1.001e-05</td> <td> 1.94e-06</td> <td>    5.168</td> <td> 0.000</td> <td> 6.21e-06</td> <td> 1.38e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(title_subjectivity, 3)</th>           <td>  2.26e-06</td> <td> 9.87e-07</td> <td>    2.290</td> <td> 0.022</td> <td> 3.25e-07</td> <td> 4.19e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>title_sentiment_polarity</th>                  <td> 3.003e-06</td> <td> 1.98e-06</td> <td>    1.516</td> <td> 0.130</td> <td> -8.8e-07</td> <td> 6.89e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(title_sentiment_polarity, 3)</th>     <td> 9.209e-07</td> <td> 1.18e-06</td> <td>    0.783</td> <td> 0.434</td> <td>-1.38e-06</td> <td> 3.23e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>abs_title_subjectivity</th>                    <td>-6.302e-06</td> <td> 2.21e-06</td> <td>   -2.851</td> <td> 0.004</td> <td>-1.06e-05</td> <td>-1.97e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(abs_title_subjectivity, 3)</th>       <td>-2.321e-06</td> <td> 7.34e-07</td> <td>   -3.162</td> <td> 0.002</td> <td>-3.76e-06</td> <td>-8.82e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>abs_title_sentiment_polarity</th>              <td> 5.832e-06</td> <td> 1.65e-06</td> <td>    3.541</td> <td> 0.000</td> <td>  2.6e-06</td> <td> 9.06e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(abs_title_sentiment_polarity, 3)</th> <td> 1.193e-06</td> <td> 1.14e-06</td> <td>    1.047</td> <td> 0.295</td> <td>-1.04e-06</td> <td> 3.43e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>data_channel</th>                              <td>  1.62e-05</td> <td> 1.88e-05</td> <td>    0.861</td> <td> 0.389</td> <td>-2.07e-05</td> <td> 5.31e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(data_channel, 3)</th>                 <td>    0.0015</td> <td>    0.001</td> <td>    1.447</td> <td> 0.148</td> <td>   -0.001</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weekday_is</th>                                <td> 3.652e-05</td> <td> 2.64e-05</td> <td>    1.384</td> <td> 0.167</td> <td>-1.52e-05</td> <td> 8.83e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(weekday_is, 3)</th>                   <td>    0.0018</td> <td>    0.002</td> <td>    1.133</td> <td> 0.257</td> <td>   -0.001</td> <td>    0.005</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>38084.337</td> <th>  Durbin-Watson:     </th>   <td>   1.996</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>15770209.863</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 9.114</td>   <th>  Prob(JB):          </th>   <td>    0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>122.845</td>  <th>  Cond. No.          </th>   <td>2.33e+16</td>  \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 2.33e+16. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                               &      shares      & \\textbf{  R-squared:         } &      0.028    \\\\\n",
       "\\textbf{Model:}                                       &       OLS        & \\textbf{  Adj. R-squared:    } &      0.027    \\\\\n",
       "\\textbf{Method:}                                      &  Least Squares   & \\textbf{  F-statistic:       } &      43.33    \\\\\n",
       "\\textbf{Date:}                                        & Fri, 11 Aug 2023 & \\textbf{  Prob (F-statistic):} &  2.03e-143    \\\\\n",
       "\\textbf{Time:}                                        &     11:34:52     & \\textbf{  Log-Likelihood:    } & -2.6445e+05   \\\\\n",
       "\\textbf{No. Observations:}                            &       25756      & \\textbf{  AIC:               } &  5.289e+05    \\\\\n",
       "\\textbf{Df Residuals:}                                &       25738      & \\textbf{  BIC:               } &  5.291e+05    \\\\\n",
       "\\textbf{Df Model:}                                    &          17      & \\textbf{                     } &               \\\\\n",
       "\\textbf{Covariance Type:}                             &    nonrobust     & \\textbf{                     } &               \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                                      & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                                    &       0.0012  &        0.000     &     3.390  &         0.001        &        0.001    &        0.002     \\\\\n",
       "\\textbf{n\\_tokens\\_title}                             &       0.0016  &        0.000     &     4.033  &         0.000        &        0.001    &        0.002     \\\\\n",
       "\\textbf{np.power(n\\_tokens\\_title, 3)}                &       0.2202  &        0.065     &     3.387  &         0.001        &        0.093    &        0.348     \\\\\n",
       "\\textbf{n\\_tokens\\_content}                           &       0.0396  &        0.048     &     0.819  &         0.413        &       -0.055    &        0.134     \\\\\n",
       "\\textbf{np.power(n\\_tokens\\_content, 3)}              &    8.425e-09  &      7.1e-09     &     1.187  &         0.235        &    -5.48e-09    &     2.23e-08     \\\\\n",
       "\\textbf{n\\_non\\_stop\\_unique\\_tokens}                 &   -2.428e-06  &     6.84e-06     &    -0.355  &         0.723        &    -1.58e-05    &      1.1e-05     \\\\\n",
       "\\textbf{np.power(n\\_non\\_stop\\_unique\\_tokens, 3)}    &    9.781e-06  &     2.54e-05     &     0.385  &         0.700        &       -4e-05    &     5.95e-05     \\\\\n",
       "\\textbf{num\\_hrefs}                                   &       0.0005  &        0.001     &     0.716  &         0.474        &       -0.001    &        0.002     \\\\\n",
       "\\textbf{np.power(num\\_hrefs, 3)}                      &       0.0007  &        0.000     &     3.320  &         0.001        &        0.000    &        0.001     \\\\\n",
       "\\textbf{num\\_self\\_hrefs}                             &       0.0001  &        0.000     &     0.761  &         0.447        &       -0.000    &        0.000     \\\\\n",
       "\\textbf{np.power(num\\_self\\_hrefs, 3)}                &      -0.0011  &        0.004     &    -0.294  &         0.769        &       -0.009    &        0.006     \\\\\n",
       "\\textbf{num\\_imgs}                                    &       0.0003  &        0.000     &     0.953  &         0.340        &       -0.000    &        0.001     \\\\\n",
       "\\textbf{np.power(num\\_imgs, 3)}                       &       0.0014  &        0.002     &     0.876  &         0.381        &       -0.002    &        0.005     \\\\\n",
       "\\textbf{num\\_videos}                                  &    9.834e-05  &     5.49e-05     &     1.792  &         0.073        &    -9.22e-06    &        0.000     \\\\\n",
       "\\textbf{np.power(num\\_videos, 3)}                     &   -5.349e-05  &        0.004     &    -0.012  &         0.990        &       -0.009    &        0.009     \\\\\n",
       "\\textbf{average\\_token\\_length}                       &    1.029e-05  &     3.42e-06     &     3.011  &         0.003        &     3.59e-06    &      1.7e-05     \\\\\n",
       "\\textbf{np.power(average\\_token\\_length, 3)}          &      -0.0002  &        0.000     &    -1.596  &         0.110        &       -0.001    &     5.39e-05     \\\\\n",
       "\\textbf{num\\_keywords}                                &       0.0002  &        0.000     &     0.982  &         0.326        &       -0.000    &        0.001     \\\\\n",
       "\\textbf{np.power(num\\_keywords, 3)}                   &       0.0398  &        0.045     &     0.885  &         0.376        &       -0.048    &        0.128     \\\\\n",
       "\\textbf{kw\\_min\\_min}                                 &   -2.313e-05  &     1.49e-05     &    -1.550  &         0.121        &    -5.24e-05    &     6.11e-06     \\\\\n",
       "\\textbf{np.power(kw\\_min\\_min, 3)}                    &       0.0001  &      1.5e-05     &     8.806  &         0.000        &        0.000    &        0.000     \\\\\n",
       "\\textbf{kw\\_max\\_min}                                 &      -0.1456  &        0.019     &    -7.493  &         0.000        &       -0.184    &       -0.107     \\\\\n",
       "\\textbf{np.power(kw\\_max\\_min, 3)}                    &    1.702e-11  &     1.73e-12     &     9.839  &         0.000        &     1.36e-11    &     2.04e-11     \\\\\n",
       "\\textbf{kw\\_min\\_max}                                 &      -0.0122  &        0.002     &    -5.875  &         0.000        &       -0.016    &       -0.008     \\\\\n",
       "\\textbf{np.power(kw\\_min\\_max, 3)}                    &    8.852e-15  &     3.88e-15     &     2.283  &         0.022        &     1.25e-15    &     1.65e-14     \\\\\n",
       "\\textbf{kw\\_max\\_max}                                 &       0.0023  &        0.001     &     4.097  &         0.000        &        0.001    &        0.003     \\\\\n",
       "\\textbf{np.power(kw\\_max\\_max, 3)}                    &   -1.031e-15  &     7.26e-16     &    -1.420  &         0.156        &    -2.45e-15    &     3.92e-16     \\\\\n",
       "\\textbf{kw\\_avg\\_max}                                 &      -0.0010  &        0.001     &    -0.912  &         0.362        &       -0.003    &        0.001     \\\\\n",
       "\\textbf{np.power(kw\\_avg\\_max, 3)}                    &    7.543e-15  &     2.72e-15     &     2.773  &         0.006        &     2.21e-15    &     1.29e-14     \\\\\n",
       "\\textbf{kw\\_min\\_avg}                                 &       0.0199  &        0.031     &     0.632  &         0.527        &       -0.042    &        0.081     \\\\\n",
       "\\textbf{np.power(kw\\_min\\_avg, 3)}                    &    5.688e-08  &     6.24e-09     &     9.113  &         0.000        &     4.46e-08    &     6.91e-08     \\\\\n",
       "\\textbf{kw\\_max\\_avg}                                 &       0.2142  &        0.014     &    14.974  &         0.000        &        0.186    &        0.242     \\\\\n",
       "\\textbf{np.power(kw\\_max\\_avg, 3)}                    &   -1.735e-11  &     1.72e-12     &   -10.109  &         0.000        &    -2.07e-11    &     -1.4e-11     \\\\\n",
       "\\textbf{self\\_reference\\_avg\\_sharess}                &       0.0346  &        0.003     &    11.979  &         0.000        &        0.029    &        0.040     \\\\\n",
       "\\textbf{np.power(self\\_reference\\_avg\\_sharess, 3)}   &   -8.732e-14  &     9.39e-15     &    -9.302  &         0.000        &    -1.06e-13    &    -6.89e-14     \\\\\n",
       "\\textbf{LDA\\_01}                                      &    2.649e-06  &     5.36e-06     &     0.494  &         0.621        &    -7.85e-06    &     1.31e-05     \\\\\n",
       "\\textbf{np.power(LDA\\_01, 3)}                         &    9.926e-07  &     3.52e-06     &     0.282  &         0.778        &     -5.9e-06    &     7.89e-06     \\\\\n",
       "\\textbf{global\\_subjectivity}                         &    3.162e-06  &     1.18e-06     &     2.682  &         0.007        &     8.51e-07    &     5.47e-06     \\\\\n",
       "\\textbf{np.power(global\\_subjectivity, 3)}            &    8.599e-07  &     2.13e-07     &     4.046  &         0.000        &     4.43e-07    &     1.28e-06     \\\\\n",
       "\\textbf{global\\_rate\\_positive\\_words}                &    3.118e-07  &     3.59e-07     &     0.869  &         0.385        &    -3.92e-07    &     1.02e-06     \\\\\n",
       "\\textbf{np.power(global\\_rate\\_positive\\_words, 3)}   &    3.308e-10  &     1.17e-09     &     0.284  &         0.777        &    -1.96e-09    &     2.62e-09     \\\\\n",
       "\\textbf{avg\\_positive\\_polarity}                      &    2.636e-06  &      1.4e-06     &     1.878  &         0.060        &    -1.15e-07    &     5.39e-06     \\\\\n",
       "\\textbf{np.power(avg\\_positive\\_polarity, 3)}         &    3.624e-07  &     1.53e-07     &     2.373  &         0.018        &      6.3e-08    &     6.62e-07     \\\\\n",
       "\\textbf{min\\_positive\\_polarity}                      &   -1.532e-06  &     2.66e-06     &    -0.577  &         0.564        &    -6.74e-06    &     3.68e-06     \\\\\n",
       "\\textbf{np.power(min\\_positive\\_polarity, 3)}         &   -2.812e-07  &     2.67e-07     &    -1.054  &         0.292        &    -8.04e-07    &     2.42e-07     \\\\\n",
       "\\textbf{max\\_positive\\_polarity}                      &    1.387e-05  &     1.26e-05     &     1.105  &         0.269        &    -1.07e-05    &     3.85e-05     \\\\\n",
       "\\textbf{np.power(max\\_positive\\_polarity, 3)}         &    1.981e-05  &     2.08e-05     &     0.953  &         0.341        &    -2.09e-05    &     6.05e-05     \\\\\n",
       "\\textbf{min\\_negative\\_polarity}                      &   -1.635e-05  &     1.35e-05     &    -1.216  &         0.224        &    -4.27e-05    &        1e-05     \\\\\n",
       "\\textbf{np.power(min\\_negative\\_polarity, 3)}         &   -1.592e-05  &     1.28e-05     &    -1.246  &         0.213        &     -4.1e-05    &     9.13e-06     \\\\\n",
       "\\textbf{max\\_negative\\_polarity}                      &    1.568e-06  &     3.01e-06     &     0.520  &         0.603        &    -4.34e-06    &     7.47e-06     \\\\\n",
       "\\textbf{np.power(max\\_negative\\_polarity, 3)}         &    3.266e-07  &     6.04e-07     &     0.541  &         0.589        &    -8.58e-07    &     1.51e-06     \\\\\n",
       "\\textbf{title\\_subjectivity}                          &    1.001e-05  &     1.94e-06     &     5.168  &         0.000        &     6.21e-06    &     1.38e-05     \\\\\n",
       "\\textbf{np.power(title\\_subjectivity, 3)}             &     2.26e-06  &     9.87e-07     &     2.290  &         0.022        &     3.25e-07    &     4.19e-06     \\\\\n",
       "\\textbf{title\\_sentiment\\_polarity}                   &    3.003e-06  &     1.98e-06     &     1.516  &         0.130        &     -8.8e-07    &     6.89e-06     \\\\\n",
       "\\textbf{np.power(title\\_sentiment\\_polarity, 3)}      &    9.209e-07  &     1.18e-06     &     0.783  &         0.434        &    -1.38e-06    &     3.23e-06     \\\\\n",
       "\\textbf{abs\\_title\\_subjectivity}                     &   -6.302e-06  &     2.21e-06     &    -2.851  &         0.004        &    -1.06e-05    &    -1.97e-06     \\\\\n",
       "\\textbf{np.power(abs\\_title\\_subjectivity, 3)}        &   -2.321e-06  &     7.34e-07     &    -3.162  &         0.002        &    -3.76e-06    &    -8.82e-07     \\\\\n",
       "\\textbf{abs\\_title\\_sentiment\\_polarity}              &    5.832e-06  &     1.65e-06     &     3.541  &         0.000        &      2.6e-06    &     9.06e-06     \\\\\n",
       "\\textbf{np.power(abs\\_title\\_sentiment\\_polarity, 3)} &    1.193e-06  &     1.14e-06     &     1.047  &         0.295        &    -1.04e-06    &     3.43e-06     \\\\\n",
       "\\textbf{data\\_channel}                                &     1.62e-05  &     1.88e-05     &     0.861  &         0.389        &    -2.07e-05    &     5.31e-05     \\\\\n",
       "\\textbf{np.power(data\\_channel, 3)}                   &       0.0015  &        0.001     &     1.447  &         0.148        &       -0.001    &        0.003     \\\\\n",
       "\\textbf{weekday\\_is}                                  &    3.652e-05  &     2.64e-05     &     1.384  &         0.167        &    -1.52e-05    &     8.83e-05     \\\\\n",
       "\\textbf{np.power(weekday\\_is, 3)}                     &       0.0018  &        0.002     &     1.133  &         0.257        &       -0.001    &        0.005     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 38084.337 & \\textbf{  Durbin-Watson:     } &      1.996    \\\\\n",
       "\\textbf{Prob(Omnibus):} &    0.000  & \\textbf{  Jarque-Bera (JB):  } & 15770209.863  \\\\\n",
       "\\textbf{Skew:}          &    9.114  & \\textbf{  Prob(JB):          } &       0.00    \\\\\n",
       "\\textbf{Kurtosis:}      &  122.845  & \\textbf{  Cond. No.          } &   2.33e+16    \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 2.33e+16. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 shares   R-squared:                       0.028\n",
       "Model:                            OLS   Adj. R-squared:                  0.027\n",
       "Method:                 Least Squares   F-statistic:                     43.33\n",
       "Date:                Fri, 11 Aug 2023   Prob (F-statistic):          2.03e-143\n",
       "Time:                        11:34:52   Log-Likelihood:            -2.6445e+05\n",
       "No. Observations:               25756   AIC:                         5.289e+05\n",
       "Df Residuals:                   25738   BIC:                         5.291e+05\n",
       "Df Model:                          17                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=============================================================================================================\n",
       "                                                coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------------------------------------\n",
       "Intercept                                     0.0012      0.000      3.390      0.001       0.001       0.002\n",
       "n_tokens_title                                0.0016      0.000      4.033      0.000       0.001       0.002\n",
       "np.power(n_tokens_title, 3)                   0.2202      0.065      3.387      0.001       0.093       0.348\n",
       "n_tokens_content                              0.0396      0.048      0.819      0.413      -0.055       0.134\n",
       "np.power(n_tokens_content, 3)              8.425e-09    7.1e-09      1.187      0.235   -5.48e-09    2.23e-08\n",
       "n_non_stop_unique_tokens                  -2.428e-06   6.84e-06     -0.355      0.723   -1.58e-05     1.1e-05\n",
       "np.power(n_non_stop_unique_tokens, 3)      9.781e-06   2.54e-05      0.385      0.700      -4e-05    5.95e-05\n",
       "num_hrefs                                     0.0005      0.001      0.716      0.474      -0.001       0.002\n",
       "np.power(num_hrefs, 3)                        0.0007      0.000      3.320      0.001       0.000       0.001\n",
       "num_self_hrefs                                0.0001      0.000      0.761      0.447      -0.000       0.000\n",
       "np.power(num_self_hrefs, 3)                  -0.0011      0.004     -0.294      0.769      -0.009       0.006\n",
       "num_imgs                                      0.0003      0.000      0.953      0.340      -0.000       0.001\n",
       "np.power(num_imgs, 3)                         0.0014      0.002      0.876      0.381      -0.002       0.005\n",
       "num_videos                                 9.834e-05   5.49e-05      1.792      0.073   -9.22e-06       0.000\n",
       "np.power(num_videos, 3)                   -5.349e-05      0.004     -0.012      0.990      -0.009       0.009\n",
       "average_token_length                       1.029e-05   3.42e-06      3.011      0.003    3.59e-06     1.7e-05\n",
       "np.power(average_token_length, 3)            -0.0002      0.000     -1.596      0.110      -0.001    5.39e-05\n",
       "num_keywords                                  0.0002      0.000      0.982      0.326      -0.000       0.001\n",
       "np.power(num_keywords, 3)                     0.0398      0.045      0.885      0.376      -0.048       0.128\n",
       "kw_min_min                                -2.313e-05   1.49e-05     -1.550      0.121   -5.24e-05    6.11e-06\n",
       "np.power(kw_min_min, 3)                       0.0001    1.5e-05      8.806      0.000       0.000       0.000\n",
       "kw_max_min                                   -0.1456      0.019     -7.493      0.000      -0.184      -0.107\n",
       "np.power(kw_max_min, 3)                    1.702e-11   1.73e-12      9.839      0.000    1.36e-11    2.04e-11\n",
       "kw_min_max                                   -0.0122      0.002     -5.875      0.000      -0.016      -0.008\n",
       "np.power(kw_min_max, 3)                    8.852e-15   3.88e-15      2.283      0.022    1.25e-15    1.65e-14\n",
       "kw_max_max                                    0.0023      0.001      4.097      0.000       0.001       0.003\n",
       "np.power(kw_max_max, 3)                   -1.031e-15   7.26e-16     -1.420      0.156   -2.45e-15    3.92e-16\n",
       "kw_avg_max                                   -0.0010      0.001     -0.912      0.362      -0.003       0.001\n",
       "np.power(kw_avg_max, 3)                    7.543e-15   2.72e-15      2.773      0.006    2.21e-15    1.29e-14\n",
       "kw_min_avg                                    0.0199      0.031      0.632      0.527      -0.042       0.081\n",
       "np.power(kw_min_avg, 3)                    5.688e-08   6.24e-09      9.113      0.000    4.46e-08    6.91e-08\n",
       "kw_max_avg                                    0.2142      0.014     14.974      0.000       0.186       0.242\n",
       "np.power(kw_max_avg, 3)                   -1.735e-11   1.72e-12    -10.109      0.000   -2.07e-11    -1.4e-11\n",
       "self_reference_avg_sharess                    0.0346      0.003     11.979      0.000       0.029       0.040\n",
       "np.power(self_reference_avg_sharess, 3)   -8.732e-14   9.39e-15     -9.302      0.000   -1.06e-13   -6.89e-14\n",
       "LDA_01                                     2.649e-06   5.36e-06      0.494      0.621   -7.85e-06    1.31e-05\n",
       "np.power(LDA_01, 3)                        9.926e-07   3.52e-06      0.282      0.778    -5.9e-06    7.89e-06\n",
       "global_subjectivity                        3.162e-06   1.18e-06      2.682      0.007    8.51e-07    5.47e-06\n",
       "np.power(global_subjectivity, 3)           8.599e-07   2.13e-07      4.046      0.000    4.43e-07    1.28e-06\n",
       "global_rate_positive_words                 3.118e-07   3.59e-07      0.869      0.385   -3.92e-07    1.02e-06\n",
       "np.power(global_rate_positive_words, 3)    3.308e-10   1.17e-09      0.284      0.777   -1.96e-09    2.62e-09\n",
       "avg_positive_polarity                      2.636e-06    1.4e-06      1.878      0.060   -1.15e-07    5.39e-06\n",
       "np.power(avg_positive_polarity, 3)         3.624e-07   1.53e-07      2.373      0.018     6.3e-08    6.62e-07\n",
       "min_positive_polarity                     -1.532e-06   2.66e-06     -0.577      0.564   -6.74e-06    3.68e-06\n",
       "np.power(min_positive_polarity, 3)        -2.812e-07   2.67e-07     -1.054      0.292   -8.04e-07    2.42e-07\n",
       "max_positive_polarity                      1.387e-05   1.26e-05      1.105      0.269   -1.07e-05    3.85e-05\n",
       "np.power(max_positive_polarity, 3)         1.981e-05   2.08e-05      0.953      0.341   -2.09e-05    6.05e-05\n",
       "min_negative_polarity                     -1.635e-05   1.35e-05     -1.216      0.224   -4.27e-05       1e-05\n",
       "np.power(min_negative_polarity, 3)        -1.592e-05   1.28e-05     -1.246      0.213    -4.1e-05    9.13e-06\n",
       "max_negative_polarity                      1.568e-06   3.01e-06      0.520      0.603   -4.34e-06    7.47e-06\n",
       "np.power(max_negative_polarity, 3)         3.266e-07   6.04e-07      0.541      0.589   -8.58e-07    1.51e-06\n",
       "title_subjectivity                         1.001e-05   1.94e-06      5.168      0.000    6.21e-06    1.38e-05\n",
       "np.power(title_subjectivity, 3)             2.26e-06   9.87e-07      2.290      0.022    3.25e-07    4.19e-06\n",
       "title_sentiment_polarity                   3.003e-06   1.98e-06      1.516      0.130    -8.8e-07    6.89e-06\n",
       "np.power(title_sentiment_polarity, 3)      9.209e-07   1.18e-06      0.783      0.434   -1.38e-06    3.23e-06\n",
       "abs_title_subjectivity                    -6.302e-06   2.21e-06     -2.851      0.004   -1.06e-05   -1.97e-06\n",
       "np.power(abs_title_subjectivity, 3)       -2.321e-06   7.34e-07     -3.162      0.002   -3.76e-06   -8.82e-07\n",
       "abs_title_sentiment_polarity               5.832e-06   1.65e-06      3.541      0.000     2.6e-06    9.06e-06\n",
       "np.power(abs_title_sentiment_polarity, 3)  1.193e-06   1.14e-06      1.047      0.295   -1.04e-06    3.43e-06\n",
       "data_channel                                1.62e-05   1.88e-05      0.861      0.389   -2.07e-05    5.31e-05\n",
       "np.power(data_channel, 3)                     0.0015      0.001      1.447      0.148      -0.001       0.003\n",
       "weekday_is                                 3.652e-05   2.64e-05      1.384      0.167   -1.52e-05    8.83e-05\n",
       "np.power(weekday_is, 3)                       0.0018      0.002      1.133      0.257      -0.001       0.005\n",
       "==============================================================================\n",
       "Omnibus:                    38084.337   Durbin-Watson:                   1.996\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):         15770209.863\n",
       "Skew:                           9.114   Prob(JB):                         0.00\n",
       "Kurtosis:                     122.845   Cond. No.                     2.33e+16\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 2.33e+16. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "degree = 3\n",
    "polynomial_regression_summary(train_df, predictors_wo_multicol, 'shares', degree).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "13f4d2b1-127d-4e87-99e9-227f789f239f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>shares</td>      <th>  R-squared:         </th>  <td>   0.005</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.005</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   18.33</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 11 Aug 2023</td> <th>  Prob (F-statistic):</th>  <td>1.62e-24</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:34:52</td>     <th>  Log-Likelihood:    </th> <td>-2.6475e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 25756</td>      <th>  AIC:               </th>  <td>5.295e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 25748</td>      <th>  BIC:               </th>  <td>5.296e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     7</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                      <td></td>                         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                                 <td> 1.901e-08</td> <td> 9.17e-10</td> <td>   20.719</td> <td> 0.000</td> <td> 1.72e-08</td> <td> 2.08e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n_tokens_title</th>                            <td> 7.749e-09</td> <td> 3.74e-10</td> <td>   20.703</td> <td> 0.000</td> <td> 7.02e-09</td> <td> 8.48e-09</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(n_tokens_title, 4)</th>               <td>-1.473e-10</td> <td> 7.11e-12</td> <td>  -20.719</td> <td> 0.000</td> <td>-1.61e-10</td> <td>-1.33e-10</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n_tokens_content</th>                          <td>-2.781e-11</td> <td> 1.34e-12</td> <td>  -20.697</td> <td> 0.000</td> <td>-3.04e-11</td> <td>-2.52e-11</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(n_tokens_content, 4)</th>             <td>  1.62e-12</td> <td> 9.01e-13</td> <td>    1.798</td> <td> 0.072</td> <td>-1.46e-13</td> <td> 3.39e-12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n_non_stop_unique_tokens</th>                  <td> 7.966e-14</td> <td> 5.68e-15</td> <td>   14.017</td> <td> 0.000</td> <td> 6.85e-14</td> <td> 9.08e-14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(n_non_stop_unique_tokens, 4)</th>     <td> 9.834e-09</td> <td> 3.95e-08</td> <td>    0.249</td> <td> 0.803</td> <td>-6.75e-08</td> <td> 8.72e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_hrefs</th>                                 <td> 6.227e-15</td> <td> 3.01e-16</td> <td>   20.710</td> <td> 0.000</td> <td> 5.64e-15</td> <td> 6.82e-15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_hrefs, 4)</th>                    <td> 4.828e-11</td> <td> 2.33e-12</td> <td>   20.716</td> <td> 0.000</td> <td> 4.37e-11</td> <td> 5.28e-11</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_self_hrefs</th>                            <td> 2.237e-15</td> <td> 1.08e-16</td> <td>   20.715</td> <td> 0.000</td> <td> 2.03e-15</td> <td> 2.45e-15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_self_hrefs, 4)</th>               <td> 2.005e-11</td> <td> 9.69e-13</td> <td>   20.696</td> <td> 0.000</td> <td> 1.82e-11</td> <td>  2.2e-11</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_imgs</th>                                  <td> 2.032e-15</td> <td> 9.83e-17</td> <td>   20.663</td> <td> 0.000</td> <td> 1.84e-15</td> <td> 2.22e-15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_imgs, 4)</th>                     <td> 1.058e-10</td> <td> 5.21e-12</td> <td>   20.323</td> <td> 0.000</td> <td> 9.56e-11</td> <td> 1.16e-10</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_videos</th>                                <td> 8.254e-16</td> <td> 3.99e-17</td> <td>   20.708</td> <td> 0.000</td> <td> 7.47e-16</td> <td> 9.04e-16</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_videos, 4)</th>                   <td> 3.371e-11</td> <td> 1.63e-12</td> <td>   20.708</td> <td> 0.000</td> <td> 3.05e-11</td> <td> 3.69e-11</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>average_token_length</th>                      <td> 3.198e-15</td> <td> 1.54e-16</td> <td>   20.708</td> <td> 0.000</td> <td>  2.9e-15</td> <td>  3.5e-15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(average_token_length, 4)</th>         <td> 3.296e-13</td> <td> 1.59e-14</td> <td>   20.708</td> <td> 0.000</td> <td> 2.98e-13</td> <td> 3.61e-13</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_keywords</th>                              <td> 4.894e-15</td> <td> 2.36e-16</td> <td>   20.708</td> <td> 0.000</td> <td> 4.43e-15</td> <td> 5.36e-15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_keywords, 4)</th>                 <td>  2.49e-12</td> <td>  1.2e-13</td> <td>   20.708</td> <td> 0.000</td> <td> 2.25e-12</td> <td> 2.73e-12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_min_min</th>                                <td> 1.156e-13</td> <td> 5.58e-15</td> <td>   20.719</td> <td> 0.000</td> <td> 1.05e-13</td> <td> 1.27e-13</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_min_min, 4)</th>                   <td> 1.172e-06</td> <td> 5.66e-08</td> <td>   20.720</td> <td> 0.000</td> <td> 1.06e-06</td> <td> 1.28e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_max_min</th>                                <td> 8.761e-13</td> <td> 4.23e-14</td> <td>   20.706</td> <td> 0.000</td> <td> 7.93e-13</td> <td> 9.59e-13</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_max_min, 4)</th>                   <td>-1.568e-17</td> <td> 1.06e-17</td> <td>   -1.479</td> <td> 0.139</td> <td>-3.65e-17</td> <td>  5.1e-18</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_min_max</th>                                <td>  5.54e-13</td> <td> 2.68e-14</td> <td>   20.698</td> <td> 0.000</td> <td> 5.02e-13</td> <td> 6.06e-13</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_min_max, 4)</th>                   <td>-1.665e-20</td> <td> 2.87e-21</td> <td>   -5.791</td> <td> 0.000</td> <td>-2.23e-20</td> <td> -1.1e-20</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_max_max</th>                                <td> 1.802e-10</td> <td> 8.71e-12</td> <td>   20.702</td> <td> 0.000</td> <td> 1.63e-10</td> <td> 1.97e-10</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_max_max, 4)</th>                   <td> 5.621e-21</td> <td> 1.17e-22</td> <td>   47.932</td> <td> 0.000</td> <td> 5.39e-21</td> <td> 5.85e-21</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_avg_max</th>                                <td>  5.15e-11</td> <td> 2.49e-12</td> <td>   20.704</td> <td> 0.000</td> <td> 4.66e-11</td> <td> 5.64e-11</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_avg_max, 4)</th>                   <td> 6.858e-21</td> <td> 2.03e-21</td> <td>    3.376</td> <td> 0.001</td> <td> 2.88e-21</td> <td> 1.08e-20</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_min_avg</th>                                <td> 3.349e-13</td> <td> 1.62e-14</td> <td>   20.715</td> <td> 0.000</td> <td> 3.03e-13</td> <td> 3.67e-13</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_min_avg, 4)</th>                   <td> 2.133e-11</td> <td> 1.49e-12</td> <td>   14.335</td> <td> 0.000</td> <td> 1.84e-11</td> <td> 2.42e-11</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_max_avg</th>                                <td> 2.882e-12</td> <td> 1.39e-13</td> <td>   20.705</td> <td> 0.000</td> <td> 2.61e-12</td> <td> 3.15e-12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_max_avg, 4)</th>                   <td> 1.744e-17</td> <td> 1.06e-17</td> <td>    1.651</td> <td> 0.099</td> <td>-3.26e-18</td> <td> 3.81e-17</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>self_reference_avg_sharess</th>                <td> 2.077e-12</td> <td>    1e-13</td> <td>   20.709</td> <td> 0.000</td> <td> 1.88e-12</td> <td> 2.27e-12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(self_reference_avg_sharess, 4)</th>   <td> 2.909e-21</td> <td> 9.07e-21</td> <td>    0.321</td> <td> 0.748</td> <td>-1.49e-20</td> <td> 2.07e-20</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LDA_01</th>                                    <td> 9.748e-17</td> <td> 4.71e-18</td> <td>   20.704</td> <td> 0.000</td> <td> 8.83e-17</td> <td> 1.07e-16</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(LDA_01, 4)</th>                       <td> 2.304e-17</td> <td> 1.11e-18</td> <td>   20.703</td> <td> 0.000</td> <td> 2.09e-17</td> <td> 2.52e-17</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>global_subjectivity</th>                       <td> 3.126e-16</td> <td> 1.51e-17</td> <td>   20.702</td> <td> 0.000</td> <td> 2.83e-16</td> <td> 3.42e-16</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(global_subjectivity, 4)</th>          <td> 3.577e-17</td> <td> 1.73e-18</td> <td>   20.702</td> <td> 0.000</td> <td> 3.24e-17</td> <td> 3.92e-17</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>global_rate_positive_words</th>                <td> 3.074e-17</td> <td> 1.48e-18</td> <td>   20.703</td> <td> 0.000</td> <td> 2.78e-17</td> <td> 3.36e-17</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(global_rate_positive_words, 4)</th>   <td> 5.631e-21</td> <td> 2.72e-22</td> <td>   20.703</td> <td> 0.000</td> <td>  5.1e-21</td> <td> 6.16e-21</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>avg_positive_polarity</th>                     <td> 2.508e-16</td> <td> 1.21e-17</td> <td>   20.703</td> <td> 0.000</td> <td> 2.27e-16</td> <td> 2.75e-16</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(avg_positive_polarity, 4)</th>        <td> 1.671e-17</td> <td> 8.07e-19</td> <td>   20.703</td> <td> 0.000</td> <td> 1.51e-17</td> <td> 1.83e-17</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>min_positive_polarity</th>                     <td> 6.854e-17</td> <td> 3.31e-18</td> <td>   20.702</td> <td> 0.000</td> <td>  6.2e-17</td> <td>  7.5e-17</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(min_positive_polarity, 4)</th>        <td> 7.629e-19</td> <td> 3.69e-20</td> <td>   20.698</td> <td> 0.000</td> <td> 6.91e-19</td> <td> 8.35e-19</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>max_positive_polarity</th>                     <td> 5.286e-16</td> <td> 2.55e-17</td> <td>   20.703</td> <td> 0.000</td> <td> 4.79e-16</td> <td> 5.79e-16</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(max_positive_polarity, 4)</th>        <td> 3.496e-16</td> <td> 1.69e-17</td> <td>   20.703</td> <td> 0.000</td> <td> 3.16e-16</td> <td> 3.83e-16</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>min_negative_polarity</th>                     <td>-3.243e-16</td> <td> 1.57e-17</td> <td>  -20.703</td> <td> 0.000</td> <td>-3.55e-16</td> <td>-2.94e-16</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(min_negative_polarity, 4)</th>        <td> 1.164e-16</td> <td> 5.62e-18</td> <td>   20.703</td> <td> 0.000</td> <td> 1.05e-16</td> <td> 1.27e-16</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>max_negative_polarity</th>                     <td>-7.909e-17</td> <td> 3.82e-18</td> <td>  -20.702</td> <td> 0.000</td> <td>-8.66e-17</td> <td>-7.16e-17</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(max_negative_polarity, 4)</th>        <td>  2.89e-18</td> <td>  1.4e-19</td> <td>   20.704</td> <td> 0.000</td> <td> 2.62e-18</td> <td> 3.16e-18</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>title_subjectivity</th>                        <td>   1.8e-16</td> <td> 8.69e-18</td> <td>   20.704</td> <td> 0.000</td> <td> 1.63e-16</td> <td> 1.97e-16</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(title_subjectivity, 4)</th>           <td> 7.236e-17</td> <td>  3.5e-18</td> <td>   20.702</td> <td> 0.000</td> <td> 6.55e-17</td> <td> 7.92e-17</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>title_sentiment_polarity</th>                  <td> 5.993e-17</td> <td> 2.89e-18</td> <td>   20.708</td> <td> 0.000</td> <td> 5.43e-17</td> <td> 6.56e-17</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(title_sentiment_polarity, 4)</th>     <td> 2.439e-17</td> <td> 1.18e-18</td> <td>   20.706</td> <td> 0.000</td> <td> 2.21e-17</td> <td> 2.67e-17</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>abs_title_subjectivity</th>                    <td> 2.423e-16</td> <td> 1.17e-17</td> <td>   20.702</td> <td> 0.000</td> <td> 2.19e-16</td> <td> 2.65e-16</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(abs_title_subjectivity, 4)</th>       <td>  2.52e-17</td> <td> 1.22e-18</td> <td>   20.701</td> <td> 0.000</td> <td> 2.28e-17</td> <td> 2.76e-17</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>abs_title_sentiment_polarity</th>              <td> 1.027e-16</td> <td> 4.96e-18</td> <td>   20.704</td> <td> 0.000</td> <td>  9.3e-17</td> <td> 1.12e-16</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(abs_title_sentiment_polarity, 4)</th> <td> 2.439e-17</td> <td> 1.18e-18</td> <td>   20.706</td> <td> 0.000</td> <td> 2.21e-17</td> <td> 2.67e-17</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>data_channel</th>                              <td> 2.915e-15</td> <td> 1.41e-16</td> <td>   20.705</td> <td> 0.000</td> <td> 2.64e-15</td> <td> 3.19e-15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(data_channel, 4)</th>                 <td> 4.848e-13</td> <td> 2.34e-14</td> <td>   20.704</td> <td> 0.000</td> <td> 4.39e-13</td> <td> 5.31e-13</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weekday_is</th>                                <td> 2.287e-15</td> <td>  1.1e-16</td> <td>   20.706</td> <td> 0.000</td> <td> 2.07e-15</td> <td>  2.5e-15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(weekday_is, 4)</th>                   <td> 2.584e-13</td> <td> 1.25e-14</td> <td>   20.704</td> <td> 0.000</td> <td> 2.34e-13</td> <td> 2.83e-13</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>38016.042</td> <th>  Durbin-Watson:     </th>   <td>   1.992</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>15440346.660</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 9.090</td>   <th>  Prob(JB):          </th>   <td>    0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>121.563</td>  <th>  Cond. No.          </th>   <td>5.02e+16</td>  \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 5.02e+16. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                               &      shares      & \\textbf{  R-squared:         } &      0.005    \\\\\n",
       "\\textbf{Model:}                                       &       OLS        & \\textbf{  Adj. R-squared:    } &      0.005    \\\\\n",
       "\\textbf{Method:}                                      &  Least Squares   & \\textbf{  F-statistic:       } &      18.33    \\\\\n",
       "\\textbf{Date:}                                        & Fri, 11 Aug 2023 & \\textbf{  Prob (F-statistic):} &   1.62e-24    \\\\\n",
       "\\textbf{Time:}                                        &     11:34:52     & \\textbf{  Log-Likelihood:    } & -2.6475e+05   \\\\\n",
       "\\textbf{No. Observations:}                            &       25756      & \\textbf{  AIC:               } &  5.295e+05    \\\\\n",
       "\\textbf{Df Residuals:}                                &       25748      & \\textbf{  BIC:               } &  5.296e+05    \\\\\n",
       "\\textbf{Df Model:}                                    &           7      & \\textbf{                     } &               \\\\\n",
       "\\textbf{Covariance Type:}                             &    nonrobust     & \\textbf{                     } &               \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                                      & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                                    &    1.901e-08  &     9.17e-10     &    20.719  &         0.000        &     1.72e-08    &     2.08e-08     \\\\\n",
       "\\textbf{n\\_tokens\\_title}                             &    7.749e-09  &     3.74e-10     &    20.703  &         0.000        &     7.02e-09    &     8.48e-09     \\\\\n",
       "\\textbf{np.power(n\\_tokens\\_title, 4)}                &   -1.473e-10  &     7.11e-12     &   -20.719  &         0.000        &    -1.61e-10    &    -1.33e-10     \\\\\n",
       "\\textbf{n\\_tokens\\_content}                           &   -2.781e-11  &     1.34e-12     &   -20.697  &         0.000        &    -3.04e-11    &    -2.52e-11     \\\\\n",
       "\\textbf{np.power(n\\_tokens\\_content, 4)}              &     1.62e-12  &     9.01e-13     &     1.798  &         0.072        &    -1.46e-13    &     3.39e-12     \\\\\n",
       "\\textbf{n\\_non\\_stop\\_unique\\_tokens}                 &    7.966e-14  &     5.68e-15     &    14.017  &         0.000        &     6.85e-14    &     9.08e-14     \\\\\n",
       "\\textbf{np.power(n\\_non\\_stop\\_unique\\_tokens, 4)}    &    9.834e-09  &     3.95e-08     &     0.249  &         0.803        &    -6.75e-08    &     8.72e-08     \\\\\n",
       "\\textbf{num\\_hrefs}                                   &    6.227e-15  &     3.01e-16     &    20.710  &         0.000        &     5.64e-15    &     6.82e-15     \\\\\n",
       "\\textbf{np.power(num\\_hrefs, 4)}                      &    4.828e-11  &     2.33e-12     &    20.716  &         0.000        &     4.37e-11    &     5.28e-11     \\\\\n",
       "\\textbf{num\\_self\\_hrefs}                             &    2.237e-15  &     1.08e-16     &    20.715  &         0.000        &     2.03e-15    &     2.45e-15     \\\\\n",
       "\\textbf{np.power(num\\_self\\_hrefs, 4)}                &    2.005e-11  &     9.69e-13     &    20.696  &         0.000        &     1.82e-11    &      2.2e-11     \\\\\n",
       "\\textbf{num\\_imgs}                                    &    2.032e-15  &     9.83e-17     &    20.663  &         0.000        &     1.84e-15    &     2.22e-15     \\\\\n",
       "\\textbf{np.power(num\\_imgs, 4)}                       &    1.058e-10  &     5.21e-12     &    20.323  &         0.000        &     9.56e-11    &     1.16e-10     \\\\\n",
       "\\textbf{num\\_videos}                                  &    8.254e-16  &     3.99e-17     &    20.708  &         0.000        &     7.47e-16    &     9.04e-16     \\\\\n",
       "\\textbf{np.power(num\\_videos, 4)}                     &    3.371e-11  &     1.63e-12     &    20.708  &         0.000        &     3.05e-11    &     3.69e-11     \\\\\n",
       "\\textbf{average\\_token\\_length}                       &    3.198e-15  &     1.54e-16     &    20.708  &         0.000        &      2.9e-15    &      3.5e-15     \\\\\n",
       "\\textbf{np.power(average\\_token\\_length, 4)}          &    3.296e-13  &     1.59e-14     &    20.708  &         0.000        &     2.98e-13    &     3.61e-13     \\\\\n",
       "\\textbf{num\\_keywords}                                &    4.894e-15  &     2.36e-16     &    20.708  &         0.000        &     4.43e-15    &     5.36e-15     \\\\\n",
       "\\textbf{np.power(num\\_keywords, 4)}                   &     2.49e-12  &      1.2e-13     &    20.708  &         0.000        &     2.25e-12    &     2.73e-12     \\\\\n",
       "\\textbf{kw\\_min\\_min}                                 &    1.156e-13  &     5.58e-15     &    20.719  &         0.000        &     1.05e-13    &     1.27e-13     \\\\\n",
       "\\textbf{np.power(kw\\_min\\_min, 4)}                    &    1.172e-06  &     5.66e-08     &    20.720  &         0.000        &     1.06e-06    &     1.28e-06     \\\\\n",
       "\\textbf{kw\\_max\\_min}                                 &    8.761e-13  &     4.23e-14     &    20.706  &         0.000        &     7.93e-13    &     9.59e-13     \\\\\n",
       "\\textbf{np.power(kw\\_max\\_min, 4)}                    &   -1.568e-17  &     1.06e-17     &    -1.479  &         0.139        &    -3.65e-17    &      5.1e-18     \\\\\n",
       "\\textbf{kw\\_min\\_max}                                 &     5.54e-13  &     2.68e-14     &    20.698  &         0.000        &     5.02e-13    &     6.06e-13     \\\\\n",
       "\\textbf{np.power(kw\\_min\\_max, 4)}                    &   -1.665e-20  &     2.87e-21     &    -5.791  &         0.000        &    -2.23e-20    &     -1.1e-20     \\\\\n",
       "\\textbf{kw\\_max\\_max}                                 &    1.802e-10  &     8.71e-12     &    20.702  &         0.000        &     1.63e-10    &     1.97e-10     \\\\\n",
       "\\textbf{np.power(kw\\_max\\_max, 4)}                    &    5.621e-21  &     1.17e-22     &    47.932  &         0.000        &     5.39e-21    &     5.85e-21     \\\\\n",
       "\\textbf{kw\\_avg\\_max}                                 &     5.15e-11  &     2.49e-12     &    20.704  &         0.000        &     4.66e-11    &     5.64e-11     \\\\\n",
       "\\textbf{np.power(kw\\_avg\\_max, 4)}                    &    6.858e-21  &     2.03e-21     &     3.376  &         0.001        &     2.88e-21    &     1.08e-20     \\\\\n",
       "\\textbf{kw\\_min\\_avg}                                 &    3.349e-13  &     1.62e-14     &    20.715  &         0.000        &     3.03e-13    &     3.67e-13     \\\\\n",
       "\\textbf{np.power(kw\\_min\\_avg, 4)}                    &    2.133e-11  &     1.49e-12     &    14.335  &         0.000        &     1.84e-11    &     2.42e-11     \\\\\n",
       "\\textbf{kw\\_max\\_avg}                                 &    2.882e-12  &     1.39e-13     &    20.705  &         0.000        &     2.61e-12    &     3.15e-12     \\\\\n",
       "\\textbf{np.power(kw\\_max\\_avg, 4)}                    &    1.744e-17  &     1.06e-17     &     1.651  &         0.099        &    -3.26e-18    &     3.81e-17     \\\\\n",
       "\\textbf{self\\_reference\\_avg\\_sharess}                &    2.077e-12  &        1e-13     &    20.709  &         0.000        &     1.88e-12    &     2.27e-12     \\\\\n",
       "\\textbf{np.power(self\\_reference\\_avg\\_sharess, 4)}   &    2.909e-21  &     9.07e-21     &     0.321  &         0.748        &    -1.49e-20    &     2.07e-20     \\\\\n",
       "\\textbf{LDA\\_01}                                      &    9.748e-17  &     4.71e-18     &    20.704  &         0.000        &     8.83e-17    &     1.07e-16     \\\\\n",
       "\\textbf{np.power(LDA\\_01, 4)}                         &    2.304e-17  &     1.11e-18     &    20.703  &         0.000        &     2.09e-17    &     2.52e-17     \\\\\n",
       "\\textbf{global\\_subjectivity}                         &    3.126e-16  &     1.51e-17     &    20.702  &         0.000        &     2.83e-16    &     3.42e-16     \\\\\n",
       "\\textbf{np.power(global\\_subjectivity, 4)}            &    3.577e-17  &     1.73e-18     &    20.702  &         0.000        &     3.24e-17    &     3.92e-17     \\\\\n",
       "\\textbf{global\\_rate\\_positive\\_words}                &    3.074e-17  &     1.48e-18     &    20.703  &         0.000        &     2.78e-17    &     3.36e-17     \\\\\n",
       "\\textbf{np.power(global\\_rate\\_positive\\_words, 4)}   &    5.631e-21  &     2.72e-22     &    20.703  &         0.000        &      5.1e-21    &     6.16e-21     \\\\\n",
       "\\textbf{avg\\_positive\\_polarity}                      &    2.508e-16  &     1.21e-17     &    20.703  &         0.000        &     2.27e-16    &     2.75e-16     \\\\\n",
       "\\textbf{np.power(avg\\_positive\\_polarity, 4)}         &    1.671e-17  &     8.07e-19     &    20.703  &         0.000        &     1.51e-17    &     1.83e-17     \\\\\n",
       "\\textbf{min\\_positive\\_polarity}                      &    6.854e-17  &     3.31e-18     &    20.702  &         0.000        &      6.2e-17    &      7.5e-17     \\\\\n",
       "\\textbf{np.power(min\\_positive\\_polarity, 4)}         &    7.629e-19  &     3.69e-20     &    20.698  &         0.000        &     6.91e-19    &     8.35e-19     \\\\\n",
       "\\textbf{max\\_positive\\_polarity}                      &    5.286e-16  &     2.55e-17     &    20.703  &         0.000        &     4.79e-16    &     5.79e-16     \\\\\n",
       "\\textbf{np.power(max\\_positive\\_polarity, 4)}         &    3.496e-16  &     1.69e-17     &    20.703  &         0.000        &     3.16e-16    &     3.83e-16     \\\\\n",
       "\\textbf{min\\_negative\\_polarity}                      &   -3.243e-16  &     1.57e-17     &   -20.703  &         0.000        &    -3.55e-16    &    -2.94e-16     \\\\\n",
       "\\textbf{np.power(min\\_negative\\_polarity, 4)}         &    1.164e-16  &     5.62e-18     &    20.703  &         0.000        &     1.05e-16    &     1.27e-16     \\\\\n",
       "\\textbf{max\\_negative\\_polarity}                      &   -7.909e-17  &     3.82e-18     &   -20.702  &         0.000        &    -8.66e-17    &    -7.16e-17     \\\\\n",
       "\\textbf{np.power(max\\_negative\\_polarity, 4)}         &     2.89e-18  &      1.4e-19     &    20.704  &         0.000        &     2.62e-18    &     3.16e-18     \\\\\n",
       "\\textbf{title\\_subjectivity}                          &      1.8e-16  &     8.69e-18     &    20.704  &         0.000        &     1.63e-16    &     1.97e-16     \\\\\n",
       "\\textbf{np.power(title\\_subjectivity, 4)}             &    7.236e-17  &      3.5e-18     &    20.702  &         0.000        &     6.55e-17    &     7.92e-17     \\\\\n",
       "\\textbf{title\\_sentiment\\_polarity}                   &    5.993e-17  &     2.89e-18     &    20.708  &         0.000        &     5.43e-17    &     6.56e-17     \\\\\n",
       "\\textbf{np.power(title\\_sentiment\\_polarity, 4)}      &    2.439e-17  &     1.18e-18     &    20.706  &         0.000        &     2.21e-17    &     2.67e-17     \\\\\n",
       "\\textbf{abs\\_title\\_subjectivity}                     &    2.423e-16  &     1.17e-17     &    20.702  &         0.000        &     2.19e-16    &     2.65e-16     \\\\\n",
       "\\textbf{np.power(abs\\_title\\_subjectivity, 4)}        &     2.52e-17  &     1.22e-18     &    20.701  &         0.000        &     2.28e-17    &     2.76e-17     \\\\\n",
       "\\textbf{abs\\_title\\_sentiment\\_polarity}              &    1.027e-16  &     4.96e-18     &    20.704  &         0.000        &      9.3e-17    &     1.12e-16     \\\\\n",
       "\\textbf{np.power(abs\\_title\\_sentiment\\_polarity, 4)} &    2.439e-17  &     1.18e-18     &    20.706  &         0.000        &     2.21e-17    &     2.67e-17     \\\\\n",
       "\\textbf{data\\_channel}                                &    2.915e-15  &     1.41e-16     &    20.705  &         0.000        &     2.64e-15    &     3.19e-15     \\\\\n",
       "\\textbf{np.power(data\\_channel, 4)}                   &    4.848e-13  &     2.34e-14     &    20.704  &         0.000        &     4.39e-13    &     5.31e-13     \\\\\n",
       "\\textbf{weekday\\_is}                                  &    2.287e-15  &      1.1e-16     &    20.706  &         0.000        &     2.07e-15    &      2.5e-15     \\\\\n",
       "\\textbf{np.power(weekday\\_is, 4)}                     &    2.584e-13  &     1.25e-14     &    20.704  &         0.000        &     2.34e-13    &     2.83e-13     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 38016.042 & \\textbf{  Durbin-Watson:     } &      1.992    \\\\\n",
       "\\textbf{Prob(Omnibus):} &    0.000  & \\textbf{  Jarque-Bera (JB):  } & 15440346.660  \\\\\n",
       "\\textbf{Skew:}          &    9.090  & \\textbf{  Prob(JB):          } &       0.00    \\\\\n",
       "\\textbf{Kurtosis:}      &  121.563  & \\textbf{  Cond. No.          } &   5.02e+16    \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 5.02e+16. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 shares   R-squared:                       0.005\n",
       "Model:                            OLS   Adj. R-squared:                  0.005\n",
       "Method:                 Least Squares   F-statistic:                     18.33\n",
       "Date:                Fri, 11 Aug 2023   Prob (F-statistic):           1.62e-24\n",
       "Time:                        11:34:52   Log-Likelihood:            -2.6475e+05\n",
       "No. Observations:               25756   AIC:                         5.295e+05\n",
       "Df Residuals:                   25748   BIC:                         5.296e+05\n",
       "Df Model:                           7                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=============================================================================================================\n",
       "                                                coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------------------------------------\n",
       "Intercept                                  1.901e-08   9.17e-10     20.719      0.000    1.72e-08    2.08e-08\n",
       "n_tokens_title                             7.749e-09   3.74e-10     20.703      0.000    7.02e-09    8.48e-09\n",
       "np.power(n_tokens_title, 4)               -1.473e-10   7.11e-12    -20.719      0.000   -1.61e-10   -1.33e-10\n",
       "n_tokens_content                          -2.781e-11   1.34e-12    -20.697      0.000   -3.04e-11   -2.52e-11\n",
       "np.power(n_tokens_content, 4)               1.62e-12   9.01e-13      1.798      0.072   -1.46e-13    3.39e-12\n",
       "n_non_stop_unique_tokens                   7.966e-14   5.68e-15     14.017      0.000    6.85e-14    9.08e-14\n",
       "np.power(n_non_stop_unique_tokens, 4)      9.834e-09   3.95e-08      0.249      0.803   -6.75e-08    8.72e-08\n",
       "num_hrefs                                  6.227e-15   3.01e-16     20.710      0.000    5.64e-15    6.82e-15\n",
       "np.power(num_hrefs, 4)                     4.828e-11   2.33e-12     20.716      0.000    4.37e-11    5.28e-11\n",
       "num_self_hrefs                             2.237e-15   1.08e-16     20.715      0.000    2.03e-15    2.45e-15\n",
       "np.power(num_self_hrefs, 4)                2.005e-11   9.69e-13     20.696      0.000    1.82e-11     2.2e-11\n",
       "num_imgs                                   2.032e-15   9.83e-17     20.663      0.000    1.84e-15    2.22e-15\n",
       "np.power(num_imgs, 4)                      1.058e-10   5.21e-12     20.323      0.000    9.56e-11    1.16e-10\n",
       "num_videos                                 8.254e-16   3.99e-17     20.708      0.000    7.47e-16    9.04e-16\n",
       "np.power(num_videos, 4)                    3.371e-11   1.63e-12     20.708      0.000    3.05e-11    3.69e-11\n",
       "average_token_length                       3.198e-15   1.54e-16     20.708      0.000     2.9e-15     3.5e-15\n",
       "np.power(average_token_length, 4)          3.296e-13   1.59e-14     20.708      0.000    2.98e-13    3.61e-13\n",
       "num_keywords                               4.894e-15   2.36e-16     20.708      0.000    4.43e-15    5.36e-15\n",
       "np.power(num_keywords, 4)                   2.49e-12    1.2e-13     20.708      0.000    2.25e-12    2.73e-12\n",
       "kw_min_min                                 1.156e-13   5.58e-15     20.719      0.000    1.05e-13    1.27e-13\n",
       "np.power(kw_min_min, 4)                    1.172e-06   5.66e-08     20.720      0.000    1.06e-06    1.28e-06\n",
       "kw_max_min                                 8.761e-13   4.23e-14     20.706      0.000    7.93e-13    9.59e-13\n",
       "np.power(kw_max_min, 4)                   -1.568e-17   1.06e-17     -1.479      0.139   -3.65e-17     5.1e-18\n",
       "kw_min_max                                  5.54e-13   2.68e-14     20.698      0.000    5.02e-13    6.06e-13\n",
       "np.power(kw_min_max, 4)                   -1.665e-20   2.87e-21     -5.791      0.000   -2.23e-20    -1.1e-20\n",
       "kw_max_max                                 1.802e-10   8.71e-12     20.702      0.000    1.63e-10    1.97e-10\n",
       "np.power(kw_max_max, 4)                    5.621e-21   1.17e-22     47.932      0.000    5.39e-21    5.85e-21\n",
       "kw_avg_max                                  5.15e-11   2.49e-12     20.704      0.000    4.66e-11    5.64e-11\n",
       "np.power(kw_avg_max, 4)                    6.858e-21   2.03e-21      3.376      0.001    2.88e-21    1.08e-20\n",
       "kw_min_avg                                 3.349e-13   1.62e-14     20.715      0.000    3.03e-13    3.67e-13\n",
       "np.power(kw_min_avg, 4)                    2.133e-11   1.49e-12     14.335      0.000    1.84e-11    2.42e-11\n",
       "kw_max_avg                                 2.882e-12   1.39e-13     20.705      0.000    2.61e-12    3.15e-12\n",
       "np.power(kw_max_avg, 4)                    1.744e-17   1.06e-17      1.651      0.099   -3.26e-18    3.81e-17\n",
       "self_reference_avg_sharess                 2.077e-12      1e-13     20.709      0.000    1.88e-12    2.27e-12\n",
       "np.power(self_reference_avg_sharess, 4)    2.909e-21   9.07e-21      0.321      0.748   -1.49e-20    2.07e-20\n",
       "LDA_01                                     9.748e-17   4.71e-18     20.704      0.000    8.83e-17    1.07e-16\n",
       "np.power(LDA_01, 4)                        2.304e-17   1.11e-18     20.703      0.000    2.09e-17    2.52e-17\n",
       "global_subjectivity                        3.126e-16   1.51e-17     20.702      0.000    2.83e-16    3.42e-16\n",
       "np.power(global_subjectivity, 4)           3.577e-17   1.73e-18     20.702      0.000    3.24e-17    3.92e-17\n",
       "global_rate_positive_words                 3.074e-17   1.48e-18     20.703      0.000    2.78e-17    3.36e-17\n",
       "np.power(global_rate_positive_words, 4)    5.631e-21   2.72e-22     20.703      0.000     5.1e-21    6.16e-21\n",
       "avg_positive_polarity                      2.508e-16   1.21e-17     20.703      0.000    2.27e-16    2.75e-16\n",
       "np.power(avg_positive_polarity, 4)         1.671e-17   8.07e-19     20.703      0.000    1.51e-17    1.83e-17\n",
       "min_positive_polarity                      6.854e-17   3.31e-18     20.702      0.000     6.2e-17     7.5e-17\n",
       "np.power(min_positive_polarity, 4)         7.629e-19   3.69e-20     20.698      0.000    6.91e-19    8.35e-19\n",
       "max_positive_polarity                      5.286e-16   2.55e-17     20.703      0.000    4.79e-16    5.79e-16\n",
       "np.power(max_positive_polarity, 4)         3.496e-16   1.69e-17     20.703      0.000    3.16e-16    3.83e-16\n",
       "min_negative_polarity                     -3.243e-16   1.57e-17    -20.703      0.000   -3.55e-16   -2.94e-16\n",
       "np.power(min_negative_polarity, 4)         1.164e-16   5.62e-18     20.703      0.000    1.05e-16    1.27e-16\n",
       "max_negative_polarity                     -7.909e-17   3.82e-18    -20.702      0.000   -8.66e-17   -7.16e-17\n",
       "np.power(max_negative_polarity, 4)          2.89e-18    1.4e-19     20.704      0.000    2.62e-18    3.16e-18\n",
       "title_subjectivity                           1.8e-16   8.69e-18     20.704      0.000    1.63e-16    1.97e-16\n",
       "np.power(title_subjectivity, 4)            7.236e-17    3.5e-18     20.702      0.000    6.55e-17    7.92e-17\n",
       "title_sentiment_polarity                   5.993e-17   2.89e-18     20.708      0.000    5.43e-17    6.56e-17\n",
       "np.power(title_sentiment_polarity, 4)      2.439e-17   1.18e-18     20.706      0.000    2.21e-17    2.67e-17\n",
       "abs_title_subjectivity                     2.423e-16   1.17e-17     20.702      0.000    2.19e-16    2.65e-16\n",
       "np.power(abs_title_subjectivity, 4)         2.52e-17   1.22e-18     20.701      0.000    2.28e-17    2.76e-17\n",
       "abs_title_sentiment_polarity               1.027e-16   4.96e-18     20.704      0.000     9.3e-17    1.12e-16\n",
       "np.power(abs_title_sentiment_polarity, 4)  2.439e-17   1.18e-18     20.706      0.000    2.21e-17    2.67e-17\n",
       "data_channel                               2.915e-15   1.41e-16     20.705      0.000    2.64e-15    3.19e-15\n",
       "np.power(data_channel, 4)                  4.848e-13   2.34e-14     20.704      0.000    4.39e-13    5.31e-13\n",
       "weekday_is                                 2.287e-15    1.1e-16     20.706      0.000    2.07e-15     2.5e-15\n",
       "np.power(weekday_is, 4)                    2.584e-13   1.25e-14     20.704      0.000    2.34e-13    2.83e-13\n",
       "==============================================================================\n",
       "Omnibus:                    38016.042   Durbin-Watson:                   1.992\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):         15440346.660\n",
       "Skew:                           9.090   Prob(JB):                         0.00\n",
       "Kurtosis:                     121.563   Cond. No.                     5.02e+16\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 5.02e+16. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "degree = 4\n",
    "polynomial_regression_summary(train_df, predictors_wo_multicol, 'shares', degree).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eadf6348-283d-40ec-8444-ae5abe23e120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>shares</td>      <th>  R-squared:         </th>  <td>  -0.015</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>  -0.015</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>  -54.39</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 11 Aug 2023</td> <th>  Prob (F-statistic):</th>   <td>  1.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:34:52</td>     <th>  Log-Likelihood:    </th> <td>-2.6500e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 25756</td>      <th>  AIC:               </th>  <td>5.300e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 25748</td>      <th>  BIC:               </th>  <td>5.301e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     7</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                      <td></td>                         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                                 <td> 9.542e-20</td> <td> 5.96e-21</td> <td>   16.015</td> <td> 0.000</td> <td> 8.37e-20</td> <td> 1.07e-19</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n_tokens_title</th>                            <td>-4.058e-20</td> <td> 2.53e-21</td> <td>  -16.037</td> <td> 0.000</td> <td>-4.55e-20</td> <td>-3.56e-20</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(n_tokens_title, 5)</th>               <td> 8.651e-22</td> <td>  5.4e-23</td> <td>   16.020</td> <td> 0.000</td> <td> 7.59e-22</td> <td> 9.71e-22</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n_tokens_content</th>                          <td>-9.985e-21</td> <td> 6.22e-22</td> <td>  -16.058</td> <td> 0.000</td> <td>-1.12e-20</td> <td>-8.77e-21</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(n_tokens_content, 5)</th>             <td>  2.19e-16</td> <td> 1.21e-16</td> <td>    1.809</td> <td> 0.070</td> <td>-1.83e-17</td> <td> 4.56e-16</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n_non_stop_unique_tokens</th>                  <td>-1.192e-24</td> <td> 7.43e-26</td> <td>  -16.048</td> <td> 0.000</td> <td>-1.34e-24</td> <td>-1.05e-24</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(n_non_stop_unique_tokens, 5)</th>     <td> 1.299e-22</td> <td> 8.11e-24</td> <td>   16.013</td> <td> 0.000</td> <td> 1.14e-22</td> <td> 1.46e-22</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_hrefs</th>                                 <td>-1.614e-30</td> <td> 1.01e-31</td> <td>  -16.022</td> <td> 0.000</td> <td>-1.81e-30</td> <td>-1.42e-30</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_hrefs, 5)</th>                    <td>  2.65e-24</td> <td> 1.65e-25</td> <td>   16.049</td> <td> 0.000</td> <td> 2.33e-24</td> <td> 2.97e-24</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_self_hrefs</th>                            <td> 7.527e-33</td> <td>  4.9e-34</td> <td>   15.359</td> <td> 0.000</td> <td> 6.57e-33</td> <td> 8.49e-33</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_self_hrefs, 5)</th>               <td> 5.225e-26</td> <td> 3.26e-27</td> <td>   16.027</td> <td> 0.000</td> <td> 4.59e-26</td> <td> 5.86e-26</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_imgs</th>                                  <td> 7.788e-32</td> <td> 4.83e-33</td> <td>   16.134</td> <td> 0.000</td> <td> 6.84e-32</td> <td> 8.73e-32</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_imgs, 5)</th>                     <td> 2.354e-25</td> <td> 5.42e-26</td> <td>    4.345</td> <td> 0.000</td> <td> 1.29e-25</td> <td> 3.42e-25</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_videos</th>                                <td> 6.361e-33</td> <td> 3.97e-34</td> <td>   16.031</td> <td> 0.000</td> <td> 5.58e-33</td> <td> 7.14e-33</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_videos, 5)</th>                   <td> 3.787e-26</td> <td> 2.36e-27</td> <td>   16.068</td> <td> 0.000</td> <td> 3.32e-26</td> <td> 4.25e-26</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>average_token_length</th>                      <td> 1.279e-32</td> <td> 7.99e-34</td> <td>   16.010</td> <td> 0.000</td> <td> 1.12e-32</td> <td> 1.44e-32</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(average_token_length, 5)</th>         <td> 6.554e-30</td> <td>  4.1e-31</td> <td>   15.991</td> <td> 0.000</td> <td> 5.75e-30</td> <td> 7.36e-30</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_keywords</th>                              <td> 5.043e-33</td> <td> 3.14e-34</td> <td>   16.039</td> <td> 0.000</td> <td> 4.43e-33</td> <td> 5.66e-33</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_keywords, 5)</th>                 <td> -1.13e-28</td> <td> 7.07e-30</td> <td>  -15.990</td> <td> 0.000</td> <td>-1.27e-28</td> <td>-9.92e-29</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_min_min</th>                                <td> 2.997e-31</td> <td> 1.87e-32</td> <td>   16.018</td> <td> 0.000</td> <td> 2.63e-31</td> <td> 3.36e-31</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_min_min, 5)</th>                   <td> 6.621e-22</td> <td> 4.13e-23</td> <td>   16.018</td> <td> 0.000</td> <td> 5.81e-22</td> <td> 7.43e-22</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_max_min</th>                                <td> 1.755e-29</td> <td>  1.1e-30</td> <td>   16.010</td> <td> 0.000</td> <td> 1.54e-29</td> <td> 1.97e-29</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_max_min, 5)</th>                   <td>-7.914e-23</td> <td> 9.02e-23</td> <td>   -0.878</td> <td> 0.380</td> <td>-2.56e-22</td> <td> 9.76e-23</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_min_max</th>                                <td> 7.666e-28</td> <td> 4.79e-29</td> <td>   16.018</td> <td> 0.000</td> <td> 6.73e-28</td> <td>  8.6e-28</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_min_max, 5)</th>                   <td>-2.031e-26</td> <td> 4.32e-27</td> <td>   -4.702</td> <td> 0.000</td> <td>-2.88e-26</td> <td>-1.18e-26</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_max_max</th>                                <td> 1.294e-27</td> <td> 8.08e-29</td> <td>   16.014</td> <td> 0.000</td> <td> 1.14e-27</td> <td> 1.45e-27</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_max_max, 5)</th>                   <td> 6.886e-27</td> <td> 1.34e-28</td> <td>   51.540</td> <td> 0.000</td> <td> 6.62e-27</td> <td> 7.15e-27</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_avg_max</th>                                <td> 1.232e-27</td> <td> 7.69e-29</td> <td>   16.012</td> <td> 0.000</td> <td> 1.08e-27</td> <td> 1.38e-27</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_avg_max, 5)</th>                   <td> 6.972e-27</td> <td> 3.34e-27</td> <td>    2.086</td> <td> 0.037</td> <td> 4.21e-28</td> <td> 1.35e-26</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_min_avg</th>                                <td> 5.728e-29</td> <td> 3.58e-30</td> <td>   16.019</td> <td> 0.000</td> <td> 5.03e-29</td> <td> 6.43e-29</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_min_avg, 5)</th>                   <td> 7.269e-15</td> <td> 4.54e-16</td> <td>   16.018</td> <td> 0.000</td> <td> 6.38e-15</td> <td> 8.16e-15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_max_avg</th>                                <td> 6.758e-29</td> <td> 4.22e-30</td> <td>   16.016</td> <td> 0.000</td> <td> 5.93e-29</td> <td> 7.58e-29</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_max_avg, 5)</th>                   <td> 8.512e-23</td> <td> 9.01e-23</td> <td>    0.945</td> <td> 0.345</td> <td>-9.15e-23</td> <td> 2.62e-22</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>self_reference_avg_sharess</th>                <td>  9.75e-29</td> <td> 6.09e-30</td> <td>   16.013</td> <td> 0.000</td> <td> 8.56e-29</td> <td> 1.09e-28</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(self_reference_avg_sharess, 5)</th>   <td> 5.514e-27</td> <td> 1.39e-26</td> <td>    0.398</td> <td> 0.691</td> <td>-2.17e-26</td> <td> 3.27e-26</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LDA_01</th>                                    <td> 8.152e-34</td> <td> 5.09e-35</td> <td>   16.019</td> <td> 0.000</td> <td> 7.15e-34</td> <td> 9.15e-34</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(LDA_01, 5)</th>                       <td> 1.291e-34</td> <td> 8.04e-36</td> <td>   16.043</td> <td> 0.000</td> <td> 1.13e-34</td> <td> 1.45e-34</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>global_subjectivity</th>                       <td> 2.163e-33</td> <td> 1.35e-34</td> <td>   16.019</td> <td> 0.000</td> <td>  1.9e-33</td> <td> 2.43e-33</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(global_subjectivity, 5)</th>          <td> 4.125e-34</td> <td> 2.58e-35</td> <td>   16.017</td> <td> 0.000</td> <td> 3.62e-34</td> <td> 4.63e-34</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>global_rate_positive_words</th>                <td> 1.801e-34</td> <td> 1.12e-35</td> <td>   16.026</td> <td> 0.000</td> <td> 1.58e-34</td> <td> 2.02e-34</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(global_rate_positive_words, 5)</th>   <td> 4.796e-39</td> <td> 2.99e-40</td> <td>   16.014</td> <td> 0.000</td> <td> 4.21e-39</td> <td> 5.38e-39</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>avg_positive_polarity</th>                     <td> 1.632e-33</td> <td> 1.02e-34</td> <td>   16.023</td> <td> 0.000</td> <td> 1.43e-33</td> <td> 1.83e-33</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(avg_positive_polarity, 5)</th>        <td> 1.386e-34</td> <td> 8.65e-36</td> <td>   16.022</td> <td> 0.000</td> <td> 1.22e-34</td> <td> 1.56e-34</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>min_positive_polarity</th>                     <td> 5.277e-34</td> <td>  3.3e-35</td> <td>   15.985</td> <td> 0.000</td> <td> 4.63e-34</td> <td> 5.92e-34</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(min_positive_polarity, 5)</th>        <td> 1.204e-35</td> <td> 7.52e-37</td> <td>   15.998</td> <td> 0.000</td> <td> 1.06e-35</td> <td> 1.35e-35</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>max_positive_polarity</th>                     <td>  2.99e-33</td> <td> 1.86e-34</td> <td>   16.034</td> <td> 0.000</td> <td> 2.62e-33</td> <td> 3.36e-33</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(max_positive_polarity, 5)</th>        <td>  3.16e-33</td> <td> 1.97e-34</td> <td>   16.054</td> <td> 0.000</td> <td> 2.77e-33</td> <td> 3.55e-33</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>min_negative_polarity</th>                     <td>-1.959e-33</td> <td> 1.22e-34</td> <td>  -16.063</td> <td> 0.000</td> <td> -2.2e-33</td> <td>-1.72e-33</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(min_negative_polarity, 5)</th>        <td>-1.183e-33</td> <td> 7.34e-35</td> <td>  -16.117</td> <td> 0.000</td> <td>-1.33e-33</td> <td>-1.04e-33</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>max_negative_polarity</th>                     <td>-5.393e-34</td> <td> 3.37e-35</td> <td>  -15.984</td> <td> 0.000</td> <td>-6.05e-34</td> <td>-4.73e-34</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(max_negative_polarity, 5)</th>        <td>-6.331e-35</td> <td> 3.96e-36</td> <td>  -16.004</td> <td> 0.000</td> <td>-7.11e-35</td> <td>-5.56e-35</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>title_subjectivity</th>                        <td> 2.142e-33</td> <td> 1.34e-34</td> <td>   16.025</td> <td> 0.000</td> <td> 1.88e-33</td> <td>  2.4e-33</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(title_subjectivity, 5)</th>           <td> 1.366e-33</td> <td> 8.53e-35</td> <td>   16.022</td> <td> 0.000</td> <td>  1.2e-33</td> <td> 1.53e-33</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>title_sentiment_polarity</th>                  <td> 9.595e-34</td> <td> 5.98e-35</td> <td>   16.051</td> <td> 0.000</td> <td> 8.42e-34</td> <td> 1.08e-33</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(title_sentiment_polarity, 5)</th>     <td> 3.452e-34</td> <td> 2.15e-35</td> <td>   16.040</td> <td> 0.000</td> <td> 3.03e-34</td> <td> 3.87e-34</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>abs_title_subjectivity</th>                    <td> 8.025e-34</td> <td> 5.02e-35</td> <td>   16.000</td> <td> 0.000</td> <td> 7.04e-34</td> <td> 9.01e-34</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(abs_title_subjectivity, 5)</th>       <td> 3.642e-35</td> <td> 2.28e-36</td> <td>   15.996</td> <td> 0.000</td> <td>  3.2e-35</td> <td> 4.09e-35</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>abs_title_sentiment_polarity</th>              <td> 1.545e-33</td> <td> 9.64e-35</td> <td>   16.029</td> <td> 0.000</td> <td> 1.36e-33</td> <td> 1.73e-33</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(abs_title_sentiment_polarity, 5)</th> <td> 5.458e-34</td> <td> 3.41e-35</td> <td>   16.025</td> <td> 0.000</td> <td> 4.79e-34</td> <td> 6.13e-34</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>data_channel</th>                              <td> 1.526e-32</td> <td> 9.54e-34</td> <td>   15.993</td> <td> 0.000</td> <td> 1.34e-32</td> <td> 1.71e-32</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(data_channel, 5)</th>                 <td> 6.386e-29</td> <td> 3.99e-30</td> <td>   16.008</td> <td> 0.000</td> <td>  5.6e-29</td> <td> 7.17e-29</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weekday_is</th>                                <td> 1.487e-32</td> <td> 9.27e-34</td> <td>   16.029</td> <td> 0.000</td> <td>  1.3e-32</td> <td> 1.67e-32</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(weekday_is, 5)</th>                   <td> 2.306e-29</td> <td> 1.44e-30</td> <td>   16.048</td> <td> 0.000</td> <td> 2.02e-29</td> <td> 2.59e-29</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>37434.597</td> <th>  Durbin-Watson:     </th>   <td>   1.985</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>14285131.299</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 8.834</td>   <th>  Prob(JB):          </th>   <td>    0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>117.013</td>  <th>  Cond. No.          </th>   <td>2.86e+16</td>  \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 2.86e+16. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                               &      shares      & \\textbf{  R-squared:         } &     -0.015    \\\\\n",
       "\\textbf{Model:}                                       &       OLS        & \\textbf{  Adj. R-squared:    } &     -0.015    \\\\\n",
       "\\textbf{Method:}                                      &  Least Squares   & \\textbf{  F-statistic:       } &     -54.39    \\\\\n",
       "\\textbf{Date:}                                        & Fri, 11 Aug 2023 & \\textbf{  Prob (F-statistic):} &      1.00     \\\\\n",
       "\\textbf{Time:}                                        &     11:34:52     & \\textbf{  Log-Likelihood:    } & -2.6500e+05   \\\\\n",
       "\\textbf{No. Observations:}                            &       25756      & \\textbf{  AIC:               } &  5.300e+05    \\\\\n",
       "\\textbf{Df Residuals:}                                &       25748      & \\textbf{  BIC:               } &  5.301e+05    \\\\\n",
       "\\textbf{Df Model:}                                    &           7      & \\textbf{                     } &               \\\\\n",
       "\\textbf{Covariance Type:}                             &    nonrobust     & \\textbf{                     } &               \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                                      & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                                    &    9.542e-20  &     5.96e-21     &    16.015  &         0.000        &     8.37e-20    &     1.07e-19     \\\\\n",
       "\\textbf{n\\_tokens\\_title}                             &   -4.058e-20  &     2.53e-21     &   -16.037  &         0.000        &    -4.55e-20    &    -3.56e-20     \\\\\n",
       "\\textbf{np.power(n\\_tokens\\_title, 5)}                &    8.651e-22  &      5.4e-23     &    16.020  &         0.000        &     7.59e-22    &     9.71e-22     \\\\\n",
       "\\textbf{n\\_tokens\\_content}                           &   -9.985e-21  &     6.22e-22     &   -16.058  &         0.000        &    -1.12e-20    &    -8.77e-21     \\\\\n",
       "\\textbf{np.power(n\\_tokens\\_content, 5)}              &     2.19e-16  &     1.21e-16     &     1.809  &         0.070        &    -1.83e-17    &     4.56e-16     \\\\\n",
       "\\textbf{n\\_non\\_stop\\_unique\\_tokens}                 &   -1.192e-24  &     7.43e-26     &   -16.048  &         0.000        &    -1.34e-24    &    -1.05e-24     \\\\\n",
       "\\textbf{np.power(n\\_non\\_stop\\_unique\\_tokens, 5)}    &    1.299e-22  &     8.11e-24     &    16.013  &         0.000        &     1.14e-22    &     1.46e-22     \\\\\n",
       "\\textbf{num\\_hrefs}                                   &   -1.614e-30  &     1.01e-31     &   -16.022  &         0.000        &    -1.81e-30    &    -1.42e-30     \\\\\n",
       "\\textbf{np.power(num\\_hrefs, 5)}                      &     2.65e-24  &     1.65e-25     &    16.049  &         0.000        &     2.33e-24    &     2.97e-24     \\\\\n",
       "\\textbf{num\\_self\\_hrefs}                             &    7.527e-33  &      4.9e-34     &    15.359  &         0.000        &     6.57e-33    &     8.49e-33     \\\\\n",
       "\\textbf{np.power(num\\_self\\_hrefs, 5)}                &    5.225e-26  &     3.26e-27     &    16.027  &         0.000        &     4.59e-26    &     5.86e-26     \\\\\n",
       "\\textbf{num\\_imgs}                                    &    7.788e-32  &     4.83e-33     &    16.134  &         0.000        &     6.84e-32    &     8.73e-32     \\\\\n",
       "\\textbf{np.power(num\\_imgs, 5)}                       &    2.354e-25  &     5.42e-26     &     4.345  &         0.000        &     1.29e-25    &     3.42e-25     \\\\\n",
       "\\textbf{num\\_videos}                                  &    6.361e-33  &     3.97e-34     &    16.031  &         0.000        &     5.58e-33    &     7.14e-33     \\\\\n",
       "\\textbf{np.power(num\\_videos, 5)}                     &    3.787e-26  &     2.36e-27     &    16.068  &         0.000        &     3.32e-26    &     4.25e-26     \\\\\n",
       "\\textbf{average\\_token\\_length}                       &    1.279e-32  &     7.99e-34     &    16.010  &         0.000        &     1.12e-32    &     1.44e-32     \\\\\n",
       "\\textbf{np.power(average\\_token\\_length, 5)}          &    6.554e-30  &      4.1e-31     &    15.991  &         0.000        &     5.75e-30    &     7.36e-30     \\\\\n",
       "\\textbf{num\\_keywords}                                &    5.043e-33  &     3.14e-34     &    16.039  &         0.000        &     4.43e-33    &     5.66e-33     \\\\\n",
       "\\textbf{np.power(num\\_keywords, 5)}                   &    -1.13e-28  &     7.07e-30     &   -15.990  &         0.000        &    -1.27e-28    &    -9.92e-29     \\\\\n",
       "\\textbf{kw\\_min\\_min}                                 &    2.997e-31  &     1.87e-32     &    16.018  &         0.000        &     2.63e-31    &     3.36e-31     \\\\\n",
       "\\textbf{np.power(kw\\_min\\_min, 5)}                    &    6.621e-22  &     4.13e-23     &    16.018  &         0.000        &     5.81e-22    &     7.43e-22     \\\\\n",
       "\\textbf{kw\\_max\\_min}                                 &    1.755e-29  &      1.1e-30     &    16.010  &         0.000        &     1.54e-29    &     1.97e-29     \\\\\n",
       "\\textbf{np.power(kw\\_max\\_min, 5)}                    &   -7.914e-23  &     9.02e-23     &    -0.878  &         0.380        &    -2.56e-22    &     9.76e-23     \\\\\n",
       "\\textbf{kw\\_min\\_max}                                 &    7.666e-28  &     4.79e-29     &    16.018  &         0.000        &     6.73e-28    &      8.6e-28     \\\\\n",
       "\\textbf{np.power(kw\\_min\\_max, 5)}                    &   -2.031e-26  &     4.32e-27     &    -4.702  &         0.000        &    -2.88e-26    &    -1.18e-26     \\\\\n",
       "\\textbf{kw\\_max\\_max}                                 &    1.294e-27  &     8.08e-29     &    16.014  &         0.000        &     1.14e-27    &     1.45e-27     \\\\\n",
       "\\textbf{np.power(kw\\_max\\_max, 5)}                    &    6.886e-27  &     1.34e-28     &    51.540  &         0.000        &     6.62e-27    &     7.15e-27     \\\\\n",
       "\\textbf{kw\\_avg\\_max}                                 &    1.232e-27  &     7.69e-29     &    16.012  &         0.000        &     1.08e-27    &     1.38e-27     \\\\\n",
       "\\textbf{np.power(kw\\_avg\\_max, 5)}                    &    6.972e-27  &     3.34e-27     &     2.086  &         0.037        &     4.21e-28    &     1.35e-26     \\\\\n",
       "\\textbf{kw\\_min\\_avg}                                 &    5.728e-29  &     3.58e-30     &    16.019  &         0.000        &     5.03e-29    &     6.43e-29     \\\\\n",
       "\\textbf{np.power(kw\\_min\\_avg, 5)}                    &    7.269e-15  &     4.54e-16     &    16.018  &         0.000        &     6.38e-15    &     8.16e-15     \\\\\n",
       "\\textbf{kw\\_max\\_avg}                                 &    6.758e-29  &     4.22e-30     &    16.016  &         0.000        &     5.93e-29    &     7.58e-29     \\\\\n",
       "\\textbf{np.power(kw\\_max\\_avg, 5)}                    &    8.512e-23  &     9.01e-23     &     0.945  &         0.345        &    -9.15e-23    &     2.62e-22     \\\\\n",
       "\\textbf{self\\_reference\\_avg\\_sharess}                &     9.75e-29  &     6.09e-30     &    16.013  &         0.000        &     8.56e-29    &     1.09e-28     \\\\\n",
       "\\textbf{np.power(self\\_reference\\_avg\\_sharess, 5)}   &    5.514e-27  &     1.39e-26     &     0.398  &         0.691        &    -2.17e-26    &     3.27e-26     \\\\\n",
       "\\textbf{LDA\\_01}                                      &    8.152e-34  &     5.09e-35     &    16.019  &         0.000        &     7.15e-34    &     9.15e-34     \\\\\n",
       "\\textbf{np.power(LDA\\_01, 5)}                         &    1.291e-34  &     8.04e-36     &    16.043  &         0.000        &     1.13e-34    &     1.45e-34     \\\\\n",
       "\\textbf{global\\_subjectivity}                         &    2.163e-33  &     1.35e-34     &    16.019  &         0.000        &      1.9e-33    &     2.43e-33     \\\\\n",
       "\\textbf{np.power(global\\_subjectivity, 5)}            &    4.125e-34  &     2.58e-35     &    16.017  &         0.000        &     3.62e-34    &     4.63e-34     \\\\\n",
       "\\textbf{global\\_rate\\_positive\\_words}                &    1.801e-34  &     1.12e-35     &    16.026  &         0.000        &     1.58e-34    &     2.02e-34     \\\\\n",
       "\\textbf{np.power(global\\_rate\\_positive\\_words, 5)}   &    4.796e-39  &     2.99e-40     &    16.014  &         0.000        &     4.21e-39    &     5.38e-39     \\\\\n",
       "\\textbf{avg\\_positive\\_polarity}                      &    1.632e-33  &     1.02e-34     &    16.023  &         0.000        &     1.43e-33    &     1.83e-33     \\\\\n",
       "\\textbf{np.power(avg\\_positive\\_polarity, 5)}         &    1.386e-34  &     8.65e-36     &    16.022  &         0.000        &     1.22e-34    &     1.56e-34     \\\\\n",
       "\\textbf{min\\_positive\\_polarity}                      &    5.277e-34  &      3.3e-35     &    15.985  &         0.000        &     4.63e-34    &     5.92e-34     \\\\\n",
       "\\textbf{np.power(min\\_positive\\_polarity, 5)}         &    1.204e-35  &     7.52e-37     &    15.998  &         0.000        &     1.06e-35    &     1.35e-35     \\\\\n",
       "\\textbf{max\\_positive\\_polarity}                      &     2.99e-33  &     1.86e-34     &    16.034  &         0.000        &     2.62e-33    &     3.36e-33     \\\\\n",
       "\\textbf{np.power(max\\_positive\\_polarity, 5)}         &     3.16e-33  &     1.97e-34     &    16.054  &         0.000        &     2.77e-33    &     3.55e-33     \\\\\n",
       "\\textbf{min\\_negative\\_polarity}                      &   -1.959e-33  &     1.22e-34     &   -16.063  &         0.000        &     -2.2e-33    &    -1.72e-33     \\\\\n",
       "\\textbf{np.power(min\\_negative\\_polarity, 5)}         &   -1.183e-33  &     7.34e-35     &   -16.117  &         0.000        &    -1.33e-33    &    -1.04e-33     \\\\\n",
       "\\textbf{max\\_negative\\_polarity}                      &   -5.393e-34  &     3.37e-35     &   -15.984  &         0.000        &    -6.05e-34    &    -4.73e-34     \\\\\n",
       "\\textbf{np.power(max\\_negative\\_polarity, 5)}         &   -6.331e-35  &     3.96e-36     &   -16.004  &         0.000        &    -7.11e-35    &    -5.56e-35     \\\\\n",
       "\\textbf{title\\_subjectivity}                          &    2.142e-33  &     1.34e-34     &    16.025  &         0.000        &     1.88e-33    &      2.4e-33     \\\\\n",
       "\\textbf{np.power(title\\_subjectivity, 5)}             &    1.366e-33  &     8.53e-35     &    16.022  &         0.000        &      1.2e-33    &     1.53e-33     \\\\\n",
       "\\textbf{title\\_sentiment\\_polarity}                   &    9.595e-34  &     5.98e-35     &    16.051  &         0.000        &     8.42e-34    &     1.08e-33     \\\\\n",
       "\\textbf{np.power(title\\_sentiment\\_polarity, 5)}      &    3.452e-34  &     2.15e-35     &    16.040  &         0.000        &     3.03e-34    &     3.87e-34     \\\\\n",
       "\\textbf{abs\\_title\\_subjectivity}                     &    8.025e-34  &     5.02e-35     &    16.000  &         0.000        &     7.04e-34    &     9.01e-34     \\\\\n",
       "\\textbf{np.power(abs\\_title\\_subjectivity, 5)}        &    3.642e-35  &     2.28e-36     &    15.996  &         0.000        &      3.2e-35    &     4.09e-35     \\\\\n",
       "\\textbf{abs\\_title\\_sentiment\\_polarity}              &    1.545e-33  &     9.64e-35     &    16.029  &         0.000        &     1.36e-33    &     1.73e-33     \\\\\n",
       "\\textbf{np.power(abs\\_title\\_sentiment\\_polarity, 5)} &    5.458e-34  &     3.41e-35     &    16.025  &         0.000        &     4.79e-34    &     6.13e-34     \\\\\n",
       "\\textbf{data\\_channel}                                &    1.526e-32  &     9.54e-34     &    15.993  &         0.000        &     1.34e-32    &     1.71e-32     \\\\\n",
       "\\textbf{np.power(data\\_channel, 5)}                   &    6.386e-29  &     3.99e-30     &    16.008  &         0.000        &      5.6e-29    &     7.17e-29     \\\\\n",
       "\\textbf{weekday\\_is}                                  &    1.487e-32  &     9.27e-34     &    16.029  &         0.000        &      1.3e-32    &     1.67e-32     \\\\\n",
       "\\textbf{np.power(weekday\\_is, 5)}                     &    2.306e-29  &     1.44e-30     &    16.048  &         0.000        &     2.02e-29    &     2.59e-29     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 37434.597 & \\textbf{  Durbin-Watson:     } &      1.985    \\\\\n",
       "\\textbf{Prob(Omnibus):} &    0.000  & \\textbf{  Jarque-Bera (JB):  } & 14285131.299  \\\\\n",
       "\\textbf{Skew:}          &    8.834  & \\textbf{  Prob(JB):          } &       0.00    \\\\\n",
       "\\textbf{Kurtosis:}      &  117.013  & \\textbf{  Cond. No.          } &   2.86e+16    \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 2.86e+16. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 shares   R-squared:                      -0.015\n",
       "Model:                            OLS   Adj. R-squared:                 -0.015\n",
       "Method:                 Least Squares   F-statistic:                    -54.39\n",
       "Date:                Fri, 11 Aug 2023   Prob (F-statistic):               1.00\n",
       "Time:                        11:34:52   Log-Likelihood:            -2.6500e+05\n",
       "No. Observations:               25756   AIC:                         5.300e+05\n",
       "Df Residuals:                   25748   BIC:                         5.301e+05\n",
       "Df Model:                           7                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=============================================================================================================\n",
       "                                                coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------------------------------------\n",
       "Intercept                                  9.542e-20   5.96e-21     16.015      0.000    8.37e-20    1.07e-19\n",
       "n_tokens_title                            -4.058e-20   2.53e-21    -16.037      0.000   -4.55e-20   -3.56e-20\n",
       "np.power(n_tokens_title, 5)                8.651e-22    5.4e-23     16.020      0.000    7.59e-22    9.71e-22\n",
       "n_tokens_content                          -9.985e-21   6.22e-22    -16.058      0.000   -1.12e-20   -8.77e-21\n",
       "np.power(n_tokens_content, 5)               2.19e-16   1.21e-16      1.809      0.070   -1.83e-17    4.56e-16\n",
       "n_non_stop_unique_tokens                  -1.192e-24   7.43e-26    -16.048      0.000   -1.34e-24   -1.05e-24\n",
       "np.power(n_non_stop_unique_tokens, 5)      1.299e-22   8.11e-24     16.013      0.000    1.14e-22    1.46e-22\n",
       "num_hrefs                                 -1.614e-30   1.01e-31    -16.022      0.000   -1.81e-30   -1.42e-30\n",
       "np.power(num_hrefs, 5)                      2.65e-24   1.65e-25     16.049      0.000    2.33e-24    2.97e-24\n",
       "num_self_hrefs                             7.527e-33    4.9e-34     15.359      0.000    6.57e-33    8.49e-33\n",
       "np.power(num_self_hrefs, 5)                5.225e-26   3.26e-27     16.027      0.000    4.59e-26    5.86e-26\n",
       "num_imgs                                   7.788e-32   4.83e-33     16.134      0.000    6.84e-32    8.73e-32\n",
       "np.power(num_imgs, 5)                      2.354e-25   5.42e-26      4.345      0.000    1.29e-25    3.42e-25\n",
       "num_videos                                 6.361e-33   3.97e-34     16.031      0.000    5.58e-33    7.14e-33\n",
       "np.power(num_videos, 5)                    3.787e-26   2.36e-27     16.068      0.000    3.32e-26    4.25e-26\n",
       "average_token_length                       1.279e-32   7.99e-34     16.010      0.000    1.12e-32    1.44e-32\n",
       "np.power(average_token_length, 5)          6.554e-30    4.1e-31     15.991      0.000    5.75e-30    7.36e-30\n",
       "num_keywords                               5.043e-33   3.14e-34     16.039      0.000    4.43e-33    5.66e-33\n",
       "np.power(num_keywords, 5)                  -1.13e-28   7.07e-30    -15.990      0.000   -1.27e-28   -9.92e-29\n",
       "kw_min_min                                 2.997e-31   1.87e-32     16.018      0.000    2.63e-31    3.36e-31\n",
       "np.power(kw_min_min, 5)                    6.621e-22   4.13e-23     16.018      0.000    5.81e-22    7.43e-22\n",
       "kw_max_min                                 1.755e-29    1.1e-30     16.010      0.000    1.54e-29    1.97e-29\n",
       "np.power(kw_max_min, 5)                   -7.914e-23   9.02e-23     -0.878      0.380   -2.56e-22    9.76e-23\n",
       "kw_min_max                                 7.666e-28   4.79e-29     16.018      0.000    6.73e-28     8.6e-28\n",
       "np.power(kw_min_max, 5)                   -2.031e-26   4.32e-27     -4.702      0.000   -2.88e-26   -1.18e-26\n",
       "kw_max_max                                 1.294e-27   8.08e-29     16.014      0.000    1.14e-27    1.45e-27\n",
       "np.power(kw_max_max, 5)                    6.886e-27   1.34e-28     51.540      0.000    6.62e-27    7.15e-27\n",
       "kw_avg_max                                 1.232e-27   7.69e-29     16.012      0.000    1.08e-27    1.38e-27\n",
       "np.power(kw_avg_max, 5)                    6.972e-27   3.34e-27      2.086      0.037    4.21e-28    1.35e-26\n",
       "kw_min_avg                                 5.728e-29   3.58e-30     16.019      0.000    5.03e-29    6.43e-29\n",
       "np.power(kw_min_avg, 5)                    7.269e-15   4.54e-16     16.018      0.000    6.38e-15    8.16e-15\n",
       "kw_max_avg                                 6.758e-29   4.22e-30     16.016      0.000    5.93e-29    7.58e-29\n",
       "np.power(kw_max_avg, 5)                    8.512e-23   9.01e-23      0.945      0.345   -9.15e-23    2.62e-22\n",
       "self_reference_avg_sharess                  9.75e-29   6.09e-30     16.013      0.000    8.56e-29    1.09e-28\n",
       "np.power(self_reference_avg_sharess, 5)    5.514e-27   1.39e-26      0.398      0.691   -2.17e-26    3.27e-26\n",
       "LDA_01                                     8.152e-34   5.09e-35     16.019      0.000    7.15e-34    9.15e-34\n",
       "np.power(LDA_01, 5)                        1.291e-34   8.04e-36     16.043      0.000    1.13e-34    1.45e-34\n",
       "global_subjectivity                        2.163e-33   1.35e-34     16.019      0.000     1.9e-33    2.43e-33\n",
       "np.power(global_subjectivity, 5)           4.125e-34   2.58e-35     16.017      0.000    3.62e-34    4.63e-34\n",
       "global_rate_positive_words                 1.801e-34   1.12e-35     16.026      0.000    1.58e-34    2.02e-34\n",
       "np.power(global_rate_positive_words, 5)    4.796e-39   2.99e-40     16.014      0.000    4.21e-39    5.38e-39\n",
       "avg_positive_polarity                      1.632e-33   1.02e-34     16.023      0.000    1.43e-33    1.83e-33\n",
       "np.power(avg_positive_polarity, 5)         1.386e-34   8.65e-36     16.022      0.000    1.22e-34    1.56e-34\n",
       "min_positive_polarity                      5.277e-34    3.3e-35     15.985      0.000    4.63e-34    5.92e-34\n",
       "np.power(min_positive_polarity, 5)         1.204e-35   7.52e-37     15.998      0.000    1.06e-35    1.35e-35\n",
       "max_positive_polarity                       2.99e-33   1.86e-34     16.034      0.000    2.62e-33    3.36e-33\n",
       "np.power(max_positive_polarity, 5)          3.16e-33   1.97e-34     16.054      0.000    2.77e-33    3.55e-33\n",
       "min_negative_polarity                     -1.959e-33   1.22e-34    -16.063      0.000    -2.2e-33   -1.72e-33\n",
       "np.power(min_negative_polarity, 5)        -1.183e-33   7.34e-35    -16.117      0.000   -1.33e-33   -1.04e-33\n",
       "max_negative_polarity                     -5.393e-34   3.37e-35    -15.984      0.000   -6.05e-34   -4.73e-34\n",
       "np.power(max_negative_polarity, 5)        -6.331e-35   3.96e-36    -16.004      0.000   -7.11e-35   -5.56e-35\n",
       "title_subjectivity                         2.142e-33   1.34e-34     16.025      0.000    1.88e-33     2.4e-33\n",
       "np.power(title_subjectivity, 5)            1.366e-33   8.53e-35     16.022      0.000     1.2e-33    1.53e-33\n",
       "title_sentiment_polarity                   9.595e-34   5.98e-35     16.051      0.000    8.42e-34    1.08e-33\n",
       "np.power(title_sentiment_polarity, 5)      3.452e-34   2.15e-35     16.040      0.000    3.03e-34    3.87e-34\n",
       "abs_title_subjectivity                     8.025e-34   5.02e-35     16.000      0.000    7.04e-34    9.01e-34\n",
       "np.power(abs_title_subjectivity, 5)        3.642e-35   2.28e-36     15.996      0.000     3.2e-35    4.09e-35\n",
       "abs_title_sentiment_polarity               1.545e-33   9.64e-35     16.029      0.000    1.36e-33    1.73e-33\n",
       "np.power(abs_title_sentiment_polarity, 5)  5.458e-34   3.41e-35     16.025      0.000    4.79e-34    6.13e-34\n",
       "data_channel                               1.526e-32   9.54e-34     15.993      0.000    1.34e-32    1.71e-32\n",
       "np.power(data_channel, 5)                  6.386e-29   3.99e-30     16.008      0.000     5.6e-29    7.17e-29\n",
       "weekday_is                                 1.487e-32   9.27e-34     16.029      0.000     1.3e-32    1.67e-32\n",
       "np.power(weekday_is, 5)                    2.306e-29   1.44e-30     16.048      0.000    2.02e-29    2.59e-29\n",
       "==============================================================================\n",
       "Omnibus:                    37434.597   Durbin-Watson:                   1.985\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):         14285131.299\n",
       "Skew:                           8.834   Prob(JB):                         0.00\n",
       "Kurtosis:                     117.013   Cond. No.                     2.86e+16\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 2.86e+16. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "degree = 5\n",
    "polynomial_regression_summary(train_df, predictors_wo_multicol, 'shares', degree).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54cd7c3-bcc4-44fc-9c47-a890a0ddcecd",
   "metadata": {},
   "source": [
    "#### Summary of Results & Analysis: Polynomial Regression without Multicollinearity\n",
    "Considering the potential impact of multicollinearity on the model's stability and interpretability, polynomial regression was further conducted using predictor variables devoid of significant multicollinearity. This refinement yielded a slightly improved R-squared value of 0.047, again at a polynomial degree of 2. This is the best model so far overall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9f1249-33e3-4000-a2cb-c819a72e426b",
   "metadata": {},
   "source": [
    "#### Polynomial Regression with Focus on Significant Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3e5ee3f9-f592-4a96-bb24-bcd31f233379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>shares</td>      <th>  R-squared:         </th>  <td>   0.050</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.048</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   29.78</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 11 Aug 2023</td> <th>  Prob (F-statistic):</th>  <td>1.86e-244</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:34:52</td>     <th>  Log-Likelihood:    </th> <td>-2.6416e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 25756</td>      <th>  AIC:               </th>  <td>5.284e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 25710</td>      <th>  BIC:               </th>  <td>5.288e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    45</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                     <td></td>                       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                              <td> 6545.8000</td> <td> 5115.684</td> <td>    1.280</td> <td> 0.201</td> <td>-3481.228</td> <td> 1.66e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n_tokens_title</th>                         <td> -221.2505</td> <td>  149.073</td> <td>   -1.484</td> <td> 0.138</td> <td> -513.442</td> <td>   70.941</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(n_tokens_title, 2)</th>            <td>   14.0249</td> <td>    6.985</td> <td>    2.008</td> <td> 0.045</td> <td>    0.333</td> <td>   27.716</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n_non_stop_unique_tokens</th>               <td> 1070.4547</td> <td>  536.831</td> <td>    1.994</td> <td> 0.046</td> <td>   18.236</td> <td> 2122.674</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(n_non_stop_unique_tokens, 2)</th>  <td>   -1.6324</td> <td>    0.825</td> <td>   -1.978</td> <td> 0.048</td> <td>   -3.250</td> <td>   -0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_hrefs</th>                              <td>   34.8145</td> <td>    7.245</td> <td>    4.805</td> <td> 0.000</td> <td>   20.614</td> <td>   49.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_hrefs, 2)</th>                 <td>    0.0123</td> <td>    0.069</td> <td>    0.178</td> <td> 0.859</td> <td>   -0.124</td> <td>    0.148</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_self_hrefs</th>                         <td>  -53.3352</td> <td>   19.432</td> <td>   -2.745</td> <td> 0.006</td> <td>  -91.423</td> <td>  -15.247</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_self_hrefs, 2)</th>            <td>    0.1804</td> <td>    0.476</td> <td>    0.379</td> <td> 0.705</td> <td>   -0.753</td> <td>    1.113</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_imgs</th>                               <td>   35.2475</td> <td>   11.502</td> <td>    3.065</td> <td> 0.002</td> <td>   12.703</td> <td>   57.792</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_imgs, 2)</th>                  <td>   -0.3294</td> <td>    0.205</td> <td>   -1.607</td> <td> 0.108</td> <td>   -0.731</td> <td>    0.072</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>average_token_length</th>                   <td>-2788.4869</td> <td> 2525.280</td> <td>   -1.104</td> <td> 0.270</td> <td>-7738.178</td> <td> 2161.205</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(average_token_length, 2)</th>      <td>  242.1117</td> <td>  263.569</td> <td>    0.919</td> <td> 0.358</td> <td> -274.497</td> <td>  758.721</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_keywords</th>                           <td>  165.8175</td> <td>  176.325</td> <td>    0.940</td> <td> 0.347</td> <td> -179.790</td> <td>  511.425</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_keywords, 2)</th>              <td>   -6.7251</td> <td>   11.999</td> <td>   -0.560</td> <td> 0.575</td> <td>  -30.245</td> <td>   16.794</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_max_min</th>                             <td>   -0.0988</td> <td>    0.024</td> <td>   -4.068</td> <td> 0.000</td> <td>   -0.146</td> <td>   -0.051</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_max_min, 2)</th>                <td> 1.226e-06</td> <td> 2.58e-07</td> <td>    4.743</td> <td> 0.000</td> <td> 7.19e-07</td> <td> 1.73e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_min_max</th>                             <td>   -0.0084</td> <td>    0.003</td> <td>   -2.942</td> <td> 0.003</td> <td>   -0.014</td> <td>   -0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_min_max, 2)</th>                <td> 5.286e-09</td> <td> 3.79e-09</td> <td>    1.395</td> <td> 0.163</td> <td>-2.14e-09</td> <td> 1.27e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_avg_max</th>                             <td>   -0.0025</td> <td>    0.001</td> <td>   -2.206</td> <td> 0.027</td> <td>   -0.005</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_avg_max, 2)</th>                <td> 5.192e-09</td> <td> 2.02e-09</td> <td>    2.568</td> <td> 0.010</td> <td> 1.23e-09</td> <td> 9.15e-09</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_min_avg</th>                             <td>   -0.9266</td> <td>    0.128</td> <td>   -7.240</td> <td> 0.000</td> <td>   -1.177</td> <td>   -0.676</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_min_avg, 2)</th>                <td>    0.0004</td> <td> 4.74e-05</td> <td>    8.750</td> <td> 0.000</td> <td>    0.000</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_max_avg</th>                             <td>    0.1580</td> <td>    0.021</td> <td>    7.563</td> <td> 0.000</td> <td>    0.117</td> <td>    0.199</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_max_avg, 2)</th>                <td> -1.34e-06</td> <td>  2.5e-07</td> <td>   -5.354</td> <td> 0.000</td> <td>-1.83e-06</td> <td> -8.5e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>self_reference_min_shares</th>              <td>    0.0503</td> <td>    0.006</td> <td>    8.412</td> <td> 0.000</td> <td>    0.039</td> <td>    0.062</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(self_reference_min_shares, 2)</th> <td>-7.768e-08</td> <td> 1.01e-08</td> <td>   -7.704</td> <td> 0.000</td> <td>-9.74e-08</td> <td>-5.79e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>self_reference_max_shares</th>              <td>    0.0174</td> <td>    0.003</td> <td>    5.086</td> <td> 0.000</td> <td>    0.011</td> <td>    0.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(self_reference_max_shares, 2)</th> <td>-2.242e-08</td> <td> 5.18e-09</td> <td>   -4.327</td> <td> 0.000</td> <td>-3.26e-08</td> <td>-1.23e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LDA_00</th>                                 <td> 2063.5531</td> <td> 1214.756</td> <td>    1.699</td> <td> 0.089</td> <td> -317.437</td> <td> 4444.543</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(LDA_00, 2)</th>                    <td> -607.9834</td> <td>  835.151</td> <td>   -0.728</td> <td> 0.467</td> <td>-2244.925</td> <td> 1028.959</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LDA_01</th>                                 <td> -624.0231</td> <td> 1213.764</td> <td>   -0.514</td> <td> 0.607</td> <td>-3003.068</td> <td> 1755.022</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(LDA_01, 2)</th>                    <td> 2075.8829</td> <td>  935.555</td> <td>    2.219</td> <td> 0.027</td> <td>  242.143</td> <td> 3909.623</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LDA_02</th>                                 <td> 1580.1159</td> <td> 1226.638</td> <td>    1.288</td> <td> 0.198</td> <td> -824.164</td> <td> 3984.395</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(LDA_02, 2)</th>                    <td>-1739.7186</td> <td>  793.442</td> <td>   -2.193</td> <td> 0.028</td> <td>-3294.910</td> <td> -184.527</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LDA_03</th>                                 <td> 3585.7111</td> <td> 1185.698</td> <td>    3.024</td> <td> 0.002</td> <td> 1261.676</td> <td> 5909.746</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(LDA_03, 2)</th>                    <td>-2610.5930</td> <td>  814.570</td> <td>   -3.205</td> <td> 0.001</td> <td>-4207.196</td> <td>-1013.990</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LDA_04</th>                                 <td>  -59.1916</td> <td> 1199.126</td> <td>   -0.049</td> <td> 0.961</td> <td>-2409.545</td> <td> 2291.162</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(LDA_04, 2)</th>                    <td> 1429.2461</td> <td>  766.622</td> <td>    1.864</td> <td> 0.062</td> <td>  -73.377</td> <td> 2931.869</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>global_subjectivity</th>                    <td> -851.3107</td> <td> 3056.340</td> <td>   -0.279</td> <td> 0.781</td> <td>-6841.908</td> <td> 5139.287</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(global_subjectivity, 2)</th>       <td> 3974.8779</td> <td> 3263.622</td> <td>    1.218</td> <td> 0.223</td> <td>-2422.004</td> <td> 1.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>title_sentiment_polarity</th>               <td>  454.7811</td> <td>  180.172</td> <td>    2.524</td> <td> 0.012</td> <td>  101.634</td> <td>  807.928</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(title_sentiment_polarity, 2)</th>  <td>  959.4809</td> <td>  276.560</td> <td>    3.469</td> <td> 0.001</td> <td>  417.408</td> <td> 1501.554</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>abs_title_subjectivity</th>                 <td> 3453.9365</td> <td> 1220.012</td> <td>    2.831</td> <td> 0.005</td> <td> 1062.644</td> <td> 5845.229</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(abs_title_subjectivity, 2)</th>    <td>-5162.4882</td> <td> 2122.937</td> <td>   -2.432</td> <td> 0.015</td> <td>-9323.564</td> <td>-1001.412</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>data_channel</th>                           <td>  -42.9665</td> <td>  162.806</td> <td>   -0.264</td> <td> 0.792</td> <td> -362.076</td> <td>  276.143</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(data_channel, 2)</th>              <td>   25.5592</td> <td>   19.726</td> <td>    1.296</td> <td> 0.195</td> <td>  -13.105</td> <td>   64.223</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>38019.017</td> <th>  Durbin-Watson:     </th>   <td>   1.994</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>15744527.785</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 9.081</td>   <th>  Prob(JB):          </th>   <td>    0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>122.755</td>  <th>  Cond. No.          </th>   <td>2.47e+15</td>  \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 2.47e+15. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                            &      shares      & \\textbf{  R-squared:         } &      0.050    \\\\\n",
       "\\textbf{Model:}                                    &       OLS        & \\textbf{  Adj. R-squared:    } &      0.048    \\\\\n",
       "\\textbf{Method:}                                   &  Least Squares   & \\textbf{  F-statistic:       } &      29.78    \\\\\n",
       "\\textbf{Date:}                                     & Fri, 11 Aug 2023 & \\textbf{  Prob (F-statistic):} &  1.86e-244    \\\\\n",
       "\\textbf{Time:}                                     &     11:34:52     & \\textbf{  Log-Likelihood:    } & -2.6416e+05   \\\\\n",
       "\\textbf{No. Observations:}                         &       25756      & \\textbf{  AIC:               } &  5.284e+05    \\\\\n",
       "\\textbf{Df Residuals:}                             &       25710      & \\textbf{  BIC:               } &  5.288e+05    \\\\\n",
       "\\textbf{Df Model:}                                 &          45      & \\textbf{                     } &               \\\\\n",
       "\\textbf{Covariance Type:}                          &    nonrobust     & \\textbf{                     } &               \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                                   & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                                 &    6545.8000  &     5115.684     &     1.280  &         0.201        &    -3481.228    &     1.66e+04     \\\\\n",
       "\\textbf{n\\_tokens\\_title}                          &    -221.2505  &      149.073     &    -1.484  &         0.138        &     -513.442    &       70.941     \\\\\n",
       "\\textbf{np.power(n\\_tokens\\_title, 2)}             &      14.0249  &        6.985     &     2.008  &         0.045        &        0.333    &       27.716     \\\\\n",
       "\\textbf{n\\_non\\_stop\\_unique\\_tokens}              &    1070.4547  &      536.831     &     1.994  &         0.046        &       18.236    &     2122.674     \\\\\n",
       "\\textbf{np.power(n\\_non\\_stop\\_unique\\_tokens, 2)} &      -1.6324  &        0.825     &    -1.978  &         0.048        &       -3.250    &       -0.015     \\\\\n",
       "\\textbf{num\\_hrefs}                                &      34.8145  &        7.245     &     4.805  &         0.000        &       20.614    &       49.015     \\\\\n",
       "\\textbf{np.power(num\\_hrefs, 2)}                   &       0.0123  &        0.069     &     0.178  &         0.859        &       -0.124    &        0.148     \\\\\n",
       "\\textbf{num\\_self\\_hrefs}                          &     -53.3352  &       19.432     &    -2.745  &         0.006        &      -91.423    &      -15.247     \\\\\n",
       "\\textbf{np.power(num\\_self\\_hrefs, 2)}             &       0.1804  &        0.476     &     0.379  &         0.705        &       -0.753    &        1.113     \\\\\n",
       "\\textbf{num\\_imgs}                                 &      35.2475  &       11.502     &     3.065  &         0.002        &       12.703    &       57.792     \\\\\n",
       "\\textbf{np.power(num\\_imgs, 2)}                    &      -0.3294  &        0.205     &    -1.607  &         0.108        &       -0.731    &        0.072     \\\\\n",
       "\\textbf{average\\_token\\_length}                    &   -2788.4869  &     2525.280     &    -1.104  &         0.270        &    -7738.178    &     2161.205     \\\\\n",
       "\\textbf{np.power(average\\_token\\_length, 2)}       &     242.1117  &      263.569     &     0.919  &         0.358        &     -274.497    &      758.721     \\\\\n",
       "\\textbf{num\\_keywords}                             &     165.8175  &      176.325     &     0.940  &         0.347        &     -179.790    &      511.425     \\\\\n",
       "\\textbf{np.power(num\\_keywords, 2)}                &      -6.7251  &       11.999     &    -0.560  &         0.575        &      -30.245    &       16.794     \\\\\n",
       "\\textbf{kw\\_max\\_min}                              &      -0.0988  &        0.024     &    -4.068  &         0.000        &       -0.146    &       -0.051     \\\\\n",
       "\\textbf{np.power(kw\\_max\\_min, 2)}                 &    1.226e-06  &     2.58e-07     &     4.743  &         0.000        &     7.19e-07    &     1.73e-06     \\\\\n",
       "\\textbf{kw\\_min\\_max}                              &      -0.0084  &        0.003     &    -2.942  &         0.003        &       -0.014    &       -0.003     \\\\\n",
       "\\textbf{np.power(kw\\_min\\_max, 2)}                 &    5.286e-09  &     3.79e-09     &     1.395  &         0.163        &    -2.14e-09    &     1.27e-08     \\\\\n",
       "\\textbf{kw\\_avg\\_max}                              &      -0.0025  &        0.001     &    -2.206  &         0.027        &       -0.005    &       -0.000     \\\\\n",
       "\\textbf{np.power(kw\\_avg\\_max, 2)}                 &    5.192e-09  &     2.02e-09     &     2.568  &         0.010        &     1.23e-09    &     9.15e-09     \\\\\n",
       "\\textbf{kw\\_min\\_avg}                              &      -0.9266  &        0.128     &    -7.240  &         0.000        &       -1.177    &       -0.676     \\\\\n",
       "\\textbf{np.power(kw\\_min\\_avg, 2)}                 &       0.0004  &     4.74e-05     &     8.750  &         0.000        &        0.000    &        0.001     \\\\\n",
       "\\textbf{kw\\_max\\_avg}                              &       0.1580  &        0.021     &     7.563  &         0.000        &        0.117    &        0.199     \\\\\n",
       "\\textbf{np.power(kw\\_max\\_avg, 2)}                 &    -1.34e-06  &      2.5e-07     &    -5.354  &         0.000        &    -1.83e-06    &     -8.5e-07     \\\\\n",
       "\\textbf{self\\_reference\\_min\\_shares}              &       0.0503  &        0.006     &     8.412  &         0.000        &        0.039    &        0.062     \\\\\n",
       "\\textbf{np.power(self\\_reference\\_min\\_shares, 2)} &   -7.768e-08  &     1.01e-08     &    -7.704  &         0.000        &    -9.74e-08    &    -5.79e-08     \\\\\n",
       "\\textbf{self\\_reference\\_max\\_shares}              &       0.0174  &        0.003     &     5.086  &         0.000        &        0.011    &        0.024     \\\\\n",
       "\\textbf{np.power(self\\_reference\\_max\\_shares, 2)} &   -2.242e-08  &     5.18e-09     &    -4.327  &         0.000        &    -3.26e-08    &    -1.23e-08     \\\\\n",
       "\\textbf{LDA\\_00}                                   &    2063.5531  &     1214.756     &     1.699  &         0.089        &     -317.437    &     4444.543     \\\\\n",
       "\\textbf{np.power(LDA\\_00, 2)}                      &    -607.9834  &      835.151     &    -0.728  &         0.467        &    -2244.925    &     1028.959     \\\\\n",
       "\\textbf{LDA\\_01}                                   &    -624.0231  &     1213.764     &    -0.514  &         0.607        &    -3003.068    &     1755.022     \\\\\n",
       "\\textbf{np.power(LDA\\_01, 2)}                      &    2075.8829  &      935.555     &     2.219  &         0.027        &      242.143    &     3909.623     \\\\\n",
       "\\textbf{LDA\\_02}                                   &    1580.1159  &     1226.638     &     1.288  &         0.198        &     -824.164    &     3984.395     \\\\\n",
       "\\textbf{np.power(LDA\\_02, 2)}                      &   -1739.7186  &      793.442     &    -2.193  &         0.028        &    -3294.910    &     -184.527     \\\\\n",
       "\\textbf{LDA\\_03}                                   &    3585.7111  &     1185.698     &     3.024  &         0.002        &     1261.676    &     5909.746     \\\\\n",
       "\\textbf{np.power(LDA\\_03, 2)}                      &   -2610.5930  &      814.570     &    -3.205  &         0.001        &    -4207.196    &    -1013.990     \\\\\n",
       "\\textbf{LDA\\_04}                                   &     -59.1916  &     1199.126     &    -0.049  &         0.961        &    -2409.545    &     2291.162     \\\\\n",
       "\\textbf{np.power(LDA\\_04, 2)}                      &    1429.2461  &      766.622     &     1.864  &         0.062        &      -73.377    &     2931.869     \\\\\n",
       "\\textbf{global\\_subjectivity}                      &    -851.3107  &     3056.340     &    -0.279  &         0.781        &    -6841.908    &     5139.287     \\\\\n",
       "\\textbf{np.power(global\\_subjectivity, 2)}         &    3974.8779  &     3263.622     &     1.218  &         0.223        &    -2422.004    &     1.04e+04     \\\\\n",
       "\\textbf{title\\_sentiment\\_polarity}                &     454.7811  &      180.172     &     2.524  &         0.012        &      101.634    &      807.928     \\\\\n",
       "\\textbf{np.power(title\\_sentiment\\_polarity, 2)}   &     959.4809  &      276.560     &     3.469  &         0.001        &      417.408    &     1501.554     \\\\\n",
       "\\textbf{abs\\_title\\_subjectivity}                  &    3453.9365  &     1220.012     &     2.831  &         0.005        &     1062.644    &     5845.229     \\\\\n",
       "\\textbf{np.power(abs\\_title\\_subjectivity, 2)}     &   -5162.4882  &     2122.937     &    -2.432  &         0.015        &    -9323.564    &    -1001.412     \\\\\n",
       "\\textbf{data\\_channel}                             &     -42.9665  &      162.806     &    -0.264  &         0.792        &     -362.076    &      276.143     \\\\\n",
       "\\textbf{np.power(data\\_channel, 2)}                &      25.5592  &       19.726     &     1.296  &         0.195        &      -13.105    &       64.223     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 38019.017 & \\textbf{  Durbin-Watson:     } &      1.994    \\\\\n",
       "\\textbf{Prob(Omnibus):} &    0.000  & \\textbf{  Jarque-Bera (JB):  } & 15744527.785  \\\\\n",
       "\\textbf{Skew:}          &    9.081  & \\textbf{  Prob(JB):          } &       0.00    \\\\\n",
       "\\textbf{Kurtosis:}      &  122.755  & \\textbf{  Cond. No.          } &   2.47e+15    \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 2.47e+15. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 shares   R-squared:                       0.050\n",
       "Model:                            OLS   Adj. R-squared:                  0.048\n",
       "Method:                 Least Squares   F-statistic:                     29.78\n",
       "Date:                Fri, 11 Aug 2023   Prob (F-statistic):          1.86e-244\n",
       "Time:                        11:34:52   Log-Likelihood:            -2.6416e+05\n",
       "No. Observations:               25756   AIC:                         5.284e+05\n",
       "Df Residuals:                   25710   BIC:                         5.288e+05\n",
       "Df Model:                          45                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==========================================================================================================\n",
       "                                             coef    std err          t      P>|t|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------------------------------\n",
       "Intercept                               6545.8000   5115.684      1.280      0.201   -3481.228    1.66e+04\n",
       "n_tokens_title                          -221.2505    149.073     -1.484      0.138    -513.442      70.941\n",
       "np.power(n_tokens_title, 2)               14.0249      6.985      2.008      0.045       0.333      27.716\n",
       "n_non_stop_unique_tokens                1070.4547    536.831      1.994      0.046      18.236    2122.674\n",
       "np.power(n_non_stop_unique_tokens, 2)     -1.6324      0.825     -1.978      0.048      -3.250      -0.015\n",
       "num_hrefs                                 34.8145      7.245      4.805      0.000      20.614      49.015\n",
       "np.power(num_hrefs, 2)                     0.0123      0.069      0.178      0.859      -0.124       0.148\n",
       "num_self_hrefs                           -53.3352     19.432     -2.745      0.006     -91.423     -15.247\n",
       "np.power(num_self_hrefs, 2)                0.1804      0.476      0.379      0.705      -0.753       1.113\n",
       "num_imgs                                  35.2475     11.502      3.065      0.002      12.703      57.792\n",
       "np.power(num_imgs, 2)                     -0.3294      0.205     -1.607      0.108      -0.731       0.072\n",
       "average_token_length                   -2788.4869   2525.280     -1.104      0.270   -7738.178    2161.205\n",
       "np.power(average_token_length, 2)        242.1117    263.569      0.919      0.358    -274.497     758.721\n",
       "num_keywords                             165.8175    176.325      0.940      0.347    -179.790     511.425\n",
       "np.power(num_keywords, 2)                 -6.7251     11.999     -0.560      0.575     -30.245      16.794\n",
       "kw_max_min                                -0.0988      0.024     -4.068      0.000      -0.146      -0.051\n",
       "np.power(kw_max_min, 2)                 1.226e-06   2.58e-07      4.743      0.000    7.19e-07    1.73e-06\n",
       "kw_min_max                                -0.0084      0.003     -2.942      0.003      -0.014      -0.003\n",
       "np.power(kw_min_max, 2)                 5.286e-09   3.79e-09      1.395      0.163   -2.14e-09    1.27e-08\n",
       "kw_avg_max                                -0.0025      0.001     -2.206      0.027      -0.005      -0.000\n",
       "np.power(kw_avg_max, 2)                 5.192e-09   2.02e-09      2.568      0.010    1.23e-09    9.15e-09\n",
       "kw_min_avg                                -0.9266      0.128     -7.240      0.000      -1.177      -0.676\n",
       "np.power(kw_min_avg, 2)                    0.0004   4.74e-05      8.750      0.000       0.000       0.001\n",
       "kw_max_avg                                 0.1580      0.021      7.563      0.000       0.117       0.199\n",
       "np.power(kw_max_avg, 2)                 -1.34e-06    2.5e-07     -5.354      0.000   -1.83e-06    -8.5e-07\n",
       "self_reference_min_shares                  0.0503      0.006      8.412      0.000       0.039       0.062\n",
       "np.power(self_reference_min_shares, 2) -7.768e-08   1.01e-08     -7.704      0.000   -9.74e-08   -5.79e-08\n",
       "self_reference_max_shares                  0.0174      0.003      5.086      0.000       0.011       0.024\n",
       "np.power(self_reference_max_shares, 2) -2.242e-08   5.18e-09     -4.327      0.000   -3.26e-08   -1.23e-08\n",
       "LDA_00                                  2063.5531   1214.756      1.699      0.089    -317.437    4444.543\n",
       "np.power(LDA_00, 2)                     -607.9834    835.151     -0.728      0.467   -2244.925    1028.959\n",
       "LDA_01                                  -624.0231   1213.764     -0.514      0.607   -3003.068    1755.022\n",
       "np.power(LDA_01, 2)                     2075.8829    935.555      2.219      0.027     242.143    3909.623\n",
       "LDA_02                                  1580.1159   1226.638      1.288      0.198    -824.164    3984.395\n",
       "np.power(LDA_02, 2)                    -1739.7186    793.442     -2.193      0.028   -3294.910    -184.527\n",
       "LDA_03                                  3585.7111   1185.698      3.024      0.002    1261.676    5909.746\n",
       "np.power(LDA_03, 2)                    -2610.5930    814.570     -3.205      0.001   -4207.196   -1013.990\n",
       "LDA_04                                   -59.1916   1199.126     -0.049      0.961   -2409.545    2291.162\n",
       "np.power(LDA_04, 2)                     1429.2461    766.622      1.864      0.062     -73.377    2931.869\n",
       "global_subjectivity                     -851.3107   3056.340     -0.279      0.781   -6841.908    5139.287\n",
       "np.power(global_subjectivity, 2)        3974.8779   3263.622      1.218      0.223   -2422.004    1.04e+04\n",
       "title_sentiment_polarity                 454.7811    180.172      2.524      0.012     101.634     807.928\n",
       "np.power(title_sentiment_polarity, 2)    959.4809    276.560      3.469      0.001     417.408    1501.554\n",
       "abs_title_subjectivity                  3453.9365   1220.012      2.831      0.005    1062.644    5845.229\n",
       "np.power(abs_title_subjectivity, 2)    -5162.4882   2122.937     -2.432      0.015   -9323.564   -1001.412\n",
       "data_channel                             -42.9665    162.806     -0.264      0.792    -362.076     276.143\n",
       "np.power(data_channel, 2)                 25.5592     19.726      1.296      0.195     -13.105      64.223\n",
       "==============================================================================\n",
       "Omnibus:                    38019.017   Durbin-Watson:                   1.994\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):         15744527.785\n",
       "Skew:                           9.081   Prob(JB):                         0.00\n",
       "Kurtosis:                     122.755   Cond. No.                     2.47e+15\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 2.47e+15. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "degree = 2\n",
    "polynomial_regression_summary(train_df, significant_predictors, 'shares', degree).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "db93e825-8954-4bac-8770-20ca0d0e3964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>shares</td>      <th>  R-squared:         </th>  <td>   0.035</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.034</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   54.19</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 11 Aug 2023</td> <th>  Prob (F-statistic):</th>  <td>1.48e-181</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:34:52</td>     <th>  Log-Likelihood:    </th> <td>-2.6436e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 25756</td>      <th>  AIC:               </th>  <td>5.288e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 25738</td>      <th>  BIC:               </th>  <td>5.289e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    17</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                     <td></td>                       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                              <td>    0.0090</td> <td>    0.001</td> <td>   10.687</td> <td> 0.000</td> <td>    0.007</td> <td>    0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n_tokens_title</th>                         <td>    0.0026</td> <td>    0.001</td> <td>    2.791</td> <td> 0.005</td> <td>    0.001</td> <td>    0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(n_tokens_title, 3)</th>            <td>    0.3324</td> <td>    0.054</td> <td>    6.112</td> <td> 0.000</td> <td>    0.226</td> <td>    0.439</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n_non_stop_unique_tokens</th>               <td>    0.0004</td> <td> 5.63e-05</td> <td>    6.905</td> <td> 0.000</td> <td>    0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(n_non_stop_unique_tokens, 3)</th>  <td> 1.259e-05</td> <td> 2.53e-05</td> <td>    0.498</td> <td> 0.619</td> <td> -3.7e-05</td> <td> 6.22e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_hrefs</th>                              <td>    0.0399</td> <td>    0.004</td> <td>    9.713</td> <td> 0.000</td> <td>    0.032</td> <td>    0.048</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_hrefs, 3)</th>                 <td>    0.0007</td> <td>    0.000</td> <td>    3.442</td> <td> 0.001</td> <td>    0.000</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_self_hrefs</th>                         <td>   -0.0012</td> <td>    0.001</td> <td>   -1.857</td> <td> 0.063</td> <td>   -0.003</td> <td>  6.9e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_self_hrefs, 3)</th>            <td>   -0.0014</td> <td>    0.004</td> <td>   -0.354</td> <td> 0.723</td> <td>   -0.009</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_imgs</th>                               <td>    0.0203</td> <td>    0.002</td> <td>    8.976</td> <td> 0.000</td> <td>    0.016</td> <td>    0.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_imgs, 3)</th>                  <td>    0.0023</td> <td>    0.001</td> <td>    1.627</td> <td> 0.104</td> <td>   -0.000</td> <td>    0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>average_token_length</th>                   <td>    0.0055</td> <td>    0.000</td> <td>   12.489</td> <td> 0.000</td> <td>    0.005</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(average_token_length, 3)</th>      <td>    0.2660</td> <td>    0.028</td> <td>    9.537</td> <td> 0.000</td> <td>    0.211</td> <td>    0.321</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_keywords</th>                           <td>    0.0108</td> <td>    0.001</td> <td>   10.778</td> <td> 0.000</td> <td>    0.009</td> <td>    0.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_keywords, 3)</th>              <td>    1.1904</td> <td>    0.135</td> <td>    8.824</td> <td> 0.000</td> <td>    0.926</td> <td>    1.455</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_max_min</th>                             <td>   -0.1131</td> <td>    0.019</td> <td>   -5.877</td> <td> 0.000</td> <td>   -0.151</td> <td>   -0.075</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_max_min, 3)</th>                <td> 1.315e-11</td> <td> 1.74e-12</td> <td>    7.554</td> <td> 0.000</td> <td> 9.74e-12</td> <td> 1.66e-11</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_min_max</th>                             <td>   -0.0083</td> <td>    0.002</td> <td>   -3.993</td> <td> 0.000</td> <td>   -0.012</td> <td>   -0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_min_max, 3)</th>                <td> 6.412e-15</td> <td> 3.78e-15</td> <td>    1.699</td> <td> 0.089</td> <td>-9.87e-16</td> <td> 1.38e-14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_avg_max</th>                             <td>    0.0022</td> <td>    0.001</td> <td>    3.874</td> <td> 0.000</td> <td>    0.001</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_avg_max, 3)</th>                <td> 6.917e-16</td> <td> 1.97e-15</td> <td>    0.352</td> <td> 0.725</td> <td>-3.16e-15</td> <td> 4.55e-15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_min_avg</th>                             <td>   -0.4449</td> <td>    0.078</td> <td>   -5.668</td> <td> 0.000</td> <td>   -0.599</td> <td>   -0.291</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_min_avg, 3)</th>                <td> 9.958e-08</td> <td>  9.3e-09</td> <td>   10.705</td> <td> 0.000</td> <td> 8.13e-08</td> <td> 1.18e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_max_avg</th>                             <td>    0.1761</td> <td>    0.015</td> <td>   12.104</td> <td> 0.000</td> <td>    0.148</td> <td>    0.205</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_max_avg, 3)</th>                <td>-1.342e-11</td> <td> 1.73e-12</td> <td>   -7.755</td> <td> 0.000</td> <td>-1.68e-11</td> <td>   -1e-11</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>self_reference_min_shares</th>              <td>    0.0499</td> <td>    0.005</td> <td>    9.900</td> <td> 0.000</td> <td>    0.040</td> <td>    0.060</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(self_reference_min_shares, 3)</th> <td>-1.188e-13</td> <td> 1.29e-14</td> <td>   -9.234</td> <td> 0.000</td> <td>-1.44e-13</td> <td>-9.36e-14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>self_reference_max_shares</th>              <td>    0.0153</td> <td>    0.003</td> <td>    5.889</td> <td> 0.000</td> <td>    0.010</td> <td>    0.020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(self_reference_max_shares, 3)</th> <td>-2.764e-14</td> <td> 5.58e-15</td> <td>   -4.957</td> <td> 0.000</td> <td>-3.86e-14</td> <td>-1.67e-14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LDA_00</th>                                 <td>   -0.0025</td> <td>    0.000</td> <td>   -7.319</td> <td> 0.000</td> <td>   -0.003</td> <td>   -0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(LDA_00, 3)</th>                    <td>   -0.0015</td> <td>    0.000</td> <td>   -7.453</td> <td> 0.000</td> <td>   -0.002</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LDA_01</th>                                 <td>   -0.0012</td> <td>    0.000</td> <td>   -7.378</td> <td> 0.000</td> <td>   -0.002</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(LDA_01, 3)</th>                    <td>   -0.0008</td> <td>    0.000</td> <td>   -7.582</td> <td> 0.000</td> <td>   -0.001</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LDA_02</th>                                 <td>    0.0030</td> <td>    0.000</td> <td>    7.583</td> <td> 0.000</td> <td>    0.002</td> <td>    0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(LDA_02, 3)</th>                    <td>    0.0020</td> <td>    0.000</td> <td>    7.474</td> <td> 0.000</td> <td>    0.001</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LDA_03</th>                                 <td>    0.0018</td> <td>    0.000</td> <td>    7.862</td> <td> 0.000</td> <td>    0.001</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(LDA_03, 3)</th>                    <td>    0.0012</td> <td>    0.000</td> <td>    7.652</td> <td> 0.000</td> <td>    0.001</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LDA_04</th>                                 <td>   -0.0005</td> <td>    0.000</td> <td>   -4.030</td> <td> 0.000</td> <td>   -0.001</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(LDA_04, 3)</th>                    <td>   -0.0004</td> <td>  8.3e-05</td> <td>   -4.532</td> <td> 0.000</td> <td>   -0.001</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>global_subjectivity</th>                    <td>    0.0003</td> <td> 2.63e-05</td> <td>   11.490</td> <td> 0.000</td> <td>    0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(global_subjectivity, 3)</th>       <td>    0.0002</td> <td> 1.35e-05</td> <td>   12.073</td> <td> 0.000</td> <td>    0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>title_sentiment_polarity</th>               <td>   -0.0001</td> <td> 2.72e-05</td> <td>   -4.170</td> <td> 0.000</td> <td>   -0.000</td> <td>   -6e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(title_sentiment_polarity, 3)</th>  <td>-4.043e-05</td> <td> 1.01e-05</td> <td>   -3.989</td> <td> 0.000</td> <td>-6.03e-05</td> <td>-2.06e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>abs_title_subjectivity</th>                 <td>    0.0003</td> <td>  2.4e-05</td> <td>   12.761</td> <td> 0.000</td> <td>    0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(abs_title_subjectivity, 3)</th>    <td> 6.831e-05</td> <td> 5.51e-06</td> <td>   12.392</td> <td> 0.000</td> <td> 5.75e-05</td> <td> 7.91e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>data_channel</th>                           <td>    0.0468</td> <td>    0.006</td> <td>    7.926</td> <td> 0.000</td> <td>    0.035</td> <td>    0.058</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(data_channel, 3)</th>              <td>    2.8980</td> <td>    0.377</td> <td>    7.689</td> <td> 0.000</td> <td>    2.159</td> <td>    3.637</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>37817.065</td> <th>  Durbin-Watson:     </th>   <td>   1.997</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>15267968.333</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 8.994</td>   <th>  Prob(JB):          </th>   <td>    0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>120.913</td>  <th>  Cond. No.          </th>   <td>7.96e+16</td>  \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 7.96e+16. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                            &      shares      & \\textbf{  R-squared:         } &      0.035    \\\\\n",
       "\\textbf{Model:}                                    &       OLS        & \\textbf{  Adj. R-squared:    } &      0.034    \\\\\n",
       "\\textbf{Method:}                                   &  Least Squares   & \\textbf{  F-statistic:       } &      54.19    \\\\\n",
       "\\textbf{Date:}                                     & Fri, 11 Aug 2023 & \\textbf{  Prob (F-statistic):} &  1.48e-181    \\\\\n",
       "\\textbf{Time:}                                     &     11:34:52     & \\textbf{  Log-Likelihood:    } & -2.6436e+05   \\\\\n",
       "\\textbf{No. Observations:}                         &       25756      & \\textbf{  AIC:               } &  5.288e+05    \\\\\n",
       "\\textbf{Df Residuals:}                             &       25738      & \\textbf{  BIC:               } &  5.289e+05    \\\\\n",
       "\\textbf{Df Model:}                                 &          17      & \\textbf{                     } &               \\\\\n",
       "\\textbf{Covariance Type:}                          &    nonrobust     & \\textbf{                     } &               \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                                   & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                                 &       0.0090  &        0.001     &    10.687  &         0.000        &        0.007    &        0.011     \\\\\n",
       "\\textbf{n\\_tokens\\_title}                          &       0.0026  &        0.001     &     2.791  &         0.005        &        0.001    &        0.004     \\\\\n",
       "\\textbf{np.power(n\\_tokens\\_title, 3)}             &       0.3324  &        0.054     &     6.112  &         0.000        &        0.226    &        0.439     \\\\\n",
       "\\textbf{n\\_non\\_stop\\_unique\\_tokens}              &       0.0004  &     5.63e-05     &     6.905  &         0.000        &        0.000    &        0.000     \\\\\n",
       "\\textbf{np.power(n\\_non\\_stop\\_unique\\_tokens, 3)} &    1.259e-05  &     2.53e-05     &     0.498  &         0.619        &     -3.7e-05    &     6.22e-05     \\\\\n",
       "\\textbf{num\\_hrefs}                                &       0.0399  &        0.004     &     9.713  &         0.000        &        0.032    &        0.048     \\\\\n",
       "\\textbf{np.power(num\\_hrefs, 3)}                   &       0.0007  &        0.000     &     3.442  &         0.001        &        0.000    &        0.001     \\\\\n",
       "\\textbf{num\\_self\\_hrefs}                          &      -0.0012  &        0.001     &    -1.857  &         0.063        &       -0.003    &      6.9e-05     \\\\\n",
       "\\textbf{np.power(num\\_self\\_hrefs, 3)}             &      -0.0014  &        0.004     &    -0.354  &         0.723        &       -0.009    &        0.006     \\\\\n",
       "\\textbf{num\\_imgs}                                 &       0.0203  &        0.002     &     8.976  &         0.000        &        0.016    &        0.025     \\\\\n",
       "\\textbf{np.power(num\\_imgs, 3)}                    &       0.0023  &        0.001     &     1.627  &         0.104        &       -0.000    &        0.005     \\\\\n",
       "\\textbf{average\\_token\\_length}                    &       0.0055  &        0.000     &    12.489  &         0.000        &        0.005    &        0.006     \\\\\n",
       "\\textbf{np.power(average\\_token\\_length, 3)}       &       0.2660  &        0.028     &     9.537  &         0.000        &        0.211    &        0.321     \\\\\n",
       "\\textbf{num\\_keywords}                             &       0.0108  &        0.001     &    10.778  &         0.000        &        0.009    &        0.013     \\\\\n",
       "\\textbf{np.power(num\\_keywords, 3)}                &       1.1904  &        0.135     &     8.824  &         0.000        &        0.926    &        1.455     \\\\\n",
       "\\textbf{kw\\_max\\_min}                              &      -0.1131  &        0.019     &    -5.877  &         0.000        &       -0.151    &       -0.075     \\\\\n",
       "\\textbf{np.power(kw\\_max\\_min, 3)}                 &    1.315e-11  &     1.74e-12     &     7.554  &         0.000        &     9.74e-12    &     1.66e-11     \\\\\n",
       "\\textbf{kw\\_min\\_max}                              &      -0.0083  &        0.002     &    -3.993  &         0.000        &       -0.012    &       -0.004     \\\\\n",
       "\\textbf{np.power(kw\\_min\\_max, 3)}                 &    6.412e-15  &     3.78e-15     &     1.699  &         0.089        &    -9.87e-16    &     1.38e-14     \\\\\n",
       "\\textbf{kw\\_avg\\_max}                              &       0.0022  &        0.001     &     3.874  &         0.000        &        0.001    &        0.003     \\\\\n",
       "\\textbf{np.power(kw\\_avg\\_max, 3)}                 &    6.917e-16  &     1.97e-15     &     0.352  &         0.725        &    -3.16e-15    &     4.55e-15     \\\\\n",
       "\\textbf{kw\\_min\\_avg}                              &      -0.4449  &        0.078     &    -5.668  &         0.000        &       -0.599    &       -0.291     \\\\\n",
       "\\textbf{np.power(kw\\_min\\_avg, 3)}                 &    9.958e-08  &      9.3e-09     &    10.705  &         0.000        &     8.13e-08    &     1.18e-07     \\\\\n",
       "\\textbf{kw\\_max\\_avg}                              &       0.1761  &        0.015     &    12.104  &         0.000        &        0.148    &        0.205     \\\\\n",
       "\\textbf{np.power(kw\\_max\\_avg, 3)}                 &   -1.342e-11  &     1.73e-12     &    -7.755  &         0.000        &    -1.68e-11    &       -1e-11     \\\\\n",
       "\\textbf{self\\_reference\\_min\\_shares}              &       0.0499  &        0.005     &     9.900  &         0.000        &        0.040    &        0.060     \\\\\n",
       "\\textbf{np.power(self\\_reference\\_min\\_shares, 3)} &   -1.188e-13  &     1.29e-14     &    -9.234  &         0.000        &    -1.44e-13    &    -9.36e-14     \\\\\n",
       "\\textbf{self\\_reference\\_max\\_shares}              &       0.0153  &        0.003     &     5.889  &         0.000        &        0.010    &        0.020     \\\\\n",
       "\\textbf{np.power(self\\_reference\\_max\\_shares, 3)} &   -2.764e-14  &     5.58e-15     &    -4.957  &         0.000        &    -3.86e-14    &    -1.67e-14     \\\\\n",
       "\\textbf{LDA\\_00}                                   &      -0.0025  &        0.000     &    -7.319  &         0.000        &       -0.003    &       -0.002     \\\\\n",
       "\\textbf{np.power(LDA\\_00, 3)}                      &      -0.0015  &        0.000     &    -7.453  &         0.000        &       -0.002    &       -0.001     \\\\\n",
       "\\textbf{LDA\\_01}                                   &      -0.0012  &        0.000     &    -7.378  &         0.000        &       -0.002    &       -0.001     \\\\\n",
       "\\textbf{np.power(LDA\\_01, 3)}                      &      -0.0008  &        0.000     &    -7.582  &         0.000        &       -0.001    &       -0.001     \\\\\n",
       "\\textbf{LDA\\_02}                                   &       0.0030  &        0.000     &     7.583  &         0.000        &        0.002    &        0.004     \\\\\n",
       "\\textbf{np.power(LDA\\_02, 3)}                      &       0.0020  &        0.000     &     7.474  &         0.000        &        0.001    &        0.003     \\\\\n",
       "\\textbf{LDA\\_03}                                   &       0.0018  &        0.000     &     7.862  &         0.000        &        0.001    &        0.002     \\\\\n",
       "\\textbf{np.power(LDA\\_03, 3)}                      &       0.0012  &        0.000     &     7.652  &         0.000        &        0.001    &        0.002     \\\\\n",
       "\\textbf{LDA\\_04}                                   &      -0.0005  &        0.000     &    -4.030  &         0.000        &       -0.001    &       -0.000     \\\\\n",
       "\\textbf{np.power(LDA\\_04, 3)}                      &      -0.0004  &      8.3e-05     &    -4.532  &         0.000        &       -0.001    &       -0.000     \\\\\n",
       "\\textbf{global\\_subjectivity}                      &       0.0003  &     2.63e-05     &    11.490  &         0.000        &        0.000    &        0.000     \\\\\n",
       "\\textbf{np.power(global\\_subjectivity, 3)}         &       0.0002  &     1.35e-05     &    12.073  &         0.000        &        0.000    &        0.000     \\\\\n",
       "\\textbf{title\\_sentiment\\_polarity}                &      -0.0001  &     2.72e-05     &    -4.170  &         0.000        &       -0.000    &       -6e-05     \\\\\n",
       "\\textbf{np.power(title\\_sentiment\\_polarity, 3)}   &   -4.043e-05  &     1.01e-05     &    -3.989  &         0.000        &    -6.03e-05    &    -2.06e-05     \\\\\n",
       "\\textbf{abs\\_title\\_subjectivity}                  &       0.0003  &      2.4e-05     &    12.761  &         0.000        &        0.000    &        0.000     \\\\\n",
       "\\textbf{np.power(abs\\_title\\_subjectivity, 3)}     &    6.831e-05  &     5.51e-06     &    12.392  &         0.000        &     5.75e-05    &     7.91e-05     \\\\\n",
       "\\textbf{data\\_channel}                             &       0.0468  &        0.006     &     7.926  &         0.000        &        0.035    &        0.058     \\\\\n",
       "\\textbf{np.power(data\\_channel, 3)}                &       2.8980  &        0.377     &     7.689  &         0.000        &        2.159    &        3.637     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 37817.065 & \\textbf{  Durbin-Watson:     } &      1.997    \\\\\n",
       "\\textbf{Prob(Omnibus):} &    0.000  & \\textbf{  Jarque-Bera (JB):  } & 15267968.333  \\\\\n",
       "\\textbf{Skew:}          &    8.994  & \\textbf{  Prob(JB):          } &       0.00    \\\\\n",
       "\\textbf{Kurtosis:}      &  120.913  & \\textbf{  Cond. No.          } &   7.96e+16    \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 7.96e+16. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 shares   R-squared:                       0.035\n",
       "Model:                            OLS   Adj. R-squared:                  0.034\n",
       "Method:                 Least Squares   F-statistic:                     54.19\n",
       "Date:                Fri, 11 Aug 2023   Prob (F-statistic):          1.48e-181\n",
       "Time:                        11:34:52   Log-Likelihood:            -2.6436e+05\n",
       "No. Observations:               25756   AIC:                         5.288e+05\n",
       "Df Residuals:                   25738   BIC:                         5.289e+05\n",
       "Df Model:                          17                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==========================================================================================================\n",
       "                                             coef    std err          t      P>|t|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------------------------------\n",
       "Intercept                                  0.0090      0.001     10.687      0.000       0.007       0.011\n",
       "n_tokens_title                             0.0026      0.001      2.791      0.005       0.001       0.004\n",
       "np.power(n_tokens_title, 3)                0.3324      0.054      6.112      0.000       0.226       0.439\n",
       "n_non_stop_unique_tokens                   0.0004   5.63e-05      6.905      0.000       0.000       0.000\n",
       "np.power(n_non_stop_unique_tokens, 3)   1.259e-05   2.53e-05      0.498      0.619    -3.7e-05    6.22e-05\n",
       "num_hrefs                                  0.0399      0.004      9.713      0.000       0.032       0.048\n",
       "np.power(num_hrefs, 3)                     0.0007      0.000      3.442      0.001       0.000       0.001\n",
       "num_self_hrefs                            -0.0012      0.001     -1.857      0.063      -0.003     6.9e-05\n",
       "np.power(num_self_hrefs, 3)               -0.0014      0.004     -0.354      0.723      -0.009       0.006\n",
       "num_imgs                                   0.0203      0.002      8.976      0.000       0.016       0.025\n",
       "np.power(num_imgs, 3)                      0.0023      0.001      1.627      0.104      -0.000       0.005\n",
       "average_token_length                       0.0055      0.000     12.489      0.000       0.005       0.006\n",
       "np.power(average_token_length, 3)          0.2660      0.028      9.537      0.000       0.211       0.321\n",
       "num_keywords                               0.0108      0.001     10.778      0.000       0.009       0.013\n",
       "np.power(num_keywords, 3)                  1.1904      0.135      8.824      0.000       0.926       1.455\n",
       "kw_max_min                                -0.1131      0.019     -5.877      0.000      -0.151      -0.075\n",
       "np.power(kw_max_min, 3)                 1.315e-11   1.74e-12      7.554      0.000    9.74e-12    1.66e-11\n",
       "kw_min_max                                -0.0083      0.002     -3.993      0.000      -0.012      -0.004\n",
       "np.power(kw_min_max, 3)                 6.412e-15   3.78e-15      1.699      0.089   -9.87e-16    1.38e-14\n",
       "kw_avg_max                                 0.0022      0.001      3.874      0.000       0.001       0.003\n",
       "np.power(kw_avg_max, 3)                 6.917e-16   1.97e-15      0.352      0.725   -3.16e-15    4.55e-15\n",
       "kw_min_avg                                -0.4449      0.078     -5.668      0.000      -0.599      -0.291\n",
       "np.power(kw_min_avg, 3)                 9.958e-08    9.3e-09     10.705      0.000    8.13e-08    1.18e-07\n",
       "kw_max_avg                                 0.1761      0.015     12.104      0.000       0.148       0.205\n",
       "np.power(kw_max_avg, 3)                -1.342e-11   1.73e-12     -7.755      0.000   -1.68e-11      -1e-11\n",
       "self_reference_min_shares                  0.0499      0.005      9.900      0.000       0.040       0.060\n",
       "np.power(self_reference_min_shares, 3) -1.188e-13   1.29e-14     -9.234      0.000   -1.44e-13   -9.36e-14\n",
       "self_reference_max_shares                  0.0153      0.003      5.889      0.000       0.010       0.020\n",
       "np.power(self_reference_max_shares, 3) -2.764e-14   5.58e-15     -4.957      0.000   -3.86e-14   -1.67e-14\n",
       "LDA_00                                    -0.0025      0.000     -7.319      0.000      -0.003      -0.002\n",
       "np.power(LDA_00, 3)                       -0.0015      0.000     -7.453      0.000      -0.002      -0.001\n",
       "LDA_01                                    -0.0012      0.000     -7.378      0.000      -0.002      -0.001\n",
       "np.power(LDA_01, 3)                       -0.0008      0.000     -7.582      0.000      -0.001      -0.001\n",
       "LDA_02                                     0.0030      0.000      7.583      0.000       0.002       0.004\n",
       "np.power(LDA_02, 3)                        0.0020      0.000      7.474      0.000       0.001       0.003\n",
       "LDA_03                                     0.0018      0.000      7.862      0.000       0.001       0.002\n",
       "np.power(LDA_03, 3)                        0.0012      0.000      7.652      0.000       0.001       0.002\n",
       "LDA_04                                    -0.0005      0.000     -4.030      0.000      -0.001      -0.000\n",
       "np.power(LDA_04, 3)                       -0.0004    8.3e-05     -4.532      0.000      -0.001      -0.000\n",
       "global_subjectivity                        0.0003   2.63e-05     11.490      0.000       0.000       0.000\n",
       "np.power(global_subjectivity, 3)           0.0002   1.35e-05     12.073      0.000       0.000       0.000\n",
       "title_sentiment_polarity                  -0.0001   2.72e-05     -4.170      0.000      -0.000      -6e-05\n",
       "np.power(title_sentiment_polarity, 3)  -4.043e-05   1.01e-05     -3.989      0.000   -6.03e-05   -2.06e-05\n",
       "abs_title_subjectivity                     0.0003    2.4e-05     12.761      0.000       0.000       0.000\n",
       "np.power(abs_title_subjectivity, 3)     6.831e-05   5.51e-06     12.392      0.000    5.75e-05    7.91e-05\n",
       "data_channel                               0.0468      0.006      7.926      0.000       0.035       0.058\n",
       "np.power(data_channel, 3)                  2.8980      0.377      7.689      0.000       2.159       3.637\n",
       "==============================================================================\n",
       "Omnibus:                    37817.065   Durbin-Watson:                   1.997\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):         15267968.333\n",
       "Skew:                           8.994   Prob(JB):                         0.00\n",
       "Kurtosis:                     120.913   Cond. No.                     7.96e+16\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 7.96e+16. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "degree = 3\n",
    "polynomial_regression_summary(train_df, significant_predictors, 'shares', degree).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "96a72883-5173-406a-a34a-ccbaa6a1a4d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>shares</td>      <th>  R-squared:         </th>  <td>  -0.102</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>  -0.102</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>  -339.7</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 11 Aug 2023</td> <th>  Prob (F-statistic):</th>   <td>  1.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:34:53</td>     <th>  Log-Likelihood:    </th> <td>-2.6606e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 25756</td>      <th>  AIC:               </th>  <td>5.321e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 25748</td>      <th>  BIC:               </th>  <td>5.322e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     7</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                     <td></td>                       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                              <td> 1.785e-09</td> <td> 7.18e-10</td> <td>    2.486</td> <td> 0.013</td> <td> 3.77e-10</td> <td> 3.19e-09</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n_tokens_title</th>                         <td>-1.318e-09</td> <td>  5.3e-10</td> <td>   -2.485</td> <td> 0.013</td> <td>-2.36e-09</td> <td>-2.78e-10</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(n_tokens_title, 4)</th>            <td>-9.008e-10</td> <td> 3.62e-10</td> <td>   -2.485</td> <td> 0.013</td> <td>-1.61e-09</td> <td> -1.9e-10</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n_non_stop_unique_tokens</th>               <td>-1.859e-10</td> <td> 7.49e-11</td> <td>   -2.483</td> <td> 0.013</td> <td>-3.33e-10</td> <td>-3.91e-11</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(n_non_stop_unique_tokens, 4)</th>  <td> 2.219e-08</td> <td> 4.15e-08</td> <td>    0.534</td> <td> 0.593</td> <td>-5.92e-08</td> <td> 1.04e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_hrefs</th>                              <td> 1.471e-12</td> <td> 5.92e-13</td> <td>    2.485</td> <td> 0.013</td> <td> 3.11e-13</td> <td> 2.63e-12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_hrefs, 4)</th>                 <td> 2.073e-06</td> <td> 8.34e-07</td> <td>    2.485</td> <td> 0.013</td> <td> 4.38e-07</td> <td> 3.71e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_self_hrefs</th>                         <td>-4.922e-15</td> <td> 1.98e-15</td> <td>   -2.485</td> <td> 0.013</td> <td>-8.81e-15</td> <td>-1.04e-15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_self_hrefs, 4)</th>            <td> 1.174e-09</td> <td> 4.72e-10</td> <td>    2.485</td> <td> 0.013</td> <td> 2.48e-10</td> <td>  2.1e-09</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_imgs</th>                               <td>-5.302e-15</td> <td> 2.14e-15</td> <td>   -2.482</td> <td> 0.013</td> <td>-9.49e-15</td> <td>-1.12e-15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_imgs, 4)</th>                  <td> 1.229e-09</td> <td> 4.94e-10</td> <td>    2.487</td> <td> 0.013</td> <td>  2.6e-10</td> <td>  2.2e-09</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>average_token_length</th>                   <td>-1.259e-14</td> <td> 5.07e-15</td> <td>   -2.485</td> <td> 0.013</td> <td>-2.25e-14</td> <td>-2.66e-15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(average_token_length, 4)</th>      <td>-1.047e-12</td> <td> 4.21e-13</td> <td>   -2.485</td> <td> 0.013</td> <td>-1.87e-12</td> <td>-2.21e-13</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_keywords</th>                           <td>-2.137e-14</td> <td>  8.6e-15</td> <td>   -2.485</td> <td> 0.013</td> <td>-3.82e-14</td> <td>-4.51e-15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_keywords, 4)</th>              <td>-1.215e-11</td> <td> 4.89e-12</td> <td>   -2.485</td> <td> 0.013</td> <td>-2.17e-11</td> <td>-2.57e-12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_max_min</th>                             <td>-3.475e-12</td> <td>  1.4e-12</td> <td>   -2.485</td> <td> 0.013</td> <td>-6.22e-12</td> <td>-7.34e-13</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_max_min, 4)</th>                <td>-2.001e-17</td> <td> 1.12e-17</td> <td>   -1.780</td> <td> 0.075</td> <td> -4.2e-17</td> <td> 2.02e-18</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_min_max</th>                             <td>-2.016e-11</td> <td> 8.11e-12</td> <td>   -2.485</td> <td> 0.013</td> <td>-3.61e-11</td> <td>-4.26e-12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_min_max, 4)</th>                <td>-5.178e-20</td> <td> 2.92e-21</td> <td>  -17.717</td> <td> 0.000</td> <td>-5.75e-20</td> <td> -4.6e-20</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_avg_max</th>                             <td> -6.38e-10</td> <td> 2.57e-10</td> <td>   -2.485</td> <td> 0.013</td> <td>-1.14e-09</td> <td>-1.35e-10</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_avg_max, 4)</th>                <td> 3.829e-20</td> <td> 2.02e-21</td> <td>   19.003</td> <td> 0.000</td> <td> 3.43e-20</td> <td> 4.22e-20</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_min_avg</th>                             <td>-1.642e-12</td> <td> 6.61e-13</td> <td>   -2.485</td> <td> 0.013</td> <td>-2.94e-12</td> <td>-3.47e-13</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_min_avg, 4)</th>                <td> 4.346e-11</td> <td>  1.5e-12</td> <td>   28.986</td> <td> 0.000</td> <td> 4.05e-11</td> <td> 4.64e-11</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_max_avg</th>                             <td>-1.486e-11</td> <td> 5.98e-12</td> <td>   -2.485</td> <td> 0.013</td> <td>-2.66e-11</td> <td>-3.14e-12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_max_avg, 4)</th>                <td> 2.199e-17</td> <td> 1.12e-17</td> <td>    1.963</td> <td> 0.050</td> <td> 3.47e-20</td> <td> 4.39e-17</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>self_reference_min_shares</th>              <td>-1.019e-11</td> <td>  4.1e-12</td> <td>   -2.485</td> <td> 0.013</td> <td>-1.82e-11</td> <td>-2.15e-12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(self_reference_min_shares, 4)</th> <td>-2.682e-23</td> <td> 1.03e-20</td> <td>   -0.003</td> <td> 0.998</td> <td>-2.01e-20</td> <td> 2.01e-20</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>self_reference_max_shares</th>              <td>-1.857e-11</td> <td> 7.47e-12</td> <td>   -2.485</td> <td> 0.013</td> <td>-3.32e-11</td> <td>-3.92e-12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(self_reference_max_shares, 4)</th> <td> 7.492e-21</td> <td> 3.57e-21</td> <td>    2.101</td> <td> 0.036</td> <td> 5.01e-22</td> <td> 1.45e-20</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LDA_00</th>                                 <td>-5.477e-16</td> <td>  2.2e-16</td> <td>   -2.485</td> <td> 0.013</td> <td> -9.8e-16</td> <td>-1.16e-16</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(LDA_00, 4)</th>                    <td>-1.728e-16</td> <td> 6.95e-17</td> <td>   -2.485</td> <td> 0.013</td> <td>-3.09e-16</td> <td>-3.65e-17</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LDA_01</th>                                 <td>-2.165e-16</td> <td> 8.71e-17</td> <td>   -2.485</td> <td> 0.013</td> <td>-3.87e-16</td> <td>-4.58e-17</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(LDA_01, 4)</th>                    <td> 3.536e-17</td> <td> 1.42e-17</td> <td>    2.485</td> <td> 0.013</td> <td> 7.47e-18</td> <td> 6.33e-17</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LDA_02</th>                                 <td>-7.785e-16</td> <td> 3.13e-16</td> <td>   -2.485</td> <td> 0.013</td> <td>-1.39e-15</td> <td>-1.64e-16</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(LDA_02, 4)</th>                    <td>-2.695e-16</td> <td> 1.08e-16</td> <td>   -2.485</td> <td> 0.013</td> <td>-4.82e-16</td> <td> -5.7e-17</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LDA_03</th>                                 <td>-4.178e-16</td> <td> 1.68e-16</td> <td>   -2.485</td> <td> 0.013</td> <td>-7.47e-16</td> <td>-8.83e-17</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(LDA_03, 4)</th>                    <td>-1.239e-16</td> <td> 4.98e-17</td> <td>   -2.485</td> <td> 0.013</td> <td>-2.22e-16</td> <td>-2.62e-17</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LDA_04</th>                                 <td>-8.359e-16</td> <td> 3.36e-16</td> <td>   -2.485</td> <td> 0.013</td> <td> -1.5e-15</td> <td>-1.77e-16</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(LDA_04, 4)</th>                    <td>-2.924e-16</td> <td> 1.18e-16</td> <td>   -2.485</td> <td> 0.013</td> <td>-5.23e-16</td> <td>-6.18e-17</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>global_subjectivity</th>                    <td>-1.248e-15</td> <td> 5.02e-16</td> <td>   -2.485</td> <td> 0.013</td> <td>-2.23e-15</td> <td>-2.64e-16</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(global_subjectivity, 4)</th>       <td> -1.38e-16</td> <td> 5.55e-17</td> <td>   -2.485</td> <td> 0.013</td> <td>-2.47e-16</td> <td>-2.92e-17</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>title_sentiment_polarity</th>               <td>-1.949e-16</td> <td> 7.84e-17</td> <td>   -2.485</td> <td> 0.013</td> <td>-3.49e-16</td> <td>-4.12e-17</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(title_sentiment_polarity, 4)</th>  <td>-8.942e-17</td> <td>  3.6e-17</td> <td>   -2.485</td> <td> 0.013</td> <td> -1.6e-16</td> <td>-1.89e-17</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>abs_title_subjectivity</th>                 <td> -9.28e-16</td> <td> 3.73e-16</td> <td>   -2.485</td> <td> 0.013</td> <td>-1.66e-15</td> <td>-1.96e-16</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(abs_title_subjectivity, 4)</th>    <td> -9.38e-17</td> <td> 3.77e-17</td> <td>   -2.485</td> <td> 0.013</td> <td>-1.68e-16</td> <td>-1.98e-17</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>data_channel</th>                           <td>-1.268e-14</td> <td>  5.1e-15</td> <td>   -2.485</td> <td> 0.013</td> <td>-2.27e-14</td> <td>-2.68e-15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(data_channel, 4)</th>              <td>-2.089e-12</td> <td>  8.4e-13</td> <td>   -2.485</td> <td> 0.013</td> <td>-3.74e-12</td> <td>-4.41e-13</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>36794.526</td> <th>  Durbin-Watson:     </th>   <td>   1.846</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>13422133.867</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 8.546</td>   <th>  Prob(JB):          </th>   <td>    0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>113.521</td>  <th>  Cond. No.          </th>   <td>1.99e+16</td>  \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.99e+16. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                            &      shares      & \\textbf{  R-squared:         } &     -0.102    \\\\\n",
       "\\textbf{Model:}                                    &       OLS        & \\textbf{  Adj. R-squared:    } &     -0.102    \\\\\n",
       "\\textbf{Method:}                                   &  Least Squares   & \\textbf{  F-statistic:       } &     -339.7    \\\\\n",
       "\\textbf{Date:}                                     & Fri, 11 Aug 2023 & \\textbf{  Prob (F-statistic):} &      1.00     \\\\\n",
       "\\textbf{Time:}                                     &     11:34:53     & \\textbf{  Log-Likelihood:    } & -2.6606e+05   \\\\\n",
       "\\textbf{No. Observations:}                         &       25756      & \\textbf{  AIC:               } &  5.321e+05    \\\\\n",
       "\\textbf{Df Residuals:}                             &       25748      & \\textbf{  BIC:               } &  5.322e+05    \\\\\n",
       "\\textbf{Df Model:}                                 &           7      & \\textbf{                     } &               \\\\\n",
       "\\textbf{Covariance Type:}                          &    nonrobust     & \\textbf{                     } &               \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                                   & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                                 &    1.785e-09  &     7.18e-10     &     2.486  &         0.013        &     3.77e-10    &     3.19e-09     \\\\\n",
       "\\textbf{n\\_tokens\\_title}                          &   -1.318e-09  &      5.3e-10     &    -2.485  &         0.013        &    -2.36e-09    &    -2.78e-10     \\\\\n",
       "\\textbf{np.power(n\\_tokens\\_title, 4)}             &   -9.008e-10  &     3.62e-10     &    -2.485  &         0.013        &    -1.61e-09    &     -1.9e-10     \\\\\n",
       "\\textbf{n\\_non\\_stop\\_unique\\_tokens}              &   -1.859e-10  &     7.49e-11     &    -2.483  &         0.013        &    -3.33e-10    &    -3.91e-11     \\\\\n",
       "\\textbf{np.power(n\\_non\\_stop\\_unique\\_tokens, 4)} &    2.219e-08  &     4.15e-08     &     0.534  &         0.593        &    -5.92e-08    &     1.04e-07     \\\\\n",
       "\\textbf{num\\_hrefs}                                &    1.471e-12  &     5.92e-13     &     2.485  &         0.013        &     3.11e-13    &     2.63e-12     \\\\\n",
       "\\textbf{np.power(num\\_hrefs, 4)}                   &    2.073e-06  &     8.34e-07     &     2.485  &         0.013        &     4.38e-07    &     3.71e-06     \\\\\n",
       "\\textbf{num\\_self\\_hrefs}                          &   -4.922e-15  &     1.98e-15     &    -2.485  &         0.013        &    -8.81e-15    &    -1.04e-15     \\\\\n",
       "\\textbf{np.power(num\\_self\\_hrefs, 4)}             &    1.174e-09  &     4.72e-10     &     2.485  &         0.013        &     2.48e-10    &      2.1e-09     \\\\\n",
       "\\textbf{num\\_imgs}                                 &   -5.302e-15  &     2.14e-15     &    -2.482  &         0.013        &    -9.49e-15    &    -1.12e-15     \\\\\n",
       "\\textbf{np.power(num\\_imgs, 4)}                    &    1.229e-09  &     4.94e-10     &     2.487  &         0.013        &      2.6e-10    &      2.2e-09     \\\\\n",
       "\\textbf{average\\_token\\_length}                    &   -1.259e-14  &     5.07e-15     &    -2.485  &         0.013        &    -2.25e-14    &    -2.66e-15     \\\\\n",
       "\\textbf{np.power(average\\_token\\_length, 4)}       &   -1.047e-12  &     4.21e-13     &    -2.485  &         0.013        &    -1.87e-12    &    -2.21e-13     \\\\\n",
       "\\textbf{num\\_keywords}                             &   -2.137e-14  &      8.6e-15     &    -2.485  &         0.013        &    -3.82e-14    &    -4.51e-15     \\\\\n",
       "\\textbf{np.power(num\\_keywords, 4)}                &   -1.215e-11  &     4.89e-12     &    -2.485  &         0.013        &    -2.17e-11    &    -2.57e-12     \\\\\n",
       "\\textbf{kw\\_max\\_min}                              &   -3.475e-12  &      1.4e-12     &    -2.485  &         0.013        &    -6.22e-12    &    -7.34e-13     \\\\\n",
       "\\textbf{np.power(kw\\_max\\_min, 4)}                 &   -2.001e-17  &     1.12e-17     &    -1.780  &         0.075        &     -4.2e-17    &     2.02e-18     \\\\\n",
       "\\textbf{kw\\_min\\_max}                              &   -2.016e-11  &     8.11e-12     &    -2.485  &         0.013        &    -3.61e-11    &    -4.26e-12     \\\\\n",
       "\\textbf{np.power(kw\\_min\\_max, 4)}                 &   -5.178e-20  &     2.92e-21     &   -17.717  &         0.000        &    -5.75e-20    &     -4.6e-20     \\\\\n",
       "\\textbf{kw\\_avg\\_max}                              &    -6.38e-10  &     2.57e-10     &    -2.485  &         0.013        &    -1.14e-09    &    -1.35e-10     \\\\\n",
       "\\textbf{np.power(kw\\_avg\\_max, 4)}                 &    3.829e-20  &     2.02e-21     &    19.003  &         0.000        &     3.43e-20    &     4.22e-20     \\\\\n",
       "\\textbf{kw\\_min\\_avg}                              &   -1.642e-12  &     6.61e-13     &    -2.485  &         0.013        &    -2.94e-12    &    -3.47e-13     \\\\\n",
       "\\textbf{np.power(kw\\_min\\_avg, 4)}                 &    4.346e-11  &      1.5e-12     &    28.986  &         0.000        &     4.05e-11    &     4.64e-11     \\\\\n",
       "\\textbf{kw\\_max\\_avg}                              &   -1.486e-11  &     5.98e-12     &    -2.485  &         0.013        &    -2.66e-11    &    -3.14e-12     \\\\\n",
       "\\textbf{np.power(kw\\_max\\_avg, 4)}                 &    2.199e-17  &     1.12e-17     &     1.963  &         0.050        &     3.47e-20    &     4.39e-17     \\\\\n",
       "\\textbf{self\\_reference\\_min\\_shares}              &   -1.019e-11  &      4.1e-12     &    -2.485  &         0.013        &    -1.82e-11    &    -2.15e-12     \\\\\n",
       "\\textbf{np.power(self\\_reference\\_min\\_shares, 4)} &   -2.682e-23  &     1.03e-20     &    -0.003  &         0.998        &    -2.01e-20    &     2.01e-20     \\\\\n",
       "\\textbf{self\\_reference\\_max\\_shares}              &   -1.857e-11  &     7.47e-12     &    -2.485  &         0.013        &    -3.32e-11    &    -3.92e-12     \\\\\n",
       "\\textbf{np.power(self\\_reference\\_max\\_shares, 4)} &    7.492e-21  &     3.57e-21     &     2.101  &         0.036        &     5.01e-22    &     1.45e-20     \\\\\n",
       "\\textbf{LDA\\_00}                                   &   -5.477e-16  &      2.2e-16     &    -2.485  &         0.013        &     -9.8e-16    &    -1.16e-16     \\\\\n",
       "\\textbf{np.power(LDA\\_00, 4)}                      &   -1.728e-16  &     6.95e-17     &    -2.485  &         0.013        &    -3.09e-16    &    -3.65e-17     \\\\\n",
       "\\textbf{LDA\\_01}                                   &   -2.165e-16  &     8.71e-17     &    -2.485  &         0.013        &    -3.87e-16    &    -4.58e-17     \\\\\n",
       "\\textbf{np.power(LDA\\_01, 4)}                      &    3.536e-17  &     1.42e-17     &     2.485  &         0.013        &     7.47e-18    &     6.33e-17     \\\\\n",
       "\\textbf{LDA\\_02}                                   &   -7.785e-16  &     3.13e-16     &    -2.485  &         0.013        &    -1.39e-15    &    -1.64e-16     \\\\\n",
       "\\textbf{np.power(LDA\\_02, 4)}                      &   -2.695e-16  &     1.08e-16     &    -2.485  &         0.013        &    -4.82e-16    &     -5.7e-17     \\\\\n",
       "\\textbf{LDA\\_03}                                   &   -4.178e-16  &     1.68e-16     &    -2.485  &         0.013        &    -7.47e-16    &    -8.83e-17     \\\\\n",
       "\\textbf{np.power(LDA\\_03, 4)}                      &   -1.239e-16  &     4.98e-17     &    -2.485  &         0.013        &    -2.22e-16    &    -2.62e-17     \\\\\n",
       "\\textbf{LDA\\_04}                                   &   -8.359e-16  &     3.36e-16     &    -2.485  &         0.013        &     -1.5e-15    &    -1.77e-16     \\\\\n",
       "\\textbf{np.power(LDA\\_04, 4)}                      &   -2.924e-16  &     1.18e-16     &    -2.485  &         0.013        &    -5.23e-16    &    -6.18e-17     \\\\\n",
       "\\textbf{global\\_subjectivity}                      &   -1.248e-15  &     5.02e-16     &    -2.485  &         0.013        &    -2.23e-15    &    -2.64e-16     \\\\\n",
       "\\textbf{np.power(global\\_subjectivity, 4)}         &    -1.38e-16  &     5.55e-17     &    -2.485  &         0.013        &    -2.47e-16    &    -2.92e-17     \\\\\n",
       "\\textbf{title\\_sentiment\\_polarity}                &   -1.949e-16  &     7.84e-17     &    -2.485  &         0.013        &    -3.49e-16    &    -4.12e-17     \\\\\n",
       "\\textbf{np.power(title\\_sentiment\\_polarity, 4)}   &   -8.942e-17  &      3.6e-17     &    -2.485  &         0.013        &     -1.6e-16    &    -1.89e-17     \\\\\n",
       "\\textbf{abs\\_title\\_subjectivity}                  &    -9.28e-16  &     3.73e-16     &    -2.485  &         0.013        &    -1.66e-15    &    -1.96e-16     \\\\\n",
       "\\textbf{np.power(abs\\_title\\_subjectivity, 4)}     &    -9.38e-17  &     3.77e-17     &    -2.485  &         0.013        &    -1.68e-16    &    -1.98e-17     \\\\\n",
       "\\textbf{data\\_channel}                             &   -1.268e-14  &      5.1e-15     &    -2.485  &         0.013        &    -2.27e-14    &    -2.68e-15     \\\\\n",
       "\\textbf{np.power(data\\_channel, 4)}                &   -2.089e-12  &      8.4e-13     &    -2.485  &         0.013        &    -3.74e-12    &    -4.41e-13     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 36794.526 & \\textbf{  Durbin-Watson:     } &      1.846    \\\\\n",
       "\\textbf{Prob(Omnibus):} &    0.000  & \\textbf{  Jarque-Bera (JB):  } & 13422133.867  \\\\\n",
       "\\textbf{Skew:}          &    8.546  & \\textbf{  Prob(JB):          } &       0.00    \\\\\n",
       "\\textbf{Kurtosis:}      &  113.521  & \\textbf{  Cond. No.          } &   1.99e+16    \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 1.99e+16. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 shares   R-squared:                      -0.102\n",
       "Model:                            OLS   Adj. R-squared:                 -0.102\n",
       "Method:                 Least Squares   F-statistic:                    -339.7\n",
       "Date:                Fri, 11 Aug 2023   Prob (F-statistic):               1.00\n",
       "Time:                        11:34:53   Log-Likelihood:            -2.6606e+05\n",
       "No. Observations:               25756   AIC:                         5.321e+05\n",
       "Df Residuals:                   25748   BIC:                         5.322e+05\n",
       "Df Model:                           7                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==========================================================================================================\n",
       "                                             coef    std err          t      P>|t|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------------------------------\n",
       "Intercept                               1.785e-09   7.18e-10      2.486      0.013    3.77e-10    3.19e-09\n",
       "n_tokens_title                         -1.318e-09    5.3e-10     -2.485      0.013   -2.36e-09   -2.78e-10\n",
       "np.power(n_tokens_title, 4)            -9.008e-10   3.62e-10     -2.485      0.013   -1.61e-09    -1.9e-10\n",
       "n_non_stop_unique_tokens               -1.859e-10   7.49e-11     -2.483      0.013   -3.33e-10   -3.91e-11\n",
       "np.power(n_non_stop_unique_tokens, 4)   2.219e-08   4.15e-08      0.534      0.593   -5.92e-08    1.04e-07\n",
       "num_hrefs                               1.471e-12   5.92e-13      2.485      0.013    3.11e-13    2.63e-12\n",
       "np.power(num_hrefs, 4)                  2.073e-06   8.34e-07      2.485      0.013    4.38e-07    3.71e-06\n",
       "num_self_hrefs                         -4.922e-15   1.98e-15     -2.485      0.013   -8.81e-15   -1.04e-15\n",
       "np.power(num_self_hrefs, 4)             1.174e-09   4.72e-10      2.485      0.013    2.48e-10     2.1e-09\n",
       "num_imgs                               -5.302e-15   2.14e-15     -2.482      0.013   -9.49e-15   -1.12e-15\n",
       "np.power(num_imgs, 4)                   1.229e-09   4.94e-10      2.487      0.013     2.6e-10     2.2e-09\n",
       "average_token_length                   -1.259e-14   5.07e-15     -2.485      0.013   -2.25e-14   -2.66e-15\n",
       "np.power(average_token_length, 4)      -1.047e-12   4.21e-13     -2.485      0.013   -1.87e-12   -2.21e-13\n",
       "num_keywords                           -2.137e-14    8.6e-15     -2.485      0.013   -3.82e-14   -4.51e-15\n",
       "np.power(num_keywords, 4)              -1.215e-11   4.89e-12     -2.485      0.013   -2.17e-11   -2.57e-12\n",
       "kw_max_min                             -3.475e-12    1.4e-12     -2.485      0.013   -6.22e-12   -7.34e-13\n",
       "np.power(kw_max_min, 4)                -2.001e-17   1.12e-17     -1.780      0.075    -4.2e-17    2.02e-18\n",
       "kw_min_max                             -2.016e-11   8.11e-12     -2.485      0.013   -3.61e-11   -4.26e-12\n",
       "np.power(kw_min_max, 4)                -5.178e-20   2.92e-21    -17.717      0.000   -5.75e-20    -4.6e-20\n",
       "kw_avg_max                              -6.38e-10   2.57e-10     -2.485      0.013   -1.14e-09   -1.35e-10\n",
       "np.power(kw_avg_max, 4)                 3.829e-20   2.02e-21     19.003      0.000    3.43e-20    4.22e-20\n",
       "kw_min_avg                             -1.642e-12   6.61e-13     -2.485      0.013   -2.94e-12   -3.47e-13\n",
       "np.power(kw_min_avg, 4)                 4.346e-11    1.5e-12     28.986      0.000    4.05e-11    4.64e-11\n",
       "kw_max_avg                             -1.486e-11   5.98e-12     -2.485      0.013   -2.66e-11   -3.14e-12\n",
       "np.power(kw_max_avg, 4)                 2.199e-17   1.12e-17      1.963      0.050    3.47e-20    4.39e-17\n",
       "self_reference_min_shares              -1.019e-11    4.1e-12     -2.485      0.013   -1.82e-11   -2.15e-12\n",
       "np.power(self_reference_min_shares, 4) -2.682e-23   1.03e-20     -0.003      0.998   -2.01e-20    2.01e-20\n",
       "self_reference_max_shares              -1.857e-11   7.47e-12     -2.485      0.013   -3.32e-11   -3.92e-12\n",
       "np.power(self_reference_max_shares, 4)  7.492e-21   3.57e-21      2.101      0.036    5.01e-22    1.45e-20\n",
       "LDA_00                                 -5.477e-16    2.2e-16     -2.485      0.013    -9.8e-16   -1.16e-16\n",
       "np.power(LDA_00, 4)                    -1.728e-16   6.95e-17     -2.485      0.013   -3.09e-16   -3.65e-17\n",
       "LDA_01                                 -2.165e-16   8.71e-17     -2.485      0.013   -3.87e-16   -4.58e-17\n",
       "np.power(LDA_01, 4)                     3.536e-17   1.42e-17      2.485      0.013    7.47e-18    6.33e-17\n",
       "LDA_02                                 -7.785e-16   3.13e-16     -2.485      0.013   -1.39e-15   -1.64e-16\n",
       "np.power(LDA_02, 4)                    -2.695e-16   1.08e-16     -2.485      0.013   -4.82e-16    -5.7e-17\n",
       "LDA_03                                 -4.178e-16   1.68e-16     -2.485      0.013   -7.47e-16   -8.83e-17\n",
       "np.power(LDA_03, 4)                    -1.239e-16   4.98e-17     -2.485      0.013   -2.22e-16   -2.62e-17\n",
       "LDA_04                                 -8.359e-16   3.36e-16     -2.485      0.013    -1.5e-15   -1.77e-16\n",
       "np.power(LDA_04, 4)                    -2.924e-16   1.18e-16     -2.485      0.013   -5.23e-16   -6.18e-17\n",
       "global_subjectivity                    -1.248e-15   5.02e-16     -2.485      0.013   -2.23e-15   -2.64e-16\n",
       "np.power(global_subjectivity, 4)        -1.38e-16   5.55e-17     -2.485      0.013   -2.47e-16   -2.92e-17\n",
       "title_sentiment_polarity               -1.949e-16   7.84e-17     -2.485      0.013   -3.49e-16   -4.12e-17\n",
       "np.power(title_sentiment_polarity, 4)  -8.942e-17    3.6e-17     -2.485      0.013    -1.6e-16   -1.89e-17\n",
       "abs_title_subjectivity                  -9.28e-16   3.73e-16     -2.485      0.013   -1.66e-15   -1.96e-16\n",
       "np.power(abs_title_subjectivity, 4)     -9.38e-17   3.77e-17     -2.485      0.013   -1.68e-16   -1.98e-17\n",
       "data_channel                           -1.268e-14    5.1e-15     -2.485      0.013   -2.27e-14   -2.68e-15\n",
       "np.power(data_channel, 4)              -2.089e-12    8.4e-13     -2.485      0.013   -3.74e-12   -4.41e-13\n",
       "==============================================================================\n",
       "Omnibus:                    36794.526   Durbin-Watson:                   1.846\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):         13422133.867\n",
       "Skew:                           8.546   Prob(JB):                         0.00\n",
       "Kurtosis:                     113.521   Cond. No.                     1.99e+16\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.99e+16. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "degree = 4\n",
    "polynomial_regression_summary(train_df, significant_predictors, 'shares', degree).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fa9eb798-e062-49b9-aa93-ed3eefd75b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>shares</td>      <th>  R-squared:         </th>  <td>  -0.120</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>  -0.120</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>  -459.2</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 11 Aug 2023</td> <th>  Prob (F-statistic):</th>   <td>  1.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:34:53</td>     <th>  Log-Likelihood:    </th> <td>-2.6627e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 25756</td>      <th>  AIC:               </th>  <td>5.326e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 25749</td>      <th>  BIC:               </th>  <td>5.326e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     6</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                     <td></td>                       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                              <td> 9.602e-20</td> <td> 3.18e-21</td> <td>   30.188</td> <td> 0.000</td> <td> 8.98e-20</td> <td> 1.02e-19</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n_tokens_title</th>                         <td>-1.973e-19</td> <td> 6.54e-21</td> <td>  -30.188</td> <td> 0.000</td> <td> -2.1e-19</td> <td>-1.84e-19</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(n_tokens_title, 5)</th>            <td> 4.972e-20</td> <td> 1.65e-21</td> <td>   30.188</td> <td> 0.000</td> <td> 4.65e-20</td> <td>  5.3e-20</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n_non_stop_unique_tokens</th>               <td>-6.949e-20</td> <td>  2.3e-21</td> <td>  -30.188</td> <td> 0.000</td> <td> -7.4e-20</td> <td> -6.5e-20</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(n_non_stop_unique_tokens, 5)</th>  <td> 1.419e-21</td> <td>  4.7e-23</td> <td>   30.188</td> <td> 0.000</td> <td> 1.33e-21</td> <td> 1.51e-21</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_hrefs</th>                              <td> 1.674e-23</td> <td> 5.55e-25</td> <td>   30.188</td> <td> 0.000</td> <td> 1.57e-23</td> <td> 1.78e-23</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_hrefs, 5)</th>                 <td> 8.371e-23</td> <td> 2.77e-24</td> <td>   30.188</td> <td> 0.000</td> <td> 7.83e-23</td> <td> 8.92e-23</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_self_hrefs</th>                         <td> 2.084e-43</td> <td> 1.14e-43</td> <td>    1.827</td> <td> 0.068</td> <td>-1.52e-44</td> <td> 4.32e-43</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_self_hrefs, 5)</th>            <td> 6.314e-25</td> <td> 2.09e-26</td> <td>   30.188</td> <td> 0.000</td> <td>  5.9e-25</td> <td> 6.72e-25</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_imgs</th>                               <td> 1.983e-30</td> <td> 6.57e-32</td> <td>   30.188</td> <td> 0.000</td> <td> 1.85e-30</td> <td> 2.11e-30</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_imgs, 5)</th>                  <td> 5.026e-24</td> <td> 1.67e-25</td> <td>   30.188</td> <td> 0.000</td> <td>  4.7e-24</td> <td> 5.35e-24</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>average_token_length</th>                   <td> 2.038e-30</td> <td> 6.75e-32</td> <td>   30.188</td> <td> 0.000</td> <td> 1.91e-30</td> <td> 2.17e-30</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(average_token_length, 5)</th>      <td> 1.024e-27</td> <td> 3.39e-29</td> <td>   30.188</td> <td> 0.000</td> <td> 9.57e-28</td> <td> 1.09e-27</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_keywords</th>                           <td> 3.218e-30</td> <td> 1.07e-31</td> <td>   30.188</td> <td> 0.000</td> <td> 3.01e-30</td> <td> 3.43e-30</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_keywords, 5)</th>              <td> 1.578e-26</td> <td> 5.23e-28</td> <td>   30.188</td> <td> 0.000</td> <td> 1.48e-26</td> <td> 1.68e-26</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_max_min</th>                             <td> 5.144e-28</td> <td>  1.7e-29</td> <td>   30.188</td> <td> 0.000</td> <td> 4.81e-28</td> <td> 5.48e-28</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_max_min, 5)</th>                <td>-1.277e-22</td> <td> 9.53e-23</td> <td>   -1.340</td> <td> 0.180</td> <td>-3.14e-22</td> <td> 5.91e-23</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_min_max</th>                             <td>  1.92e-27</td> <td> 6.36e-29</td> <td>   30.188</td> <td> 0.000</td> <td>  1.8e-27</td> <td> 2.04e-27</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_min_max, 5)</th>                <td>-6.777e-26</td> <td> 4.43e-27</td> <td>  -15.288</td> <td> 0.000</td> <td>-7.65e-26</td> <td>-5.91e-26</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_avg_max</th>                             <td> 9.545e-26</td> <td> 3.16e-27</td> <td>   30.188</td> <td> 0.000</td> <td> 8.93e-26</td> <td> 1.02e-25</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_avg_max, 5)</th>                <td> 4.957e-26</td> <td>  3.4e-27</td> <td>   14.574</td> <td> 0.000</td> <td> 4.29e-26</td> <td> 5.62e-26</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_min_avg</th>                             <td> 4.231e-28</td> <td>  1.4e-29</td> <td>   30.188</td> <td> 0.000</td> <td> 3.96e-28</td> <td> 4.51e-28</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_min_avg, 5)</th>                <td> 1.381e-14</td> <td> 4.58e-16</td> <td>   30.188</td> <td> 0.000</td> <td> 1.29e-14</td> <td> 1.47e-14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_max_avg</th>                             <td> 2.316e-27</td> <td> 7.67e-29</td> <td>   30.188</td> <td> 0.000</td> <td> 2.17e-27</td> <td> 2.47e-27</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_max_avg, 5)</th>                <td> 1.346e-22</td> <td> 9.53e-23</td> <td>    1.413</td> <td> 0.158</td> <td>-5.21e-23</td> <td> 3.21e-22</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>self_reference_min_shares</th>              <td> 1.532e-27</td> <td> 5.08e-29</td> <td>   30.188</td> <td> 0.000</td> <td> 1.43e-27</td> <td> 1.63e-27</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(self_reference_min_shares, 5)</th> <td> 2.923e-27</td> <td> 1.54e-26</td> <td>    0.190</td> <td> 0.849</td> <td>-2.72e-26</td> <td>  3.3e-26</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>self_reference_max_shares</th>              <td>  3.61e-27</td> <td>  1.2e-28</td> <td>   30.188</td> <td> 0.000</td> <td> 3.38e-27</td> <td> 3.84e-27</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(self_reference_max_shares, 5)</th> <td>  1.13e-26</td> <td> 4.75e-27</td> <td>    2.378</td> <td> 0.017</td> <td> 1.99e-27</td> <td> 2.06e-26</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LDA_00</th>                                 <td>  7.77e-32</td> <td> 2.57e-33</td> <td>   30.188</td> <td> 0.000</td> <td> 7.27e-32</td> <td> 8.27e-32</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(LDA_00, 5)</th>                    <td> 1.872e-32</td> <td>  6.2e-34</td> <td>   30.188</td> <td> 0.000</td> <td> 1.75e-32</td> <td> 1.99e-32</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LDA_01</th>                                 <td> 6.376e-32</td> <td> 2.11e-33</td> <td>   30.188</td> <td> 0.000</td> <td> 5.96e-32</td> <td> 6.79e-32</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(LDA_01, 5)</th>                    <td> 1.262e-32</td> <td> 4.18e-34</td> <td>   30.188</td> <td> 0.000</td> <td> 1.18e-32</td> <td> 1.34e-32</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LDA_02</th>                                 <td>  1.01e-31</td> <td> 3.35e-33</td> <td>   30.188</td> <td> 0.000</td> <td> 9.44e-32</td> <td> 1.08e-31</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(LDA_02, 5)</th>                    <td> 2.728e-32</td> <td> 9.04e-34</td> <td>   30.188</td> <td> 0.000</td> <td> 2.55e-32</td> <td>  2.9e-32</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LDA_03</th>                                 <td> 8.223e-32</td> <td> 2.72e-33</td> <td>   30.188</td> <td> 0.000</td> <td> 7.69e-32</td> <td> 8.76e-32</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(LDA_03, 5)</th>                    <td> 2.172e-32</td> <td>  7.2e-34</td> <td>   30.188</td> <td> 0.000</td> <td> 2.03e-32</td> <td> 2.31e-32</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LDA_04</th>                                 <td> 1.098e-31</td> <td> 3.64e-33</td> <td>   30.188</td> <td> 0.000</td> <td> 1.03e-31</td> <td> 1.17e-31</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(LDA_04, 5)</th>                    <td>  3.03e-32</td> <td>    1e-33</td> <td>   30.188</td> <td> 0.000</td> <td> 2.83e-32</td> <td> 3.23e-32</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>global_subjectivity</th>                    <td> 1.973e-31</td> <td> 6.54e-33</td> <td>   30.188</td> <td> 0.000</td> <td> 1.84e-31</td> <td>  2.1e-31</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(global_subjectivity, 5)</th>       <td> 1.171e-32</td> <td> 3.88e-34</td> <td>   30.188</td> <td> 0.000</td> <td>  1.1e-32</td> <td> 1.25e-32</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>title_sentiment_polarity</th>               <td> 3.127e-32</td> <td> 1.04e-33</td> <td>   30.188</td> <td> 0.000</td> <td> 2.92e-32</td> <td> 3.33e-32</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(title_sentiment_polarity, 5)</th>  <td> 6.115e-33</td> <td> 2.03e-34</td> <td>   30.188</td> <td> 0.000</td> <td> 5.72e-33</td> <td> 6.51e-33</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>abs_title_subjectivity</th>                 <td> 1.493e-31</td> <td> 4.94e-33</td> <td>   30.188</td> <td> 0.000</td> <td>  1.4e-31</td> <td> 1.59e-31</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(abs_title_subjectivity, 5)</th>    <td> 7.547e-33</td> <td>  2.5e-34</td> <td>   30.188</td> <td> 0.000</td> <td> 7.06e-33</td> <td> 8.04e-33</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>data_channel</th>                           <td> 1.863e-30</td> <td> 6.17e-32</td> <td>   30.188</td> <td> 0.000</td> <td> 1.74e-30</td> <td> 1.98e-30</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(data_channel, 5)</th>              <td> 1.889e-27</td> <td> 6.26e-29</td> <td>   30.188</td> <td> 0.000</td> <td> 1.77e-27</td> <td> 2.01e-27</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>36988.998</td> <th>  Durbin-Watson:     </th>   <td>   1.808</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>13743321.041</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 8.630</td>   <th>  Prob(JB):          </th>   <td>    0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>114.841</td>  <th>  Cond. No.          </th>   <td>1.25e+16</td>  \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.25e+16. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                            &      shares      & \\textbf{  R-squared:         } &     -0.120    \\\\\n",
       "\\textbf{Model:}                                    &       OLS        & \\textbf{  Adj. R-squared:    } &     -0.120    \\\\\n",
       "\\textbf{Method:}                                   &  Least Squares   & \\textbf{  F-statistic:       } &     -459.2    \\\\\n",
       "\\textbf{Date:}                                     & Fri, 11 Aug 2023 & \\textbf{  Prob (F-statistic):} &      1.00     \\\\\n",
       "\\textbf{Time:}                                     &     11:34:53     & \\textbf{  Log-Likelihood:    } & -2.6627e+05   \\\\\n",
       "\\textbf{No. Observations:}                         &       25756      & \\textbf{  AIC:               } &  5.326e+05    \\\\\n",
       "\\textbf{Df Residuals:}                             &       25749      & \\textbf{  BIC:               } &  5.326e+05    \\\\\n",
       "\\textbf{Df Model:}                                 &           6      & \\textbf{                     } &               \\\\\n",
       "\\textbf{Covariance Type:}                          &    nonrobust     & \\textbf{                     } &               \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                                   & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                                 &    9.602e-20  &     3.18e-21     &    30.188  &         0.000        &     8.98e-20    &     1.02e-19     \\\\\n",
       "\\textbf{n\\_tokens\\_title}                          &   -1.973e-19  &     6.54e-21     &   -30.188  &         0.000        &     -2.1e-19    &    -1.84e-19     \\\\\n",
       "\\textbf{np.power(n\\_tokens\\_title, 5)}             &    4.972e-20  &     1.65e-21     &    30.188  &         0.000        &     4.65e-20    &      5.3e-20     \\\\\n",
       "\\textbf{n\\_non\\_stop\\_unique\\_tokens}              &   -6.949e-20  &      2.3e-21     &   -30.188  &         0.000        &     -7.4e-20    &     -6.5e-20     \\\\\n",
       "\\textbf{np.power(n\\_non\\_stop\\_unique\\_tokens, 5)} &    1.419e-21  &      4.7e-23     &    30.188  &         0.000        &     1.33e-21    &     1.51e-21     \\\\\n",
       "\\textbf{num\\_hrefs}                                &    1.674e-23  &     5.55e-25     &    30.188  &         0.000        &     1.57e-23    &     1.78e-23     \\\\\n",
       "\\textbf{np.power(num\\_hrefs, 5)}                   &    8.371e-23  &     2.77e-24     &    30.188  &         0.000        &     7.83e-23    &     8.92e-23     \\\\\n",
       "\\textbf{num\\_self\\_hrefs}                          &    2.084e-43  &     1.14e-43     &     1.827  &         0.068        &    -1.52e-44    &     4.32e-43     \\\\\n",
       "\\textbf{np.power(num\\_self\\_hrefs, 5)}             &    6.314e-25  &     2.09e-26     &    30.188  &         0.000        &      5.9e-25    &     6.72e-25     \\\\\n",
       "\\textbf{num\\_imgs}                                 &    1.983e-30  &     6.57e-32     &    30.188  &         0.000        &     1.85e-30    &     2.11e-30     \\\\\n",
       "\\textbf{np.power(num\\_imgs, 5)}                    &    5.026e-24  &     1.67e-25     &    30.188  &         0.000        &      4.7e-24    &     5.35e-24     \\\\\n",
       "\\textbf{average\\_token\\_length}                    &    2.038e-30  &     6.75e-32     &    30.188  &         0.000        &     1.91e-30    &     2.17e-30     \\\\\n",
       "\\textbf{np.power(average\\_token\\_length, 5)}       &    1.024e-27  &     3.39e-29     &    30.188  &         0.000        &     9.57e-28    &     1.09e-27     \\\\\n",
       "\\textbf{num\\_keywords}                             &    3.218e-30  &     1.07e-31     &    30.188  &         0.000        &     3.01e-30    &     3.43e-30     \\\\\n",
       "\\textbf{np.power(num\\_keywords, 5)}                &    1.578e-26  &     5.23e-28     &    30.188  &         0.000        &     1.48e-26    &     1.68e-26     \\\\\n",
       "\\textbf{kw\\_max\\_min}                              &    5.144e-28  &      1.7e-29     &    30.188  &         0.000        &     4.81e-28    &     5.48e-28     \\\\\n",
       "\\textbf{np.power(kw\\_max\\_min, 5)}                 &   -1.277e-22  &     9.53e-23     &    -1.340  &         0.180        &    -3.14e-22    &     5.91e-23     \\\\\n",
       "\\textbf{kw\\_min\\_max}                              &     1.92e-27  &     6.36e-29     &    30.188  &         0.000        &      1.8e-27    &     2.04e-27     \\\\\n",
       "\\textbf{np.power(kw\\_min\\_max, 5)}                 &   -6.777e-26  &     4.43e-27     &   -15.288  &         0.000        &    -7.65e-26    &    -5.91e-26     \\\\\n",
       "\\textbf{kw\\_avg\\_max}                              &    9.545e-26  &     3.16e-27     &    30.188  &         0.000        &     8.93e-26    &     1.02e-25     \\\\\n",
       "\\textbf{np.power(kw\\_avg\\_max, 5)}                 &    4.957e-26  &      3.4e-27     &    14.574  &         0.000        &     4.29e-26    &     5.62e-26     \\\\\n",
       "\\textbf{kw\\_min\\_avg}                              &    4.231e-28  &      1.4e-29     &    30.188  &         0.000        &     3.96e-28    &     4.51e-28     \\\\\n",
       "\\textbf{np.power(kw\\_min\\_avg, 5)}                 &    1.381e-14  &     4.58e-16     &    30.188  &         0.000        &     1.29e-14    &     1.47e-14     \\\\\n",
       "\\textbf{kw\\_max\\_avg}                              &    2.316e-27  &     7.67e-29     &    30.188  &         0.000        &     2.17e-27    &     2.47e-27     \\\\\n",
       "\\textbf{np.power(kw\\_max\\_avg, 5)}                 &    1.346e-22  &     9.53e-23     &     1.413  &         0.158        &    -5.21e-23    &     3.21e-22     \\\\\n",
       "\\textbf{self\\_reference\\_min\\_shares}              &    1.532e-27  &     5.08e-29     &    30.188  &         0.000        &     1.43e-27    &     1.63e-27     \\\\\n",
       "\\textbf{np.power(self\\_reference\\_min\\_shares, 5)} &    2.923e-27  &     1.54e-26     &     0.190  &         0.849        &    -2.72e-26    &      3.3e-26     \\\\\n",
       "\\textbf{self\\_reference\\_max\\_shares}              &     3.61e-27  &      1.2e-28     &    30.188  &         0.000        &     3.38e-27    &     3.84e-27     \\\\\n",
       "\\textbf{np.power(self\\_reference\\_max\\_shares, 5)} &     1.13e-26  &     4.75e-27     &     2.378  &         0.017        &     1.99e-27    &     2.06e-26     \\\\\n",
       "\\textbf{LDA\\_00}                                   &     7.77e-32  &     2.57e-33     &    30.188  &         0.000        &     7.27e-32    &     8.27e-32     \\\\\n",
       "\\textbf{np.power(LDA\\_00, 5)}                      &    1.872e-32  &      6.2e-34     &    30.188  &         0.000        &     1.75e-32    &     1.99e-32     \\\\\n",
       "\\textbf{LDA\\_01}                                   &    6.376e-32  &     2.11e-33     &    30.188  &         0.000        &     5.96e-32    &     6.79e-32     \\\\\n",
       "\\textbf{np.power(LDA\\_01, 5)}                      &    1.262e-32  &     4.18e-34     &    30.188  &         0.000        &     1.18e-32    &     1.34e-32     \\\\\n",
       "\\textbf{LDA\\_02}                                   &     1.01e-31  &     3.35e-33     &    30.188  &         0.000        &     9.44e-32    &     1.08e-31     \\\\\n",
       "\\textbf{np.power(LDA\\_02, 5)}                      &    2.728e-32  &     9.04e-34     &    30.188  &         0.000        &     2.55e-32    &      2.9e-32     \\\\\n",
       "\\textbf{LDA\\_03}                                   &    8.223e-32  &     2.72e-33     &    30.188  &         0.000        &     7.69e-32    &     8.76e-32     \\\\\n",
       "\\textbf{np.power(LDA\\_03, 5)}                      &    2.172e-32  &      7.2e-34     &    30.188  &         0.000        &     2.03e-32    &     2.31e-32     \\\\\n",
       "\\textbf{LDA\\_04}                                   &    1.098e-31  &     3.64e-33     &    30.188  &         0.000        &     1.03e-31    &     1.17e-31     \\\\\n",
       "\\textbf{np.power(LDA\\_04, 5)}                      &     3.03e-32  &        1e-33     &    30.188  &         0.000        &     2.83e-32    &     3.23e-32     \\\\\n",
       "\\textbf{global\\_subjectivity}                      &    1.973e-31  &     6.54e-33     &    30.188  &         0.000        &     1.84e-31    &      2.1e-31     \\\\\n",
       "\\textbf{np.power(global\\_subjectivity, 5)}         &    1.171e-32  &     3.88e-34     &    30.188  &         0.000        &      1.1e-32    &     1.25e-32     \\\\\n",
       "\\textbf{title\\_sentiment\\_polarity}                &    3.127e-32  &     1.04e-33     &    30.188  &         0.000        &     2.92e-32    &     3.33e-32     \\\\\n",
       "\\textbf{np.power(title\\_sentiment\\_polarity, 5)}   &    6.115e-33  &     2.03e-34     &    30.188  &         0.000        &     5.72e-33    &     6.51e-33     \\\\\n",
       "\\textbf{abs\\_title\\_subjectivity}                  &    1.493e-31  &     4.94e-33     &    30.188  &         0.000        &      1.4e-31    &     1.59e-31     \\\\\n",
       "\\textbf{np.power(abs\\_title\\_subjectivity, 5)}     &    7.547e-33  &      2.5e-34     &    30.188  &         0.000        &     7.06e-33    &     8.04e-33     \\\\\n",
       "\\textbf{data\\_channel}                             &    1.863e-30  &     6.17e-32     &    30.188  &         0.000        &     1.74e-30    &     1.98e-30     \\\\\n",
       "\\textbf{np.power(data\\_channel, 5)}                &    1.889e-27  &     6.26e-29     &    30.188  &         0.000        &     1.77e-27    &     2.01e-27     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 36988.998 & \\textbf{  Durbin-Watson:     } &      1.808    \\\\\n",
       "\\textbf{Prob(Omnibus):} &    0.000  & \\textbf{  Jarque-Bera (JB):  } & 13743321.041  \\\\\n",
       "\\textbf{Skew:}          &    8.630  & \\textbf{  Prob(JB):          } &       0.00    \\\\\n",
       "\\textbf{Kurtosis:}      &  114.841  & \\textbf{  Cond. No.          } &   1.25e+16    \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 1.25e+16. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 shares   R-squared:                      -0.120\n",
       "Model:                            OLS   Adj. R-squared:                 -0.120\n",
       "Method:                 Least Squares   F-statistic:                    -459.2\n",
       "Date:                Fri, 11 Aug 2023   Prob (F-statistic):               1.00\n",
       "Time:                        11:34:53   Log-Likelihood:            -2.6627e+05\n",
       "No. Observations:               25756   AIC:                         5.326e+05\n",
       "Df Residuals:                   25749   BIC:                         5.326e+05\n",
       "Df Model:                           6                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==========================================================================================================\n",
       "                                             coef    std err          t      P>|t|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------------------------------\n",
       "Intercept                               9.602e-20   3.18e-21     30.188      0.000    8.98e-20    1.02e-19\n",
       "n_tokens_title                         -1.973e-19   6.54e-21    -30.188      0.000    -2.1e-19   -1.84e-19\n",
       "np.power(n_tokens_title, 5)             4.972e-20   1.65e-21     30.188      0.000    4.65e-20     5.3e-20\n",
       "n_non_stop_unique_tokens               -6.949e-20    2.3e-21    -30.188      0.000    -7.4e-20    -6.5e-20\n",
       "np.power(n_non_stop_unique_tokens, 5)   1.419e-21    4.7e-23     30.188      0.000    1.33e-21    1.51e-21\n",
       "num_hrefs                               1.674e-23   5.55e-25     30.188      0.000    1.57e-23    1.78e-23\n",
       "np.power(num_hrefs, 5)                  8.371e-23   2.77e-24     30.188      0.000    7.83e-23    8.92e-23\n",
       "num_self_hrefs                          2.084e-43   1.14e-43      1.827      0.068   -1.52e-44    4.32e-43\n",
       "np.power(num_self_hrefs, 5)             6.314e-25   2.09e-26     30.188      0.000     5.9e-25    6.72e-25\n",
       "num_imgs                                1.983e-30   6.57e-32     30.188      0.000    1.85e-30    2.11e-30\n",
       "np.power(num_imgs, 5)                   5.026e-24   1.67e-25     30.188      0.000     4.7e-24    5.35e-24\n",
       "average_token_length                    2.038e-30   6.75e-32     30.188      0.000    1.91e-30    2.17e-30\n",
       "np.power(average_token_length, 5)       1.024e-27   3.39e-29     30.188      0.000    9.57e-28    1.09e-27\n",
       "num_keywords                            3.218e-30   1.07e-31     30.188      0.000    3.01e-30    3.43e-30\n",
       "np.power(num_keywords, 5)               1.578e-26   5.23e-28     30.188      0.000    1.48e-26    1.68e-26\n",
       "kw_max_min                              5.144e-28    1.7e-29     30.188      0.000    4.81e-28    5.48e-28\n",
       "np.power(kw_max_min, 5)                -1.277e-22   9.53e-23     -1.340      0.180   -3.14e-22    5.91e-23\n",
       "kw_min_max                               1.92e-27   6.36e-29     30.188      0.000     1.8e-27    2.04e-27\n",
       "np.power(kw_min_max, 5)                -6.777e-26   4.43e-27    -15.288      0.000   -7.65e-26   -5.91e-26\n",
       "kw_avg_max                              9.545e-26   3.16e-27     30.188      0.000    8.93e-26    1.02e-25\n",
       "np.power(kw_avg_max, 5)                 4.957e-26    3.4e-27     14.574      0.000    4.29e-26    5.62e-26\n",
       "kw_min_avg                              4.231e-28    1.4e-29     30.188      0.000    3.96e-28    4.51e-28\n",
       "np.power(kw_min_avg, 5)                 1.381e-14   4.58e-16     30.188      0.000    1.29e-14    1.47e-14\n",
       "kw_max_avg                              2.316e-27   7.67e-29     30.188      0.000    2.17e-27    2.47e-27\n",
       "np.power(kw_max_avg, 5)                 1.346e-22   9.53e-23      1.413      0.158   -5.21e-23    3.21e-22\n",
       "self_reference_min_shares               1.532e-27   5.08e-29     30.188      0.000    1.43e-27    1.63e-27\n",
       "np.power(self_reference_min_shares, 5)  2.923e-27   1.54e-26      0.190      0.849   -2.72e-26     3.3e-26\n",
       "self_reference_max_shares                3.61e-27    1.2e-28     30.188      0.000    3.38e-27    3.84e-27\n",
       "np.power(self_reference_max_shares, 5)   1.13e-26   4.75e-27      2.378      0.017    1.99e-27    2.06e-26\n",
       "LDA_00                                   7.77e-32   2.57e-33     30.188      0.000    7.27e-32    8.27e-32\n",
       "np.power(LDA_00, 5)                     1.872e-32    6.2e-34     30.188      0.000    1.75e-32    1.99e-32\n",
       "LDA_01                                  6.376e-32   2.11e-33     30.188      0.000    5.96e-32    6.79e-32\n",
       "np.power(LDA_01, 5)                     1.262e-32   4.18e-34     30.188      0.000    1.18e-32    1.34e-32\n",
       "LDA_02                                   1.01e-31   3.35e-33     30.188      0.000    9.44e-32    1.08e-31\n",
       "np.power(LDA_02, 5)                     2.728e-32   9.04e-34     30.188      0.000    2.55e-32     2.9e-32\n",
       "LDA_03                                  8.223e-32   2.72e-33     30.188      0.000    7.69e-32    8.76e-32\n",
       "np.power(LDA_03, 5)                     2.172e-32    7.2e-34     30.188      0.000    2.03e-32    2.31e-32\n",
       "LDA_04                                  1.098e-31   3.64e-33     30.188      0.000    1.03e-31    1.17e-31\n",
       "np.power(LDA_04, 5)                      3.03e-32      1e-33     30.188      0.000    2.83e-32    3.23e-32\n",
       "global_subjectivity                     1.973e-31   6.54e-33     30.188      0.000    1.84e-31     2.1e-31\n",
       "np.power(global_subjectivity, 5)        1.171e-32   3.88e-34     30.188      0.000     1.1e-32    1.25e-32\n",
       "title_sentiment_polarity                3.127e-32   1.04e-33     30.188      0.000    2.92e-32    3.33e-32\n",
       "np.power(title_sentiment_polarity, 5)   6.115e-33   2.03e-34     30.188      0.000    5.72e-33    6.51e-33\n",
       "abs_title_subjectivity                  1.493e-31   4.94e-33     30.188      0.000     1.4e-31    1.59e-31\n",
       "np.power(abs_title_subjectivity, 5)     7.547e-33    2.5e-34     30.188      0.000    7.06e-33    8.04e-33\n",
       "data_channel                            1.863e-30   6.17e-32     30.188      0.000    1.74e-30    1.98e-30\n",
       "np.power(data_channel, 5)               1.889e-27   6.26e-29     30.188      0.000    1.77e-27    2.01e-27\n",
       "==============================================================================\n",
       "Omnibus:                    36988.998   Durbin-Watson:                   1.808\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):         13743321.041\n",
       "Skew:                           8.630   Prob(JB):                         0.00\n",
       "Kurtosis:                     114.841   Cond. No.                     1.25e+16\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.25e+16. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "degree = 5\n",
    "polynomial_regression_summary(train_df, significant_predictors, 'shares', degree).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc12a8d-80a9-400e-bf2a-fc73ca8d0bbd",
   "metadata": {},
   "source": [
    "#### Run polynomial regression on 2nd iteration of statistically significant predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "119282bd-2915-46fc-babc-f4532bc53950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>shares</td>      <th>  R-squared:         </th>  <td>   0.043</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.042</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   47.71</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 11 Aug 2023</td> <th>  Prob (F-statistic):</th>  <td>1.94e-221</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:34:53</td>     <th>  Log-Likelihood:    </th> <td>-2.6425e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 25756</td>      <th>  AIC:               </th>  <td>5.286e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 25731</td>      <th>  BIC:               </th>  <td>5.288e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    24</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                     <td></td>                       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                              <td> 1.211e+04</td> <td> 6088.454</td> <td>    1.989</td> <td> 0.047</td> <td>  174.676</td> <td>  2.4e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_max_avg</th>                             <td>    0.1012</td> <td>    0.012</td> <td>    8.178</td> <td> 0.000</td> <td>    0.077</td> <td>    0.125</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_max_avg, 2)</th>                <td>-3.975e-07</td> <td> 9.66e-08</td> <td>   -4.113</td> <td> 0.000</td> <td>-5.87e-07</td> <td>-2.08e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>global_subjectivity</th>                    <td>-1644.6435</td> <td> 3028.144</td> <td>   -0.543</td> <td> 0.587</td> <td>-7579.976</td> <td> 4290.689</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(global_subjectivity, 2)</th>       <td> 5962.2589</td> <td> 3230.706</td> <td>    1.845</td> <td> 0.065</td> <td> -370.107</td> <td> 1.23e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>self_reference_min_shares</th>              <td>    0.0709</td> <td>    0.005</td> <td>   14.113</td> <td> 0.000</td> <td>    0.061</td> <td>    0.081</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(self_reference_min_shares, 2)</th> <td>-1.047e-07</td> <td> 8.79e-09</td> <td>  -11.911</td> <td> 0.000</td> <td>-1.22e-07</td> <td>-8.75e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_hrefs</th>                              <td>   33.2455</td> <td>    6.969</td> <td>    4.771</td> <td> 0.000</td> <td>   19.587</td> <td>   46.904</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_hrefs, 2)</th>                 <td>    0.0254</td> <td>    0.069</td> <td>    0.370</td> <td> 0.711</td> <td>   -0.109</td> <td>    0.160</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>data_channel</th>                           <td> -228.5836</td> <td>  138.153</td> <td>   -1.655</td> <td> 0.098</td> <td> -499.370</td> <td>   42.203</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(data_channel, 2)</th>              <td>   41.3909</td> <td>   16.429</td> <td>    2.519</td> <td> 0.012</td> <td>    9.189</td> <td>   73.592</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>title_sentiment_polarity</th>               <td>  403.7566</td> <td>  176.308</td> <td>    2.290</td> <td> 0.022</td> <td>   58.182</td> <td>  749.331</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(title_sentiment_polarity, 2)</th>  <td> 1021.8826</td> <td>  274.691</td> <td>    3.720</td> <td> 0.000</td> <td>  483.472</td> <td> 1560.293</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_self_hrefs</th>                         <td>  -25.6834</td> <td>   18.693</td> <td>   -1.374</td> <td> 0.169</td> <td>  -62.322</td> <td>   10.955</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_self_hrefs, 2)</th>            <td>   -0.1466</td> <td>    0.472</td> <td>   -0.311</td> <td> 0.756</td> <td>   -1.071</td> <td>    0.778</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_imgs</th>                               <td>   33.9166</td> <td>   10.537</td> <td>    3.219</td> <td> 0.001</td> <td>   13.264</td> <td>   54.569</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_imgs, 2)</th>                  <td>   -0.3318</td> <td>    0.202</td> <td>   -1.645</td> <td> 0.100</td> <td>   -0.727</td> <td>    0.064</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n_tokens_title</th>                         <td> -247.0095</td> <td>  149.314</td> <td>   -1.654</td> <td> 0.098</td> <td> -539.674</td> <td>   45.655</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(n_tokens_title, 2)</th>            <td>   14.5613</td> <td>    7.004</td> <td>    2.079</td> <td> 0.038</td> <td>    0.833</td> <td>   28.290</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>average_token_length</th>                   <td>-4079.4339</td> <td> 2504.552</td> <td>   -1.629</td> <td> 0.103</td> <td>-8988.497</td> <td>  829.629</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(average_token_length, 2)</th>      <td>  357.5119</td> <td>  261.833</td> <td>    1.365</td> <td> 0.172</td> <td> -155.695</td> <td>  870.719</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_keywords</th>                           <td>  362.2013</td> <td>  162.149</td> <td>    2.234</td> <td> 0.026</td> <td>   44.380</td> <td>  680.022</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_keywords, 2)</th>              <td>  -19.6971</td> <td>   11.197</td> <td>   -1.759</td> <td> 0.079</td> <td>  -41.645</td> <td>    2.251</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_min_avg</th>                             <td>   -1.0617</td> <td>    0.124</td> <td>   -8.582</td> <td> 0.000</td> <td>   -1.304</td> <td>   -0.819</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_min_avg, 2)</th>                <td>    0.0005</td> <td> 4.36e-05</td> <td>   10.421</td> <td> 0.000</td> <td>    0.000</td> <td>    0.001</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>38019.676</td> <th>  Durbin-Watson:     </th>   <td>   1.995</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>15661143.976</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 9.084</td>   <th>  Prob(JB):          </th>   <td>    0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>122.429</td>  <th>  Cond. No.          </th>   <td>1.68e+12</td>  \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.68e+12. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                            &      shares      & \\textbf{  R-squared:         } &      0.043    \\\\\n",
       "\\textbf{Model:}                                    &       OLS        & \\textbf{  Adj. R-squared:    } &      0.042    \\\\\n",
       "\\textbf{Method:}                                   &  Least Squares   & \\textbf{  F-statistic:       } &      47.71    \\\\\n",
       "\\textbf{Date:}                                     & Fri, 11 Aug 2023 & \\textbf{  Prob (F-statistic):} &  1.94e-221    \\\\\n",
       "\\textbf{Time:}                                     &     11:34:53     & \\textbf{  Log-Likelihood:    } & -2.6425e+05   \\\\\n",
       "\\textbf{No. Observations:}                         &       25756      & \\textbf{  AIC:               } &  5.286e+05    \\\\\n",
       "\\textbf{Df Residuals:}                             &       25731      & \\textbf{  BIC:               } &  5.288e+05    \\\\\n",
       "\\textbf{Df Model:}                                 &          24      & \\textbf{                     } &               \\\\\n",
       "\\textbf{Covariance Type:}                          &    nonrobust     & \\textbf{                     } &               \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                                   & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                                 &    1.211e+04  &     6088.454     &     1.989  &         0.047        &      174.676    &      2.4e+04     \\\\\n",
       "\\textbf{kw\\_max\\_avg}                              &       0.1012  &        0.012     &     8.178  &         0.000        &        0.077    &        0.125     \\\\\n",
       "\\textbf{np.power(kw\\_max\\_avg, 2)}                 &   -3.975e-07  &     9.66e-08     &    -4.113  &         0.000        &    -5.87e-07    &    -2.08e-07     \\\\\n",
       "\\textbf{global\\_subjectivity}                      &   -1644.6435  &     3028.144     &    -0.543  &         0.587        &    -7579.976    &     4290.689     \\\\\n",
       "\\textbf{np.power(global\\_subjectivity, 2)}         &    5962.2589  &     3230.706     &     1.845  &         0.065        &     -370.107    &     1.23e+04     \\\\\n",
       "\\textbf{self\\_reference\\_min\\_shares}              &       0.0709  &        0.005     &    14.113  &         0.000        &        0.061    &        0.081     \\\\\n",
       "\\textbf{np.power(self\\_reference\\_min\\_shares, 2)} &   -1.047e-07  &     8.79e-09     &   -11.911  &         0.000        &    -1.22e-07    &    -8.75e-08     \\\\\n",
       "\\textbf{num\\_hrefs}                                &      33.2455  &        6.969     &     4.771  &         0.000        &       19.587    &       46.904     \\\\\n",
       "\\textbf{np.power(num\\_hrefs, 2)}                   &       0.0254  &        0.069     &     0.370  &         0.711        &       -0.109    &        0.160     \\\\\n",
       "\\textbf{data\\_channel}                             &    -228.5836  &      138.153     &    -1.655  &         0.098        &     -499.370    &       42.203     \\\\\n",
       "\\textbf{np.power(data\\_channel, 2)}                &      41.3909  &       16.429     &     2.519  &         0.012        &        9.189    &       73.592     \\\\\n",
       "\\textbf{title\\_sentiment\\_polarity}                &     403.7566  &      176.308     &     2.290  &         0.022        &       58.182    &      749.331     \\\\\n",
       "\\textbf{np.power(title\\_sentiment\\_polarity, 2)}   &    1021.8826  &      274.691     &     3.720  &         0.000        &      483.472    &     1560.293     \\\\\n",
       "\\textbf{num\\_self\\_hrefs}                          &     -25.6834  &       18.693     &    -1.374  &         0.169        &      -62.322    &       10.955     \\\\\n",
       "\\textbf{np.power(num\\_self\\_hrefs, 2)}             &      -0.1466  &        0.472     &    -0.311  &         0.756        &       -1.071    &        0.778     \\\\\n",
       "\\textbf{num\\_imgs}                                 &      33.9166  &       10.537     &     3.219  &         0.001        &       13.264    &       54.569     \\\\\n",
       "\\textbf{np.power(num\\_imgs, 2)}                    &      -0.3318  &        0.202     &    -1.645  &         0.100        &       -0.727    &        0.064     \\\\\n",
       "\\textbf{n\\_tokens\\_title}                          &    -247.0095  &      149.314     &    -1.654  &         0.098        &     -539.674    &       45.655     \\\\\n",
       "\\textbf{np.power(n\\_tokens\\_title, 2)}             &      14.5613  &        7.004     &     2.079  &         0.038        &        0.833    &       28.290     \\\\\n",
       "\\textbf{average\\_token\\_length}                    &   -4079.4339  &     2504.552     &    -1.629  &         0.103        &    -8988.497    &      829.629     \\\\\n",
       "\\textbf{np.power(average\\_token\\_length, 2)}       &     357.5119  &      261.833     &     1.365  &         0.172        &     -155.695    &      870.719     \\\\\n",
       "\\textbf{num\\_keywords}                             &     362.2013  &      162.149     &     2.234  &         0.026        &       44.380    &      680.022     \\\\\n",
       "\\textbf{np.power(num\\_keywords, 2)}                &     -19.6971  &       11.197     &    -1.759  &         0.079        &      -41.645    &        2.251     \\\\\n",
       "\\textbf{kw\\_min\\_avg}                              &      -1.0617  &        0.124     &    -8.582  &         0.000        &       -1.304    &       -0.819     \\\\\n",
       "\\textbf{np.power(kw\\_min\\_avg, 2)}                 &       0.0005  &     4.36e-05     &    10.421  &         0.000        &        0.000    &        0.001     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 38019.676 & \\textbf{  Durbin-Watson:     } &      1.995    \\\\\n",
       "\\textbf{Prob(Omnibus):} &    0.000  & \\textbf{  Jarque-Bera (JB):  } & 15661143.976  \\\\\n",
       "\\textbf{Skew:}          &    9.084  & \\textbf{  Prob(JB):          } &       0.00    \\\\\n",
       "\\textbf{Kurtosis:}      &  122.429  & \\textbf{  Cond. No.          } &   1.68e+12    \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 1.68e+12. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 shares   R-squared:                       0.043\n",
       "Model:                            OLS   Adj. R-squared:                  0.042\n",
       "Method:                 Least Squares   F-statistic:                     47.71\n",
       "Date:                Fri, 11 Aug 2023   Prob (F-statistic):          1.94e-221\n",
       "Time:                        11:34:53   Log-Likelihood:            -2.6425e+05\n",
       "No. Observations:               25756   AIC:                         5.286e+05\n",
       "Df Residuals:                   25731   BIC:                         5.288e+05\n",
       "Df Model:                          24                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==========================================================================================================\n",
       "                                             coef    std err          t      P>|t|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------------------------------\n",
       "Intercept                               1.211e+04   6088.454      1.989      0.047     174.676     2.4e+04\n",
       "kw_max_avg                                 0.1012      0.012      8.178      0.000       0.077       0.125\n",
       "np.power(kw_max_avg, 2)                -3.975e-07   9.66e-08     -4.113      0.000   -5.87e-07   -2.08e-07\n",
       "global_subjectivity                    -1644.6435   3028.144     -0.543      0.587   -7579.976    4290.689\n",
       "np.power(global_subjectivity, 2)        5962.2589   3230.706      1.845      0.065    -370.107    1.23e+04\n",
       "self_reference_min_shares                  0.0709      0.005     14.113      0.000       0.061       0.081\n",
       "np.power(self_reference_min_shares, 2) -1.047e-07   8.79e-09    -11.911      0.000   -1.22e-07   -8.75e-08\n",
       "num_hrefs                                 33.2455      6.969      4.771      0.000      19.587      46.904\n",
       "np.power(num_hrefs, 2)                     0.0254      0.069      0.370      0.711      -0.109       0.160\n",
       "data_channel                            -228.5836    138.153     -1.655      0.098    -499.370      42.203\n",
       "np.power(data_channel, 2)                 41.3909     16.429      2.519      0.012       9.189      73.592\n",
       "title_sentiment_polarity                 403.7566    176.308      2.290      0.022      58.182     749.331\n",
       "np.power(title_sentiment_polarity, 2)   1021.8826    274.691      3.720      0.000     483.472    1560.293\n",
       "num_self_hrefs                           -25.6834     18.693     -1.374      0.169     -62.322      10.955\n",
       "np.power(num_self_hrefs, 2)               -0.1466      0.472     -0.311      0.756      -1.071       0.778\n",
       "num_imgs                                  33.9166     10.537      3.219      0.001      13.264      54.569\n",
       "np.power(num_imgs, 2)                     -0.3318      0.202     -1.645      0.100      -0.727       0.064\n",
       "n_tokens_title                          -247.0095    149.314     -1.654      0.098    -539.674      45.655\n",
       "np.power(n_tokens_title, 2)               14.5613      7.004      2.079      0.038       0.833      28.290\n",
       "average_token_length                   -4079.4339   2504.552     -1.629      0.103   -8988.497     829.629\n",
       "np.power(average_token_length, 2)        357.5119    261.833      1.365      0.172    -155.695     870.719\n",
       "num_keywords                             362.2013    162.149      2.234      0.026      44.380     680.022\n",
       "np.power(num_keywords, 2)                -19.6971     11.197     -1.759      0.079     -41.645       2.251\n",
       "kw_min_avg                                -1.0617      0.124     -8.582      0.000      -1.304      -0.819\n",
       "np.power(kw_min_avg, 2)                    0.0005   4.36e-05     10.421      0.000       0.000       0.001\n",
       "==============================================================================\n",
       "Omnibus:                    38019.676   Durbin-Watson:                   1.995\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):         15661143.976\n",
       "Skew:                           9.084   Prob(JB):                         0.00\n",
       "Kurtosis:                     122.429   Cond. No.                     1.68e+12\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.68e+12. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "degree = 2\n",
    "polynomial_regression_summary(train_df, significant_predictors2, 'shares', degree).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "13bc63f7-d39f-42c4-8f45-6f780efea59f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>shares</td>      <th>  R-squared:         </th>  <td>   0.033</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.033</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   80.46</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 11 Aug 2023</td> <th>  Prob (F-statistic):</th>  <td>1.51e-179</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:34:53</td>     <th>  Log-Likelihood:    </th> <td>-2.6438e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 25756</td>      <th>  AIC:               </th>  <td>5.288e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 25744</td>      <th>  BIC:               </th>  <td>5.289e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    11</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                     <td></td>                       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                              <td>    0.0269</td> <td>    0.009</td> <td>    3.057</td> <td> 0.002</td> <td>    0.010</td> <td>    0.044</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_max_avg</th>                             <td>    0.0931</td> <td>    0.009</td> <td>   10.372</td> <td> 0.000</td> <td>    0.076</td> <td>    0.111</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_max_avg, 3)</th>                <td> -9.02e-13</td> <td> 2.91e-13</td> <td>   -3.096</td> <td> 0.002</td> <td>-1.47e-12</td> <td>-3.31e-13</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>global_subjectivity</th>                    <td>    0.0565</td> <td>    0.004</td> <td>   12.744</td> <td> 0.000</td> <td>    0.048</td> <td>    0.065</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(global_subjectivity, 3)</th>       <td>    0.0279</td> <td>    0.002</td> <td>   11.909</td> <td> 0.000</td> <td>    0.023</td> <td>    0.033</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>self_reference_min_shares</th>              <td>    0.0687</td> <td>    0.004</td> <td>   15.598</td> <td> 0.000</td> <td>    0.060</td> <td>    0.077</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(self_reference_min_shares, 3)</th> <td>-1.535e-13</td> <td> 1.17e-14</td> <td>  -13.114</td> <td> 0.000</td> <td>-1.76e-13</td> <td>-1.31e-13</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_hrefs</th>                              <td>   40.9518</td> <td>    4.003</td> <td>   10.230</td> <td> 0.000</td> <td>   33.105</td> <td>   48.799</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_hrefs, 3)</th>                 <td>   -0.0002</td> <td>    0.000</td> <td>   -1.020</td> <td> 0.308</td> <td>   -0.001</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>data_channel</th>                           <td>   -0.1572</td> <td>    0.041</td> <td>   -3.808</td> <td> 0.000</td> <td>   -0.238</td> <td>   -0.076</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(data_channel, 3)</th>              <td>    2.8797</td> <td>    0.385</td> <td>    7.475</td> <td> 0.000</td> <td>    2.125</td> <td>    3.635</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>title_sentiment_polarity</th>               <td>    0.0509</td> <td>    0.005</td> <td>   10.622</td> <td> 0.000</td> <td>    0.041</td> <td>    0.060</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(title_sentiment_polarity, 3)</th>  <td>    0.0319</td> <td>    0.003</td> <td>   10.262</td> <td> 0.000</td> <td>    0.026</td> <td>    0.038</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_self_hrefs</th>                         <td>    6.2244</td> <td>    0.604</td> <td>   10.299</td> <td> 0.000</td> <td>    5.040</td> <td>    7.409</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_self_hrefs, 3)</th>            <td>   -0.0074</td> <td>    0.004</td> <td>   -1.890</td> <td> 0.059</td> <td>   -0.015</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_imgs</th>                               <td>   13.5361</td> <td>    1.331</td> <td>   10.172</td> <td> 0.000</td> <td>   10.928</td> <td>   16.144</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_imgs, 3)</th>                  <td>   -0.0008</td> <td>    0.001</td> <td>   -0.582</td> <td> 0.561</td> <td>   -0.004</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n_tokens_title</th>                         <td>    0.1303</td> <td>    0.064</td> <td>    2.032</td> <td> 0.042</td> <td>    0.005</td> <td>    0.256</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(n_tokens_title, 3)</th>            <td>    0.3621</td> <td>    0.054</td> <td>    6.725</td> <td> 0.000</td> <td>    0.257</td> <td>    0.468</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>average_token_length</th>                   <td>    0.1698</td> <td>    0.044</td> <td>    3.835</td> <td> 0.000</td> <td>    0.083</td> <td>    0.257</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(average_token_length, 3)</th>      <td>    5.2843</td> <td>    1.154</td> <td>    4.579</td> <td> 0.000</td> <td>    3.022</td> <td>    7.546</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_keywords</th>                           <td>    0.2234</td> <td>    0.037</td> <td>    6.105</td> <td> 0.000</td> <td>    0.152</td> <td>    0.295</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_keywords, 3)</th>              <td>    0.8298</td> <td>    0.142</td> <td>    5.849</td> <td> 0.000</td> <td>    0.552</td> <td>    1.108</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_min_avg</th>                             <td>   -0.4656</td> <td>    0.077</td> <td>   -6.057</td> <td> 0.000</td> <td>   -0.616</td> <td>   -0.315</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_min_avg, 3)</th>                <td> 9.526e-08</td> <td> 8.75e-09</td> <td>   10.888</td> <td> 0.000</td> <td> 7.81e-08</td> <td> 1.12e-07</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>37893.960</td> <th>  Durbin-Watson:     </th>   <td>   1.997</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>15384020.250</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 9.029</td>   <th>  Prob(JB):          </th>   <td>    0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>121.360</td>  <th>  Cond. No.          </th>   <td>6.78e+17</td>  \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 6.78e+17. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                            &      shares      & \\textbf{  R-squared:         } &      0.033    \\\\\n",
       "\\textbf{Model:}                                    &       OLS        & \\textbf{  Adj. R-squared:    } &      0.033    \\\\\n",
       "\\textbf{Method:}                                   &  Least Squares   & \\textbf{  F-statistic:       } &      80.46    \\\\\n",
       "\\textbf{Date:}                                     & Fri, 11 Aug 2023 & \\textbf{  Prob (F-statistic):} &  1.51e-179    \\\\\n",
       "\\textbf{Time:}                                     &     11:34:53     & \\textbf{  Log-Likelihood:    } & -2.6438e+05   \\\\\n",
       "\\textbf{No. Observations:}                         &       25756      & \\textbf{  AIC:               } &  5.288e+05    \\\\\n",
       "\\textbf{Df Residuals:}                             &       25744      & \\textbf{  BIC:               } &  5.289e+05    \\\\\n",
       "\\textbf{Df Model:}                                 &          11      & \\textbf{                     } &               \\\\\n",
       "\\textbf{Covariance Type:}                          &    nonrobust     & \\textbf{                     } &               \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                                   & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                                 &       0.0269  &        0.009     &     3.057  &         0.002        &        0.010    &        0.044     \\\\\n",
       "\\textbf{kw\\_max\\_avg}                              &       0.0931  &        0.009     &    10.372  &         0.000        &        0.076    &        0.111     \\\\\n",
       "\\textbf{np.power(kw\\_max\\_avg, 3)}                 &    -9.02e-13  &     2.91e-13     &    -3.096  &         0.002        &    -1.47e-12    &    -3.31e-13     \\\\\n",
       "\\textbf{global\\_subjectivity}                      &       0.0565  &        0.004     &    12.744  &         0.000        &        0.048    &        0.065     \\\\\n",
       "\\textbf{np.power(global\\_subjectivity, 3)}         &       0.0279  &        0.002     &    11.909  &         0.000        &        0.023    &        0.033     \\\\\n",
       "\\textbf{self\\_reference\\_min\\_shares}              &       0.0687  &        0.004     &    15.598  &         0.000        &        0.060    &        0.077     \\\\\n",
       "\\textbf{np.power(self\\_reference\\_min\\_shares, 3)} &   -1.535e-13  &     1.17e-14     &   -13.114  &         0.000        &    -1.76e-13    &    -1.31e-13     \\\\\n",
       "\\textbf{num\\_hrefs}                                &      40.9518  &        4.003     &    10.230  &         0.000        &       33.105    &       48.799     \\\\\n",
       "\\textbf{np.power(num\\_hrefs, 3)}                   &      -0.0002  &        0.000     &    -1.020  &         0.308        &       -0.001    &        0.000     \\\\\n",
       "\\textbf{data\\_channel}                             &      -0.1572  &        0.041     &    -3.808  &         0.000        &       -0.238    &       -0.076     \\\\\n",
       "\\textbf{np.power(data\\_channel, 3)}                &       2.8797  &        0.385     &     7.475  &         0.000        &        2.125    &        3.635     \\\\\n",
       "\\textbf{title\\_sentiment\\_polarity}                &       0.0509  &        0.005     &    10.622  &         0.000        &        0.041    &        0.060     \\\\\n",
       "\\textbf{np.power(title\\_sentiment\\_polarity, 3)}   &       0.0319  &        0.003     &    10.262  &         0.000        &        0.026    &        0.038     \\\\\n",
       "\\textbf{num\\_self\\_hrefs}                          &       6.2244  &        0.604     &    10.299  &         0.000        &        5.040    &        7.409     \\\\\n",
       "\\textbf{np.power(num\\_self\\_hrefs, 3)}             &      -0.0074  &        0.004     &    -1.890  &         0.059        &       -0.015    &        0.000     \\\\\n",
       "\\textbf{num\\_imgs}                                 &      13.5361  &        1.331     &    10.172  &         0.000        &       10.928    &       16.144     \\\\\n",
       "\\textbf{np.power(num\\_imgs, 3)}                    &      -0.0008  &        0.001     &    -0.582  &         0.561        &       -0.004    &        0.002     \\\\\n",
       "\\textbf{n\\_tokens\\_title}                          &       0.1303  &        0.064     &     2.032  &         0.042        &        0.005    &        0.256     \\\\\n",
       "\\textbf{np.power(n\\_tokens\\_title, 3)}             &       0.3621  &        0.054     &     6.725  &         0.000        &        0.257    &        0.468     \\\\\n",
       "\\textbf{average\\_token\\_length}                    &       0.1698  &        0.044     &     3.835  &         0.000        &        0.083    &        0.257     \\\\\n",
       "\\textbf{np.power(average\\_token\\_length, 3)}       &       5.2843  &        1.154     &     4.579  &         0.000        &        3.022    &        7.546     \\\\\n",
       "\\textbf{num\\_keywords}                             &       0.2234  &        0.037     &     6.105  &         0.000        &        0.152    &        0.295     \\\\\n",
       "\\textbf{np.power(num\\_keywords, 3)}                &       0.8298  &        0.142     &     5.849  &         0.000        &        0.552    &        1.108     \\\\\n",
       "\\textbf{kw\\_min\\_avg}                              &      -0.4656  &        0.077     &    -6.057  &         0.000        &       -0.616    &       -0.315     \\\\\n",
       "\\textbf{np.power(kw\\_min\\_avg, 3)}                 &    9.526e-08  &     8.75e-09     &    10.888  &         0.000        &     7.81e-08    &     1.12e-07     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 37893.960 & \\textbf{  Durbin-Watson:     } &      1.997    \\\\\n",
       "\\textbf{Prob(Omnibus):} &    0.000  & \\textbf{  Jarque-Bera (JB):  } & 15384020.250  \\\\\n",
       "\\textbf{Skew:}          &    9.029  & \\textbf{  Prob(JB):          } &       0.00    \\\\\n",
       "\\textbf{Kurtosis:}      &  121.360  & \\textbf{  Cond. No.          } &   6.78e+17    \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 6.78e+17. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 shares   R-squared:                       0.033\n",
       "Model:                            OLS   Adj. R-squared:                  0.033\n",
       "Method:                 Least Squares   F-statistic:                     80.46\n",
       "Date:                Fri, 11 Aug 2023   Prob (F-statistic):          1.51e-179\n",
       "Time:                        11:34:53   Log-Likelihood:            -2.6438e+05\n",
       "No. Observations:               25756   AIC:                         5.288e+05\n",
       "Df Residuals:                   25744   BIC:                         5.289e+05\n",
       "Df Model:                          11                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==========================================================================================================\n",
       "                                             coef    std err          t      P>|t|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------------------------------\n",
       "Intercept                                  0.0269      0.009      3.057      0.002       0.010       0.044\n",
       "kw_max_avg                                 0.0931      0.009     10.372      0.000       0.076       0.111\n",
       "np.power(kw_max_avg, 3)                 -9.02e-13   2.91e-13     -3.096      0.002   -1.47e-12   -3.31e-13\n",
       "global_subjectivity                        0.0565      0.004     12.744      0.000       0.048       0.065\n",
       "np.power(global_subjectivity, 3)           0.0279      0.002     11.909      0.000       0.023       0.033\n",
       "self_reference_min_shares                  0.0687      0.004     15.598      0.000       0.060       0.077\n",
       "np.power(self_reference_min_shares, 3) -1.535e-13   1.17e-14    -13.114      0.000   -1.76e-13   -1.31e-13\n",
       "num_hrefs                                 40.9518      4.003     10.230      0.000      33.105      48.799\n",
       "np.power(num_hrefs, 3)                    -0.0002      0.000     -1.020      0.308      -0.001       0.000\n",
       "data_channel                              -0.1572      0.041     -3.808      0.000      -0.238      -0.076\n",
       "np.power(data_channel, 3)                  2.8797      0.385      7.475      0.000       2.125       3.635\n",
       "title_sentiment_polarity                   0.0509      0.005     10.622      0.000       0.041       0.060\n",
       "np.power(title_sentiment_polarity, 3)      0.0319      0.003     10.262      0.000       0.026       0.038\n",
       "num_self_hrefs                             6.2244      0.604     10.299      0.000       5.040       7.409\n",
       "np.power(num_self_hrefs, 3)               -0.0074      0.004     -1.890      0.059      -0.015       0.000\n",
       "num_imgs                                  13.5361      1.331     10.172      0.000      10.928      16.144\n",
       "np.power(num_imgs, 3)                     -0.0008      0.001     -0.582      0.561      -0.004       0.002\n",
       "n_tokens_title                             0.1303      0.064      2.032      0.042       0.005       0.256\n",
       "np.power(n_tokens_title, 3)                0.3621      0.054      6.725      0.000       0.257       0.468\n",
       "average_token_length                       0.1698      0.044      3.835      0.000       0.083       0.257\n",
       "np.power(average_token_length, 3)          5.2843      1.154      4.579      0.000       3.022       7.546\n",
       "num_keywords                               0.2234      0.037      6.105      0.000       0.152       0.295\n",
       "np.power(num_keywords, 3)                  0.8298      0.142      5.849      0.000       0.552       1.108\n",
       "kw_min_avg                                -0.4656      0.077     -6.057      0.000      -0.616      -0.315\n",
       "np.power(kw_min_avg, 3)                 9.526e-08   8.75e-09     10.888      0.000    7.81e-08    1.12e-07\n",
       "==============================================================================\n",
       "Omnibus:                    37893.960   Durbin-Watson:                   1.997\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):         15384020.250\n",
       "Skew:                           9.029   Prob(JB):                         0.00\n",
       "Kurtosis:                     121.360   Cond. No.                     6.78e+17\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 6.78e+17. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "degree = 3\n",
    "polynomial_regression_summary(train_df, significant_predictors2, 'shares', degree).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3a6604b3-1f95-49ac-b003-d0554c06161b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>shares</td>      <th>  R-squared:         </th>  <td>  -0.120</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>  -0.120</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>  -917.0</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 11 Aug 2023</td> <th>  Prob (F-statistic):</th>   <td>  1.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:34:53</td>     <th>  Log-Likelihood:    </th> <td>-2.6627e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 25756</td>      <th>  AIC:               </th>  <td>5.325e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 25752</td>      <th>  BIC:               </th>  <td>5.326e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                     <td></td>                       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                              <td> 5.388e-16</td> <td> 2.12e-16</td> <td>    2.536</td> <td> 0.011</td> <td> 1.22e-16</td> <td> 9.55e-16</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_max_avg</th>                             <td> 1.776e-11</td> <td>    7e-12</td> <td>    2.536</td> <td> 0.011</td> <td> 4.03e-12</td> <td> 3.15e-11</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_max_avg, 4)</th>                <td> 2.383e-18</td> <td> 9.38e-19</td> <td>    2.542</td> <td> 0.011</td> <td> 5.45e-19</td> <td> 4.22e-18</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>global_subjectivity</th>                    <td>  2.54e-16</td> <td>    1e-16</td> <td>    2.536</td> <td> 0.011</td> <td> 5.77e-17</td> <td>  4.5e-16</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(global_subjectivity, 4)</th>       <td> 2.908e-17</td> <td> 1.15e-17</td> <td>    2.536</td> <td> 0.011</td> <td>  6.6e-18</td> <td> 5.15e-17</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>self_reference_min_shares</th>              <td> 9.073e-13</td> <td> 3.58e-13</td> <td>    2.536</td> <td> 0.011</td> <td> 2.06e-13</td> <td> 1.61e-12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(self_reference_min_shares, 4)</th> <td> 1.312e-20</td> <td> 9.71e-21</td> <td>    1.351</td> <td> 0.177</td> <td>-5.91e-21</td> <td> 3.22e-20</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_hrefs</th>                              <td> 1.249e-13</td> <td> 4.93e-14</td> <td>    2.536</td> <td> 0.011</td> <td> 2.84e-14</td> <td> 2.21e-13</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_hrefs, 4)</th>                 <td> 2.133e-06</td> <td> 8.41e-07</td> <td>    2.536</td> <td> 0.011</td> <td> 4.85e-07</td> <td> 3.78e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>data_channel</th>                           <td> 1.705e-15</td> <td> 6.72e-16</td> <td>    2.536</td> <td> 0.011</td> <td> 3.87e-16</td> <td> 3.02e-15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(data_channel, 4)</th>              <td> 2.603e-13</td> <td> 1.03e-13</td> <td>    2.536</td> <td> 0.011</td> <td> 5.91e-14</td> <td> 4.61e-13</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>title_sentiment_polarity</th>               <td>  2.79e-17</td> <td>  1.1e-17</td> <td>    2.536</td> <td> 0.011</td> <td> 6.34e-18</td> <td> 4.95e-17</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(title_sentiment_polarity, 4)</th>  <td> 1.604e-17</td> <td> 6.33e-18</td> <td>    2.536</td> <td> 0.011</td> <td> 3.64e-18</td> <td> 2.84e-17</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_self_hrefs</th>                         <td> 6.678e-15</td> <td> 2.63e-15</td> <td>    2.536</td> <td> 0.011</td> <td> 1.52e-15</td> <td> 1.18e-14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_self_hrefs, 4)</th>            <td> 1.253e-09</td> <td> 4.94e-10</td> <td>    2.536</td> <td> 0.011</td> <td> 2.85e-10</td> <td> 2.22e-09</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_imgs</th>                               <td> 6.794e-15</td> <td> 2.68e-15</td> <td>    2.536</td> <td> 0.011</td> <td> 1.54e-15</td> <td>  1.2e-14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_imgs, 4)</th>                  <td> 8.904e-10</td> <td> 3.51e-10</td> <td>    2.536</td> <td> 0.011</td> <td> 2.02e-10</td> <td> 1.58e-09</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n_tokens_title</th>                         <td> 5.508e-15</td> <td> 2.17e-15</td> <td>    2.536</td> <td> 0.011</td> <td> 1.25e-15</td> <td> 9.76e-15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(n_tokens_title, 4)</th>            <td> 6.751e-12</td> <td> 2.66e-12</td> <td>    2.536</td> <td> 0.011</td> <td> 1.53e-12</td> <td>  1.2e-11</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>average_token_length</th>                   <td> 3.079e-15</td> <td> 1.21e-15</td> <td>    2.536</td> <td> 0.011</td> <td> 6.99e-16</td> <td> 5.46e-15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(average_token_length, 4)</th>      <td>  6.17e-13</td> <td> 2.43e-13</td> <td>    2.536</td> <td> 0.011</td> <td>  1.4e-13</td> <td> 1.09e-12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_keywords</th>                           <td> 4.588e-15</td> <td> 1.81e-15</td> <td>    2.536</td> <td> 0.011</td> <td> 1.04e-15</td> <td> 8.13e-15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_keywords, 4)</th>              <td> 2.913e-12</td> <td> 1.15e-12</td> <td>    2.536</td> <td> 0.011</td> <td> 6.62e-13</td> <td> 5.16e-12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_min_avg</th>                             <td> 5.989e-13</td> <td> 2.36e-13</td> <td>    2.536</td> <td> 0.011</td> <td> 1.36e-13</td> <td> 1.06e-12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_min_avg, 4)</th>                <td> 5.604e-11</td> <td> 1.28e-12</td> <td>   43.913</td> <td> 0.000</td> <td> 5.35e-11</td> <td> 5.85e-11</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>37102.804</td> <th>  Durbin-Watson:     </th>   <td>   1.813</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>13908757.617</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 8.681</td>   <th>  Prob(JB):          </th>   <td>    0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>115.512</td>  <th>  Cond. No.          </th>   <td>3.20e+23</td>  \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 3.2e+23. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                            &      shares      & \\textbf{  R-squared:         } &     -0.120    \\\\\n",
       "\\textbf{Model:}                                    &       OLS        & \\textbf{  Adj. R-squared:    } &     -0.120    \\\\\n",
       "\\textbf{Method:}                                   &  Least Squares   & \\textbf{  F-statistic:       } &     -917.0    \\\\\n",
       "\\textbf{Date:}                                     & Fri, 11 Aug 2023 & \\textbf{  Prob (F-statistic):} &      1.00     \\\\\n",
       "\\textbf{Time:}                                     &     11:34:53     & \\textbf{  Log-Likelihood:    } & -2.6627e+05   \\\\\n",
       "\\textbf{No. Observations:}                         &       25756      & \\textbf{  AIC:               } &  5.325e+05    \\\\\n",
       "\\textbf{Df Residuals:}                             &       25752      & \\textbf{  BIC:               } &  5.326e+05    \\\\\n",
       "\\textbf{Df Model:}                                 &           3      & \\textbf{                     } &               \\\\\n",
       "\\textbf{Covariance Type:}                          &    nonrobust     & \\textbf{                     } &               \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                                   & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                                 &    5.388e-16  &     2.12e-16     &     2.536  &         0.011        &     1.22e-16    &     9.55e-16     \\\\\n",
       "\\textbf{kw\\_max\\_avg}                              &    1.776e-11  &        7e-12     &     2.536  &         0.011        &     4.03e-12    &     3.15e-11     \\\\\n",
       "\\textbf{np.power(kw\\_max\\_avg, 4)}                 &    2.383e-18  &     9.38e-19     &     2.542  &         0.011        &     5.45e-19    &     4.22e-18     \\\\\n",
       "\\textbf{global\\_subjectivity}                      &     2.54e-16  &        1e-16     &     2.536  &         0.011        &     5.77e-17    &      4.5e-16     \\\\\n",
       "\\textbf{np.power(global\\_subjectivity, 4)}         &    2.908e-17  &     1.15e-17     &     2.536  &         0.011        &      6.6e-18    &     5.15e-17     \\\\\n",
       "\\textbf{self\\_reference\\_min\\_shares}              &    9.073e-13  &     3.58e-13     &     2.536  &         0.011        &     2.06e-13    &     1.61e-12     \\\\\n",
       "\\textbf{np.power(self\\_reference\\_min\\_shares, 4)} &    1.312e-20  &     9.71e-21     &     1.351  &         0.177        &    -5.91e-21    &     3.22e-20     \\\\\n",
       "\\textbf{num\\_hrefs}                                &    1.249e-13  &     4.93e-14     &     2.536  &         0.011        &     2.84e-14    &     2.21e-13     \\\\\n",
       "\\textbf{np.power(num\\_hrefs, 4)}                   &    2.133e-06  &     8.41e-07     &     2.536  &         0.011        &     4.85e-07    &     3.78e-06     \\\\\n",
       "\\textbf{data\\_channel}                             &    1.705e-15  &     6.72e-16     &     2.536  &         0.011        &     3.87e-16    &     3.02e-15     \\\\\n",
       "\\textbf{np.power(data\\_channel, 4)}                &    2.603e-13  &     1.03e-13     &     2.536  &         0.011        &     5.91e-14    &     4.61e-13     \\\\\n",
       "\\textbf{title\\_sentiment\\_polarity}                &     2.79e-17  &      1.1e-17     &     2.536  &         0.011        &     6.34e-18    &     4.95e-17     \\\\\n",
       "\\textbf{np.power(title\\_sentiment\\_polarity, 4)}   &    1.604e-17  &     6.33e-18     &     2.536  &         0.011        &     3.64e-18    &     2.84e-17     \\\\\n",
       "\\textbf{num\\_self\\_hrefs}                          &    6.678e-15  &     2.63e-15     &     2.536  &         0.011        &     1.52e-15    &     1.18e-14     \\\\\n",
       "\\textbf{np.power(num\\_self\\_hrefs, 4)}             &    1.253e-09  &     4.94e-10     &     2.536  &         0.011        &     2.85e-10    &     2.22e-09     \\\\\n",
       "\\textbf{num\\_imgs}                                 &    6.794e-15  &     2.68e-15     &     2.536  &         0.011        &     1.54e-15    &      1.2e-14     \\\\\n",
       "\\textbf{np.power(num\\_imgs, 4)}                    &    8.904e-10  &     3.51e-10     &     2.536  &         0.011        &     2.02e-10    &     1.58e-09     \\\\\n",
       "\\textbf{n\\_tokens\\_title}                          &    5.508e-15  &     2.17e-15     &     2.536  &         0.011        &     1.25e-15    &     9.76e-15     \\\\\n",
       "\\textbf{np.power(n\\_tokens\\_title, 4)}             &    6.751e-12  &     2.66e-12     &     2.536  &         0.011        &     1.53e-12    &      1.2e-11     \\\\\n",
       "\\textbf{average\\_token\\_length}                    &    3.079e-15  &     1.21e-15     &     2.536  &         0.011        &     6.99e-16    &     5.46e-15     \\\\\n",
       "\\textbf{np.power(average\\_token\\_length, 4)}       &     6.17e-13  &     2.43e-13     &     2.536  &         0.011        &      1.4e-13    &     1.09e-12     \\\\\n",
       "\\textbf{num\\_keywords}                             &    4.588e-15  &     1.81e-15     &     2.536  &         0.011        &     1.04e-15    &     8.13e-15     \\\\\n",
       "\\textbf{np.power(num\\_keywords, 4)}                &    2.913e-12  &     1.15e-12     &     2.536  &         0.011        &     6.62e-13    &     5.16e-12     \\\\\n",
       "\\textbf{kw\\_min\\_avg}                              &    5.989e-13  &     2.36e-13     &     2.536  &         0.011        &     1.36e-13    &     1.06e-12     \\\\\n",
       "\\textbf{np.power(kw\\_min\\_avg, 4)}                 &    5.604e-11  &     1.28e-12     &    43.913  &         0.000        &     5.35e-11    &     5.85e-11     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 37102.804 & \\textbf{  Durbin-Watson:     } &      1.813    \\\\\n",
       "\\textbf{Prob(Omnibus):} &    0.000  & \\textbf{  Jarque-Bera (JB):  } & 13908757.617  \\\\\n",
       "\\textbf{Skew:}          &    8.681  & \\textbf{  Prob(JB):          } &       0.00    \\\\\n",
       "\\textbf{Kurtosis:}      &  115.512  & \\textbf{  Cond. No.          } &   3.20e+23    \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 3.2e+23. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 shares   R-squared:                      -0.120\n",
       "Model:                            OLS   Adj. R-squared:                 -0.120\n",
       "Method:                 Least Squares   F-statistic:                    -917.0\n",
       "Date:                Fri, 11 Aug 2023   Prob (F-statistic):               1.00\n",
       "Time:                        11:34:53   Log-Likelihood:            -2.6627e+05\n",
       "No. Observations:               25756   AIC:                         5.325e+05\n",
       "Df Residuals:                   25752   BIC:                         5.326e+05\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==========================================================================================================\n",
       "                                             coef    std err          t      P>|t|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------------------------------\n",
       "Intercept                               5.388e-16   2.12e-16      2.536      0.011    1.22e-16    9.55e-16\n",
       "kw_max_avg                              1.776e-11      7e-12      2.536      0.011    4.03e-12    3.15e-11\n",
       "np.power(kw_max_avg, 4)                 2.383e-18   9.38e-19      2.542      0.011    5.45e-19    4.22e-18\n",
       "global_subjectivity                      2.54e-16      1e-16      2.536      0.011    5.77e-17     4.5e-16\n",
       "np.power(global_subjectivity, 4)        2.908e-17   1.15e-17      2.536      0.011     6.6e-18    5.15e-17\n",
       "self_reference_min_shares               9.073e-13   3.58e-13      2.536      0.011    2.06e-13    1.61e-12\n",
       "np.power(self_reference_min_shares, 4)  1.312e-20   9.71e-21      1.351      0.177   -5.91e-21    3.22e-20\n",
       "num_hrefs                               1.249e-13   4.93e-14      2.536      0.011    2.84e-14    2.21e-13\n",
       "np.power(num_hrefs, 4)                  2.133e-06   8.41e-07      2.536      0.011    4.85e-07    3.78e-06\n",
       "data_channel                            1.705e-15   6.72e-16      2.536      0.011    3.87e-16    3.02e-15\n",
       "np.power(data_channel, 4)               2.603e-13   1.03e-13      2.536      0.011    5.91e-14    4.61e-13\n",
       "title_sentiment_polarity                 2.79e-17    1.1e-17      2.536      0.011    6.34e-18    4.95e-17\n",
       "np.power(title_sentiment_polarity, 4)   1.604e-17   6.33e-18      2.536      0.011    3.64e-18    2.84e-17\n",
       "num_self_hrefs                          6.678e-15   2.63e-15      2.536      0.011    1.52e-15    1.18e-14\n",
       "np.power(num_self_hrefs, 4)             1.253e-09   4.94e-10      2.536      0.011    2.85e-10    2.22e-09\n",
       "num_imgs                                6.794e-15   2.68e-15      2.536      0.011    1.54e-15     1.2e-14\n",
       "np.power(num_imgs, 4)                   8.904e-10   3.51e-10      2.536      0.011    2.02e-10    1.58e-09\n",
       "n_tokens_title                          5.508e-15   2.17e-15      2.536      0.011    1.25e-15    9.76e-15\n",
       "np.power(n_tokens_title, 4)             6.751e-12   2.66e-12      2.536      0.011    1.53e-12     1.2e-11\n",
       "average_token_length                    3.079e-15   1.21e-15      2.536      0.011    6.99e-16    5.46e-15\n",
       "np.power(average_token_length, 4)        6.17e-13   2.43e-13      2.536      0.011     1.4e-13    1.09e-12\n",
       "num_keywords                            4.588e-15   1.81e-15      2.536      0.011    1.04e-15    8.13e-15\n",
       "np.power(num_keywords, 4)               2.913e-12   1.15e-12      2.536      0.011    6.62e-13    5.16e-12\n",
       "kw_min_avg                              5.989e-13   2.36e-13      2.536      0.011    1.36e-13    1.06e-12\n",
       "np.power(kw_min_avg, 4)                 5.604e-11   1.28e-12     43.913      0.000    5.35e-11    5.85e-11\n",
       "==============================================================================\n",
       "Omnibus:                    37102.804   Durbin-Watson:                   1.813\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):         13908757.617\n",
       "Skew:                           8.681   Prob(JB):                         0.00\n",
       "Kurtosis:                     115.512   Cond. No.                     3.20e+23\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 3.2e+23. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "degree = 4\n",
    "polynomial_regression_summary(train_df, significant_predictors2, 'shares', degree).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ed715d22-96d2-4bfb-9733-3ef0cd19ceaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>shares</td>      <th>  R-squared:         </th>  <td>  -0.131</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>  -0.131</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>  -1494.</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 11 Aug 2023</td> <th>  Prob (F-statistic):</th>   <td>  1.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:34:53</td>     <th>  Log-Likelihood:    </th> <td>-2.6640e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 25756</td>      <th>  AIC:               </th>  <td>5.328e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 25753</td>      <th>  BIC:               </th>  <td>5.328e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                     <td></td>                       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                              <td> 5.663e-32</td> <td> 1.39e-33</td> <td>   40.697</td> <td> 0.000</td> <td> 5.39e-32</td> <td> 5.94e-32</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_max_avg</th>                             <td>-1.205e-22</td> <td> 2.96e-24</td> <td>  -40.697</td> <td> 0.000</td> <td>-1.26e-22</td> <td>-1.15e-22</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_max_avg, 5)</th>                <td> 7.509e-24</td> <td> 3.17e-24</td> <td>    2.367</td> <td> 0.018</td> <td> 1.29e-24</td> <td> 1.37e-23</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>global_subjectivity</th>                    <td> 1.446e-44</td> <td> 3.55e-46</td> <td>   40.758</td> <td> 0.000</td> <td> 1.38e-44</td> <td> 1.52e-44</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(global_subjectivity, 5)</th>       <td> 2.229e-33</td> <td> 5.48e-35</td> <td>   40.697</td> <td> 0.000</td> <td> 2.12e-33</td> <td> 2.34e-33</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>self_reference_min_shares</th>              <td> 3.377e-28</td> <td>  8.3e-30</td> <td>   40.697</td> <td> 0.000</td> <td> 3.21e-28</td> <td> 3.54e-28</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(self_reference_min_shares, 5)</th> <td> 2.042e-26</td> <td> 1.47e-26</td> <td>    1.390</td> <td> 0.165</td> <td>-8.37e-27</td> <td> 4.92e-26</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_hrefs</th>                              <td> 7.504e-31</td> <td> 1.84e-32</td> <td>   40.697</td> <td> 0.000</td> <td> 7.14e-31</td> <td> 7.87e-31</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_hrefs, 5)</th>                 <td> 1.354e-23</td> <td> 3.33e-25</td> <td>   40.697</td> <td> 0.000</td> <td> 1.29e-23</td> <td> 1.42e-23</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>data_channel</th>                           <td> 2.597e-31</td> <td> 6.38e-33</td> <td>   40.697</td> <td> 0.000</td> <td> 2.47e-31</td> <td> 2.72e-31</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(data_channel, 5)</th>              <td> 3.887e-28</td> <td> 9.55e-30</td> <td>   40.697</td> <td> 0.000</td> <td>  3.7e-28</td> <td> 4.07e-28</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>title_sentiment_polarity</th>               <td> 5.238e-33</td> <td> 1.29e-34</td> <td>   40.697</td> <td> 0.000</td> <td> 4.99e-33</td> <td> 5.49e-33</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(title_sentiment_polarity, 5)</th>  <td> 1.251e-33</td> <td> 3.07e-35</td> <td>   40.697</td> <td> 0.000</td> <td> 1.19e-33</td> <td> 1.31e-33</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_self_hrefs</th>                         <td> 1.862e-31</td> <td> 4.58e-33</td> <td>   40.697</td> <td> 0.000</td> <td> 1.77e-31</td> <td> 1.95e-31</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_self_hrefs, 5)</th>            <td> 1.495e-25</td> <td> 3.67e-27</td> <td>   40.697</td> <td> 0.000</td> <td> 1.42e-25</td> <td> 1.57e-25</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_imgs</th>                               <td> 3.651e-31</td> <td> 8.97e-33</td> <td>   40.697</td> <td> 0.000</td> <td> 3.48e-31</td> <td> 3.83e-31</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_imgs, 5)</th>                  <td> 1.139e-24</td> <td>  2.8e-26</td> <td>   40.697</td> <td> 0.000</td> <td> 1.08e-24</td> <td> 1.19e-24</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>n_tokens_title</th>                         <td> 5.853e-31</td> <td> 1.44e-32</td> <td>   40.697</td> <td> 0.000</td> <td> 5.57e-31</td> <td> 6.14e-31</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(n_tokens_title, 5)</th>            <td> 9.586e-27</td> <td> 2.36e-28</td> <td>   40.697</td> <td> 0.000</td> <td> 9.12e-27</td> <td>    1e-26</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>average_token_length</th>                   <td> 2.653e-31</td> <td> 6.52e-33</td> <td>   40.697</td> <td> 0.000</td> <td> 2.52e-31</td> <td> 2.78e-31</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(average_token_length, 5)</th>      <td> 1.335e-28</td> <td> 3.28e-30</td> <td>   40.697</td> <td> 0.000</td> <td> 1.27e-28</td> <td>  1.4e-28</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_keywords</th>                           <td> 3.577e-31</td> <td> 8.79e-33</td> <td>   40.697</td> <td> 0.000</td> <td>  3.4e-31</td> <td> 3.75e-31</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(num_keywords, 5)</th>              <td> 1.269e-27</td> <td> 3.12e-29</td> <td>   40.697</td> <td> 0.000</td> <td> 1.21e-27</td> <td> 1.33e-27</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kw_min_avg</th>                             <td> 1.676e-28</td> <td> 4.12e-30</td> <td>   40.697</td> <td> 0.000</td> <td>  1.6e-28</td> <td> 1.76e-28</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(kw_min_avg, 5)</th>                <td> 1.631e-14</td> <td> 4.01e-16</td> <td>   40.697</td> <td> 0.000</td> <td> 1.55e-14</td> <td> 1.71e-14</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>37211.834</td> <th>  Durbin-Watson:     </th>   <td>   1.787</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>14087045.465</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 8.729</td>   <th>  Prob(JB):          </th>   <td>    0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>116.234</td>  <th>  Cond. No.          </th>   <td>3.79e+29</td>  \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 3.79e+29. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                            &      shares      & \\textbf{  R-squared:         } &     -0.131    \\\\\n",
       "\\textbf{Model:}                                    &       OLS        & \\textbf{  Adj. R-squared:    } &     -0.131    \\\\\n",
       "\\textbf{Method:}                                   &  Least Squares   & \\textbf{  F-statistic:       } &     -1494.    \\\\\n",
       "\\textbf{Date:}                                     & Fri, 11 Aug 2023 & \\textbf{  Prob (F-statistic):} &      1.00     \\\\\n",
       "\\textbf{Time:}                                     &     11:34:53     & \\textbf{  Log-Likelihood:    } & -2.6640e+05   \\\\\n",
       "\\textbf{No. Observations:}                         &       25756      & \\textbf{  AIC:               } &  5.328e+05    \\\\\n",
       "\\textbf{Df Residuals:}                             &       25753      & \\textbf{  BIC:               } &  5.328e+05    \\\\\n",
       "\\textbf{Df Model:}                                 &           2      & \\textbf{                     } &               \\\\\n",
       "\\textbf{Covariance Type:}                          &    nonrobust     & \\textbf{                     } &               \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                                   & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                                 &    5.663e-32  &     1.39e-33     &    40.697  &         0.000        &     5.39e-32    &     5.94e-32     \\\\\n",
       "\\textbf{kw\\_max\\_avg}                              &   -1.205e-22  &     2.96e-24     &   -40.697  &         0.000        &    -1.26e-22    &    -1.15e-22     \\\\\n",
       "\\textbf{np.power(kw\\_max\\_avg, 5)}                 &    7.509e-24  &     3.17e-24     &     2.367  &         0.018        &     1.29e-24    &     1.37e-23     \\\\\n",
       "\\textbf{global\\_subjectivity}                      &    1.446e-44  &     3.55e-46     &    40.758  &         0.000        &     1.38e-44    &     1.52e-44     \\\\\n",
       "\\textbf{np.power(global\\_subjectivity, 5)}         &    2.229e-33  &     5.48e-35     &    40.697  &         0.000        &     2.12e-33    &     2.34e-33     \\\\\n",
       "\\textbf{self\\_reference\\_min\\_shares}              &    3.377e-28  &      8.3e-30     &    40.697  &         0.000        &     3.21e-28    &     3.54e-28     \\\\\n",
       "\\textbf{np.power(self\\_reference\\_min\\_shares, 5)} &    2.042e-26  &     1.47e-26     &     1.390  &         0.165        &    -8.37e-27    &     4.92e-26     \\\\\n",
       "\\textbf{num\\_hrefs}                                &    7.504e-31  &     1.84e-32     &    40.697  &         0.000        &     7.14e-31    &     7.87e-31     \\\\\n",
       "\\textbf{np.power(num\\_hrefs, 5)}                   &    1.354e-23  &     3.33e-25     &    40.697  &         0.000        &     1.29e-23    &     1.42e-23     \\\\\n",
       "\\textbf{data\\_channel}                             &    2.597e-31  &     6.38e-33     &    40.697  &         0.000        &     2.47e-31    &     2.72e-31     \\\\\n",
       "\\textbf{np.power(data\\_channel, 5)}                &    3.887e-28  &     9.55e-30     &    40.697  &         0.000        &      3.7e-28    &     4.07e-28     \\\\\n",
       "\\textbf{title\\_sentiment\\_polarity}                &    5.238e-33  &     1.29e-34     &    40.697  &         0.000        &     4.99e-33    &     5.49e-33     \\\\\n",
       "\\textbf{np.power(title\\_sentiment\\_polarity, 5)}   &    1.251e-33  &     3.07e-35     &    40.697  &         0.000        &     1.19e-33    &     1.31e-33     \\\\\n",
       "\\textbf{num\\_self\\_hrefs}                          &    1.862e-31  &     4.58e-33     &    40.697  &         0.000        &     1.77e-31    &     1.95e-31     \\\\\n",
       "\\textbf{np.power(num\\_self\\_hrefs, 5)}             &    1.495e-25  &     3.67e-27     &    40.697  &         0.000        &     1.42e-25    &     1.57e-25     \\\\\n",
       "\\textbf{num\\_imgs}                                 &    3.651e-31  &     8.97e-33     &    40.697  &         0.000        &     3.48e-31    &     3.83e-31     \\\\\n",
       "\\textbf{np.power(num\\_imgs, 5)}                    &    1.139e-24  &      2.8e-26     &    40.697  &         0.000        &     1.08e-24    &     1.19e-24     \\\\\n",
       "\\textbf{n\\_tokens\\_title}                          &    5.853e-31  &     1.44e-32     &    40.697  &         0.000        &     5.57e-31    &     6.14e-31     \\\\\n",
       "\\textbf{np.power(n\\_tokens\\_title, 5)}             &    9.586e-27  &     2.36e-28     &    40.697  &         0.000        &     9.12e-27    &        1e-26     \\\\\n",
       "\\textbf{average\\_token\\_length}                    &    2.653e-31  &     6.52e-33     &    40.697  &         0.000        &     2.52e-31    &     2.78e-31     \\\\\n",
       "\\textbf{np.power(average\\_token\\_length, 5)}       &    1.335e-28  &     3.28e-30     &    40.697  &         0.000        &     1.27e-28    &      1.4e-28     \\\\\n",
       "\\textbf{num\\_keywords}                             &    3.577e-31  &     8.79e-33     &    40.697  &         0.000        &      3.4e-31    &     3.75e-31     \\\\\n",
       "\\textbf{np.power(num\\_keywords, 5)}                &    1.269e-27  &     3.12e-29     &    40.697  &         0.000        &     1.21e-27    &     1.33e-27     \\\\\n",
       "\\textbf{kw\\_min\\_avg}                              &    1.676e-28  &     4.12e-30     &    40.697  &         0.000        &      1.6e-28    &     1.76e-28     \\\\\n",
       "\\textbf{np.power(kw\\_min\\_avg, 5)}                 &    1.631e-14  &     4.01e-16     &    40.697  &         0.000        &     1.55e-14    &     1.71e-14     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 37211.834 & \\textbf{  Durbin-Watson:     } &      1.787    \\\\\n",
       "\\textbf{Prob(Omnibus):} &    0.000  & \\textbf{  Jarque-Bera (JB):  } & 14087045.465  \\\\\n",
       "\\textbf{Skew:}          &    8.729  & \\textbf{  Prob(JB):          } &       0.00    \\\\\n",
       "\\textbf{Kurtosis:}      &  116.234  & \\textbf{  Cond. No.          } &   3.79e+29    \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 3.79e+29. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 shares   R-squared:                      -0.131\n",
       "Model:                            OLS   Adj. R-squared:                 -0.131\n",
       "Method:                 Least Squares   F-statistic:                    -1494.\n",
       "Date:                Fri, 11 Aug 2023   Prob (F-statistic):               1.00\n",
       "Time:                        11:34:53   Log-Likelihood:            -2.6640e+05\n",
       "No. Observations:               25756   AIC:                         5.328e+05\n",
       "Df Residuals:                   25753   BIC:                         5.328e+05\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==========================================================================================================\n",
       "                                             coef    std err          t      P>|t|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------------------------------\n",
       "Intercept                               5.663e-32   1.39e-33     40.697      0.000    5.39e-32    5.94e-32\n",
       "kw_max_avg                             -1.205e-22   2.96e-24    -40.697      0.000   -1.26e-22   -1.15e-22\n",
       "np.power(kw_max_avg, 5)                 7.509e-24   3.17e-24      2.367      0.018    1.29e-24    1.37e-23\n",
       "global_subjectivity                     1.446e-44   3.55e-46     40.758      0.000    1.38e-44    1.52e-44\n",
       "np.power(global_subjectivity, 5)        2.229e-33   5.48e-35     40.697      0.000    2.12e-33    2.34e-33\n",
       "self_reference_min_shares               3.377e-28    8.3e-30     40.697      0.000    3.21e-28    3.54e-28\n",
       "np.power(self_reference_min_shares, 5)  2.042e-26   1.47e-26      1.390      0.165   -8.37e-27    4.92e-26\n",
       "num_hrefs                               7.504e-31   1.84e-32     40.697      0.000    7.14e-31    7.87e-31\n",
       "np.power(num_hrefs, 5)                  1.354e-23   3.33e-25     40.697      0.000    1.29e-23    1.42e-23\n",
       "data_channel                            2.597e-31   6.38e-33     40.697      0.000    2.47e-31    2.72e-31\n",
       "np.power(data_channel, 5)               3.887e-28   9.55e-30     40.697      0.000     3.7e-28    4.07e-28\n",
       "title_sentiment_polarity                5.238e-33   1.29e-34     40.697      0.000    4.99e-33    5.49e-33\n",
       "np.power(title_sentiment_polarity, 5)   1.251e-33   3.07e-35     40.697      0.000    1.19e-33    1.31e-33\n",
       "num_self_hrefs                          1.862e-31   4.58e-33     40.697      0.000    1.77e-31    1.95e-31\n",
       "np.power(num_self_hrefs, 5)             1.495e-25   3.67e-27     40.697      0.000    1.42e-25    1.57e-25\n",
       "num_imgs                                3.651e-31   8.97e-33     40.697      0.000    3.48e-31    3.83e-31\n",
       "np.power(num_imgs, 5)                   1.139e-24    2.8e-26     40.697      0.000    1.08e-24    1.19e-24\n",
       "n_tokens_title                          5.853e-31   1.44e-32     40.697      0.000    5.57e-31    6.14e-31\n",
       "np.power(n_tokens_title, 5)             9.586e-27   2.36e-28     40.697      0.000    9.12e-27       1e-26\n",
       "average_token_length                    2.653e-31   6.52e-33     40.697      0.000    2.52e-31    2.78e-31\n",
       "np.power(average_token_length, 5)       1.335e-28   3.28e-30     40.697      0.000    1.27e-28     1.4e-28\n",
       "num_keywords                            3.577e-31   8.79e-33     40.697      0.000     3.4e-31    3.75e-31\n",
       "np.power(num_keywords, 5)               1.269e-27   3.12e-29     40.697      0.000    1.21e-27    1.33e-27\n",
       "kw_min_avg                              1.676e-28   4.12e-30     40.697      0.000     1.6e-28    1.76e-28\n",
       "np.power(kw_min_avg, 5)                 1.631e-14   4.01e-16     40.697      0.000    1.55e-14    1.71e-14\n",
       "==============================================================================\n",
       "Omnibus:                    37211.834   Durbin-Watson:                   1.787\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):         14087045.465\n",
       "Skew:                           8.729   Prob(JB):                         0.00\n",
       "Kurtosis:                     116.234   Cond. No.                     3.79e+29\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 3.79e+29. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "degree = 5\n",
    "polynomial_regression_summary(train_df, significant_predictors2, 'shares', degree).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d14b9c5-8cd1-43a5-9c49-16db2831105a",
   "metadata": {},
   "source": [
    "#### Summary of Results & Analysis: Focus on Significant Predictors\n",
    "Given the goal of identifying the most influential factors affecting article shares, polynomial regression was employed on the subset of significant predictor variables. Remarkably, this approach produced the highest R-squared value of the entire analysis: 0.050 at a polynomial degree of 2. This underscores the importance of focusing on key predictors to build a more concise and accurate model, although, this model is still not very accurate as the R-squared is closer to 0 than 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e65b76f-b30e-4426-868a-c35a64e1c41c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Summary of Results & Analysis: Polynomial Regression Overall\n",
    "The application of polynomial regression provided valuable insights into the underlying relationships between predictor variables and the response variable. By flexibly accommodating nonlinear trends, this technique improved the model's predictive capability compared to the linear and multilinear regression approaches. The achieved R-squared values highlight the impact of careful predictor selection and the importance of accounting for nonlinearity in understanding the determinants of article shares. The highest R-squared achieved in this entire analysis was 0.05, which does mean that the model accuracy is not very good, as the model is significantly underfit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b24ec38-76ff-4570-baf8-a868877b3d85",
   "metadata": {},
   "source": [
    "## Discussion & Conclusion\n",
    "In this study, my goal was to explore article sharing behavior in the digital landscape. I went into this project with the hope that I could construct a model that could be used to accurately predict the number of shares an article was likely to get. I explored every technique learned in class and some techniques that I taught myself. From the straightforward linear regression to the more sophisticated multilinear and polynomial regressions, I left no stone unturned in my quest for insights. Although I was not able to construct an accurate model, I did learn several new techniques that I hope to employ in future projects.\n",
    "\n",
    "**Learning and Takeaways**  \n",
    "In the process of conducting this analysis, several valuable insights have been gleaned. The application of various regression techniques, including linear, multilinear, and polynomial regression, shed light on the complex relationship between predictor variables and article shares.\n",
    "\n",
    "One key takeaway is the importance of careful predictor selection. Through rigorous analysis, it became evident that certain predictors have a significant impact on article shares. This emphasizes the need for a thoughtful approach to feature engineering, which ultimately enhances model performance.\n",
    "\n",
    "Furthermore, the implementation of polynomial regression proved to be a crucial step in capturing nonlinear trends. This technique provided the flexibility to model intricate relationships that would have been missed by simpler linear models. By accommodating these nonlinearities, the model was able to make more accurate predictions and achieve a better fit to the data, although, it was ultimately underfit.\n",
    "\n",
    "**Opportunities for Improvement**  \n",
    "One notable limitation is the relatively low R-squared values achieved across the different models. This suggests that there are factors influencing article shares that are not accounted for in the current predictor set. To address this, future analyses could explore additional predictors or consider more advanced techniques that capture more complex interactions. Furthermore, more complex models like neural net or deep learning methods could be useful to understand the structure of the data better.\n",
    "\n",
    "**GitHub Repo**  \n",
    "https://github.com/MYWeidner/OnlineNewsPopularity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa1ecdc-4ea2-4788-9f39-1d9fc8e78662",
   "metadata": {},
   "source": [
    "**References:**  \n",
    "https://www.javatpoint.com/linear-regression-vs-logistic-regression-in-machine-learning#:~:text=Linear%20regression%20is%20used%20to,used%20for%20solving%20Regression%20problem\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2020/12/beginners-take-how-logistic-regression-is-related-to-linear-regression/\n",
    "\n",
    "https://www.datacamp.com/tutorial/tutorial-lasso-ridge-regression\n",
    "\n",
    "https://www.tutorialspoint.com/ridge-and-lasso-regression-explained\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2016/01/ridge-lasso-regression-python-complete-tutorial/#:~:text=The%20ridge%20coefficients%20are%20a,in%20comparison%20to%20the%20ridge.\n",
    "\n",
    "https://towardsdatascience.com/random-forest-in-python-24d0893d51c0\n",
    "\n",
    "https://williamkoehrsen.medium.com/random-forest-simple-explanation-377895a60d2d\n",
    "\n",
    "https://www.statisticshowto.com/variance-inflation-factor/\n",
    "\n",
    "https://statisticsbyjim.com/regression/variance-inflation-factors/\n",
    "\n",
    "https://enjoymachinelearning.com/blog/multivariate-polynomial-regression-python/?expand_article=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
